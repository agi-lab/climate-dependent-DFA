---
title: "DFA model Version Final"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\usepackage{booktabs}
\usepackage{siunitx}
\newcolumntype{d}{S[
    input-open-uncertainty=,
    input-close-uncertainty=,
    parse-numbers = false,
    table-align-text-pre=false,
    table-align-text-post=false
 ]}
 
# Introduction:

This Rmarkdown file seeks to reproduce the key results in the paper ``Dynamic Financial Analysis (DFA) of General Insurers under Climate Change", which contains the following sections that need to be run in the order specified: 

- Section [Loading required packages]: Load the required packages used in the paper.
- Section [Files and data path]: Define the file paths of all the data sources used in the paper. Those data sources are all stored in the `Data` folder. Unless a new data source is used, users do not need to change anything here. 
- Section [Define functions to be used]: This section defines all the functions used in the paper. 
- Section [Section 3.1 Data and calibration]: This section calibrates the model parameters based on historical data as per Section 3.1 in the paper. 
- Section [Section 3.2 Key simulation results from individual modules]: This section generates the results shown in Section 3.2 in the paper. 

    - The simulation control parameters are defined in [Define the control variables]. These include the number of simulations, random seed, maximum forecasting horizon, and asset allocations, etc. Users can modify these parameters to suit specific needs. After changing these parameters, only Sections **3.2** and **3.3** need to be rerun, and previous sections do not require rerunning.  
    - All graphs shown in Section 3.2 in the paper are plotted in [Presentations of simulation results]. 
    
- Section [Section 3.3 Risk and return measures]: This section presents the results shown in Section 3.3 in the paper.


# Loading required packages

```{r Loading required packages}
# Load all required packages using pacman
if (!require("pacman")) install.packages("pacman")
pacman::p_load(
  dplyr, readxl, ggplot2, tidyr, lubridate, splines, lmtest, sarima, astsa, 
  mgcv, gamlss, fitdistrplus, actuar, kableExtra, tweedie, statmod, Ecfun, 
  scatterplot3d, gratia, pracma, aTSA, ncdf4, raster, rgdal, sf, corrplot, 
  quantreg, wordcloud, tm, scales, stringr, modelsummary, coda, 
  plotly, GDPuc, colorspace, Rmpfr, cvar, VGAM, data.table
)
```


Note: If the `rgdal` package (used for processing spatial data) is not installed before running this script, please set         `clean_data_required <- TRUE` in the setting below. 

```{r Indicate whether clean data is required (for rgdal package)}
clean_data_required <- T
```


# Files and data path

We apologize that we are unable to provide the Total Returns series of the All-Ordinaries Shares Index and the collection of Woodside Energy Limited's financial statements from FactSet. Users are encouraged to obtain the data directly from FactSet.

For users without a FactSet subscription, we have provided pseudo data for model calibration. To use the pseudo data, please set the `Factset_subscription` indicator to `FALSE`. Please note that the resulting calibrated parameters from the pseudo data may differ slightly from those shown in the paper.

```{r FactSet data}
Factset_subscription <- FALSE
```


Enter the file paths of economic and financial data here: 

```{r Paths for economic and financial data}
pop_dat_path <- "Data/Economic/World_pop.xlsx" ## Historical population data path
GDP_dat_path <- "Data/Economic/AU-GDP-LCU.xlsx" ## Historical GDP data path
GDP_projections_dat_path <- "Data/Economic/SSP_scenarios.csv" ## GDP projections data path
Pop_projections_dat_path <- "Data/Economic/AU_population_projections.xlsx" ## Population projections data path
Consumption_dat_path <- "Data/Economic/AU_Consumption_LCU.xlsx" ## Consumption data path
CPI_dat_path <- "Data/Economic/AU_inflation_640101.xlsx" ## CPI data path
Interest_dat_path_1 <- "Data/Economic/Australian_cash_rate_data.xls"  ## Enter the data path for interest rates (2011 and before)
Interest_dat_path_2 <- "Data/Economic/Australian_cash_rate_data_2011_2023.xls" ## Enter the data path for interest rates (after 2011)
potential_growth_dat_path <- "Data/Economic/Potential-growth-database.xlsx" ## Enter the data path for potential growth
stock_dat_path <- if(Factset_subscription == T){"Data/Equity return/All-Ordinaries-Factset-Monthly.xlsx"} else{"Data/Equity return/All-Ordinaries-Monthly-pseudo.xlsx"} ## Enter data path for historical series of All-Ordinaries share
corporate_earnings_path <- "Data/Economic/AU_corporate_earnings.xlsx" ## Enter data path for Australian corporate earnings
gas_production_dat_path <- "Data/Economic/OECD_Gas_SSP.xlsx" ## Enter data path for planned gas production
oil_production_dat_path <- "Data/Economic/OECD_Oil_SSP.xlsx" ## Enter data path for planned oil production
rep_oil_gas_dat_path <-  if(Factset_subscription == T){"Data/Equity return/Woodside_Financials.xlsx"}else{"Data/Equity return/Woodside_Financials_pseudo.xlsx"} ## Enter data path for financial statements data of a representative oil and gas production firm
CPI_regress_coef_path <- "Data/CPI regression results KoKuLiNi24/FEOLS_results/dlCPITOTL_Tspeiday_lintren_DLDK_lag_coef_lagN11.csv"
```

Enter the file paths of hazard data here: 

```{r Paths for hazard data}
ICA_dat_path <- "Data/Hazards loss/ICA-Historical-Normalised-Catastrophe-August-2023.xlsx" ## CAT loss data path
EM_dat_path <- "Data/Hazards loss/EM_DAT_AU.xlsx" ## EM-DAT loss data path
```

Enter the file paths of historical climate data here: 

```{r Path for historical climate data, warning=FALSE}
Prep_hist_dat_path <- "Data/Precipitations/era5-x0.25_timeseries_pr,rx1day,rx5day_timeseries_annual_1950-2022_mean_historical_era5_x0.25_mean.xlsx" ## Data path for historical precipitation data
australia_shape <- if(clean_data_required == T){NULL}else{readOGR("Data/AUS_2021_AUST_GDA2020.shp")} ## Enter the shape file for Australia
## Enter the data path for fire weather index for each historical period: 
filenames_FWI <- list.files("Data/FWI", pattern="*.nc", full.names=TRUE) # Enter the path folder for the fire weather index
filenames_Sea <- list.files("Data/SST and MSLP/SST_MSLP_ERA5", pattern="*.nc", full.names=TRUE) # Enter the path folder for the sea-surface temperature and MSLP
nc_tas_data_ERA5_hist <- brick('Data/Near-surface temperature/ERA5.nc') # Enter the path folder for the near-surface temperature
nc_ta_data_ERA5_hist <- brick('Data/Air temperature/ERA5_air_temp_hpa300.nc') # Enter the path folder for the air temperature
```

Enter the file paths of climate data projections from CMIP6 model ensemble here: 

```{r Path for climate data projections}
## Near-surface temperature
tas_projections_dirc <- "Data/CMIP6_ensemble_near_surface_temperature/Projections_mod/csv files"
tas_hist_backcasts_dirc <- "Data/CMIP6_ensemble_near_surface_temperature/Historical/csv files/historical_ensemble_tas.csv"
## Air temperature:
ta_projections_dirc <- "Data/CMIP6_ensemble_air_temperature/Projections/csv files"
ta_hist_backcasts_dirc <- "Data/CMIP6_ensemble_air_temperature/Historical/csv files/historical_ensemble_ta.csv"
## Sea-surface temperature:
SST_projections_dirc <- "Data/CMIP6_ensemble_SST/Projections/csv files"
SST_hist_backcasts_dirc <- "Data/CMIP6_ensemble_SST/Historical/csv files/SST_ensemble_hist_data.csv"
SSTgrad_projections_dirc <- "Data/CMIP6_ensemble_SST/Projections/csv files"
SSTgrad_hist_backcasts_dirc <- "Data/CMIP6_ensemble_SST/Historical/csv files/SST_grad_ensemble_hist_data.csv"
## MSLP:
MSLP_projections_dirc <- "Data/CMIP6_ensemble_MSLP/Projections/csv files"
MSLP_hist_backcasts_dirc <- "Data/CMIP6_ensemble_MSLP/Historical/csv files/MSLP_ensemble_hist_data.csv"
## FWI:
fwi_projections_dirc <- "Data/FWI/Projections"
mfwi_hist_backcasts_dirc <- "Data/FWI/Historical/mfwixx_ensemble_hist_data.csv"
xfwi_hist_backcasts_dirc <- "Data/FWI/Historical/xfwixx_ensemble_hist_data.csv"
## Precipitation: 
extreme_precipitation_path <- "Data/CMIP6_ensemble_precipitation/cmip6_rx5day_ensemble.xlsx"
```


Enter file path for all market statistics:  

```{r Path for all market statistics}
GI_dev_stats_input <- "Data/Market statistics/General insurance claims development statistics database December 2022.xlsx" ## Enter the file path for claims development statistics from APRA
GI_fins_stats_input  <- "Data/Market statistics/Annual general insurance institution-level statistics database March 2005 to June 2023.xlsx"  ## Enter the file path for institution-level financial statistics from APRA
GI_perf_data_path <- "Data/Market statistics/Quarterly general insurance performance statistics database December 2002 to June 2023.xlsx" ## Enter the file path for quarterly GI performance statistics from APRA
```



# Define functions to be used

**This section defines all the functions used in the paper**.

The following functions are defined for importing, processing, and cleaning climate-related data for analysis:

- `transform_nc_FWI_func`: function to import and cleanse the Fire Weather Index data
- `transform_nc_SST_era5_func`: function to import and cleanse the Sea Surface Temperature data
- `transform_nc_MSLP_era5_func `: function to import and cleanse the Sea-Level Pressure data
- `transform_nc_tas_mm_func`: function to import and cleanse the surface and air temperature data

```{r Functions for cleaning and importing climate data}
###################################
## Fire weather index:
###################################
transform_nc_FWI_func <- function(australia_shape, nc_data){
    # Transform the Australian shape file to the same coordinate system as the FWI data:
    australia_shape_1 <- spTransform(australia_shape, crs(nc_data))
    # Crop the temperature data such that only the Australian region is covered: 
    nc_data_masked = mask(nc_data, australia_shape_1)
    ## Convert to data frame format for further valuation:
    nc_data_df_0 <- as.data.frame(nc_data_masked, xy = TRUE)
    ## Convert to long format and remove the values that are outside the Australian boundary
    nc_data_df_1 <- as.data.frame(na.omit(pivot_longer(nc_data_df_0, cols = starts_with("X", ignore.case = FALSE), names_to = "Dates", values_to = "FWI"))) %>% mutate(Year = as.numeric(substr(Dates, start = 2, stop = 5)), Month = as.numeric(substr(Dates, start = 7, stop = 8)), Day = as.numeric(substr(Dates, start = 10, stop = 11)))
    return(nc_data_df_1)
}

###################################
## SST:
###################################
transform_nc_SST_era5_func <- function(nc_data, lon_range = c(-Inf, Inf), lat_range = c(-Inf, Inf), Monthly = F){
    ## Convert to data frame format for further valuation:
    nc_data_df_0 <- as.data.frame(nc_data, xy = TRUE) %>% filter(x>=lon_range[1] & x<=lon_range[2]) %>% filter(y>=lat_range[1] & y<=lat_range[2])
    ## Convert to long format and remove the values that are outside the Australian boundary
    nc_data_df_1 <- as.data.frame(na.omit(pivot_longer(nc_data_df_0, cols = starts_with("X", ignore.case = FALSE), names_to = "Dates", values_to = "SST"))) %>% mutate(Year = as.numeric(substr(Dates, start = 2, stop = 5))) %>% mutate(Month = as.numeric(substr(Dates, start = 7, stop = 8)))
    nc_data_mm_avg <- nc_data_df_1 %>% group_by(Year, Month) %>% summarise(SST_mean_mm = mean(SST-273.15, na.rm = T))
    nc_data_avg <- nc_data_mm_avg %>% group_by(Year) %>% summarise(SST_mean = mean(SST_mean_mm),
                                                                   SST_max = max(SST_mean_mm),
                                                                  SST_thre = sum(SST_mean_mm >= 26)) 
    if(Monthly == T){return(nc_data_mm_avg)} else{return(nc_data_avg)}
}

###################################
## MSLP:
###################################

transform_nc_MSLP_era5_func <- function(nc_data,  lon_range = c(-Inf, Inf), lat_range = c(-Inf, Inf), Monthly = F){
    ## Convert to data frame format for further valuation:
    nc_data_df_0 <- as.data.frame(nc_data, xy = TRUE) %>% filter(x>=lon_range[1] & x<=lon_range[2]) %>% filter(y>=lat_range[1] & y<=lat_range[2])
    ## Convert to long format and remove the values that are outside the Australian boundary
    nc_data_df_1 <- as.data.frame(na.omit(pivot_longer(nc_data_df_0, cols = starts_with("X", ignore.case = FALSE), names_to = "Dates", values_to = "MSLP"))) %>% mutate(Year = as.numeric(substr(Dates, start = 2, stop = 5))) %>% mutate(Month = as.numeric(substr(Dates, start = 7, stop = 8)))
    nc_data_mm_avg <- nc_data_df_1 %>% group_by(Year, Month) %>% summarise(MSLP_mm = mean(MSLP, na.rm = T))
    nc_data_avg <- nc_data_mm_avg %>% group_by(Year) %>% summarise(MSLP_mean = mean(MSLP_mm),
                                                                 MSLP_max = max(MSLP_mm, na.rm = T)) ## Convert K to celsius
    if(Monthly == T){return(nc_data_mm_avg)} else{return(nc_data_avg)}
}

###################################
## Surface and air temperature: 
###################################
transform_nc_tas_mm_func <- function(australia_shape, nc_data){
    # Transform the Australian shape file to the same coordinate system as the temperature data:
    australia_shape_1 <- spTransform(australia_shape, crs(nc_data))
    # Crop the temperature data such that only the Australian region is covered: 
    nc_data_masked = mask(nc_data, australia_shape_1)
    ## Convert to data frame format for further valuation:
    nc_data_df_0 <- as.data.frame(nc_data_masked, xy=TRUE)
    ## Convert to long format and remove the values that are outside the Australian boundary
    nc_data_df_1 <- as.data.frame(na.omit(pivot_longer(nc_data_df_0, cols = starts_with("X", ignore.case = FALSE), names_to = "Dates", values_to = "Temperature_K"))) 
    ## Get the monthly average temperature data: 
    temp_avg_data <- nc_data_df_1 %>% group_by(Dates) %>% summarise(Temperature= mean(Temperature_K-273.15)) %>% ## Convert temperature in Kelvin to Celsius
        mutate(Year = as.numeric(substr(Dates, start = 2, stop = 5)), Month = as.numeric(substr(Dates, start = 7, stop = 8))) %>% dplyr::select(Year, Month, Temperature) ## mutate a Month and Year column
    return(temp_avg_data)
}

transform_nc_rh_mm_func <- function(australia_shape, nc_data){
    # Transform the Australian shape file to the same coordinate system as the temperature data:
    australia_shape_1 <- spTransform(australia_shape, crs(nc_data))
    # Crop the temperature data such that only the Australian region is covered: 
    nc_data_masked = mask(nc_data, australia_shape_1)
    ## Convert to data frame format for further valuation:
    nc_data_df_0 <- as.data.frame(nc_data_masked, xy=TRUE)
    ## Convert to long format and remove the values that are outside the Australian boundary
    nc_data_df_1 <- as.data.frame(na.omit(pivot_longer(nc_data_df_0, cols = starts_with("X", ignore.case = FALSE), names_to = "Dates", values_to = "value"))) 
    ## Get the monthly average temperature data: 
    rh_avg_data <- nc_data_df_1 %>% group_by(Dates) %>% summarise(Relative_humidity = mean(value)) %>% 
        mutate(Year = as.numeric(substr(Dates, start = 2, stop = 5)), Month = as.numeric(substr(Dates, start = 7, stop = 8))) %>% dplyr::select(Year, Month, Relative_humidity)
    return(rh_avg_data)
}
```

The following functions are defined for simulating the future climate variables: 

- `simulate_clim_var`: function to simulate future climate variables using the methodology outlined in Section 2.2.1.

```{r Functions for simulations: Climate variables}
simulate_clim_var <- function(scenario_index, ensemble_hist_dat, var_name, ensemble_all_scenarios_list, Model_list, n.sims, start_year, forecast_period){
    j <- scenario_index 
    full_pred_data <- ensemble_all_scenarios_list[[j]] %>% filter(Year >= start_year) %>%     filter(Year <= (start_year + forecast_period-1))
    ## Create two empty matrices to store the outputs
    sim_var_mm <- matrix(NA, nrow = nrow(full_pred_data), ncol = n.sims)
    sim_var_year <- matrix(NA, nrow = forecast_period, ncol = n.sims)
    for(i in 1:n.sims){
        mod_sampled <- sample(1:length(Model_list), size = 1, replace = T)
        lm_bias <- lm(sort(ensemble_hist_dat[, var_name]) ~ sort(ensemble_hist_dat[, Model_list[mod_sampled]]))
        hist_pred <- lm_bias$coefficients[1] + lm_bias$coefficients[2]*ensemble_hist_dat[, Model_list[mod_sampled]]
        noise_hist <- ensemble_hist_dat[, var_name] - hist_pred 
        var_pred <- lm_bias$coefficients[1] + lm_bias$coefficients[2]*full_pred_data[, Model_list[mod_sampled]]
        
        ## simulate noises:
        noises <- rnorm(n = length(var_pred), mean = 0, sd = sd(noise_hist))
        sim_var_mm_vec <- as.numeric(var_pred) + noises
        sim_var_year_dat <- data.frame(Year = full_pred_data$Year, Var = sim_var_mm_vec) %>% group_by(Year) %>% summarise(Var_yy = mean(Var))
        sim_var_mm[, i] <- sim_var_mm_vec
        sim_var_year[, i] <- as.numeric(sim_var_year_dat$Var_yy)
    }
return(list(sim_var_mm = sim_var_mm, sim_var_year = sim_var_year))
}
```

The following functions are defined for simulating the future inflation rates and interest rates:

- `convert_T_to_covariates_func`: function to convert the raw surface temperature data to the required format of the input data to the inflation rates model.
- `Infla_climate_pressure_func`: function to calculate the climate overlay on inflation rates based on the temperature input. 
- `simulateAR1_Inflation_Clim`: function to simulate the inflation rates
- `simulate_SR_AR1`: function to simulate the risk-free real interest rates. 

```{r Functions for simulations: Macro Economic Variables}
###################################
### Inflation rates: 
###################################

## Function to convert temperature data to inflation model input: 
convert_T_to_covariates_func <- function(Tmean_proj_data, Tmean_hist_period, Tmean_hist_data, proj_start_year){
    
    ## Obtain the covariate dataframe:
    Temp_proj_cov_df <- left_join(Tmean_proj_data, Tmean_hist_period, by = "Month") %>% mutate(Tmean_d = Temperature-Temperature_mean_hist) %>% filter(Year >= proj_start_year) %>%
        mutate(Interactions = Tmean_d*Temperature)
    Mean_hist_ERA5_temp_mm_df_22 <-Tmean_hist_data %>% filter(Year == (proj_start_year-1)) %>% dplyr::select(Month, Year, Temperature) %>% left_join(Tmean_hist_period, by = "Month") %>% mutate(Tmean_d = Temperature-Temperature_mean_hist) %>% mutate(Interactions = Tmean_d*Temperature)
    Temp_proj_cov_df_1 <- rbind(Mean_hist_ERA5_temp_mm_df_22, Temp_proj_cov_df)
    ## Obtain the covariates matrix by adding the lagged values:
    Temp_proj_cov_df_1 <- Temp_proj_cov_df_1 %>%
        mutate(Tmean_d_L1 = lag(Tmean_d,1),
               Tmean_d_L2 = lag(Tmean_d,2),
               Tmean_d_L3 = lag(Tmean_d,3),
               Tmean_d_L4 = lag(Tmean_d,4),
               Tmean_d_L5 = lag(Tmean_d,5),
               Tmean_d_L6 = lag(Tmean_d,6),
               Tmean_d_L7 = lag(Tmean_d,7),
               Tmean_d_L8 = lag(Tmean_d,8),
               Tmean_d_L9 = lag(Tmean_d,9),
               Tmean_d_L10 = lag(Tmean_d,10),
               Tmean_d_L11 = lag(Tmean_d,11)) %>%
        mutate(Interactions_L1 = lag(Interactions,1),
               Interactions_L2 = lag(Interactions,2),
               Interactions_L3 = lag(Interactions,3),
               Interactions_L4 = lag(Interactions,4),
               Interactions_L5 = lag(Interactions,5),
               Interactions_L6 = lag(Interactions,6),
               Interactions_L7 = lag(Interactions,7),
               Interactions_L8 = lag(Interactions,8),
               Interactions_L9 = lag(Interactions,9),
               Interactions_L10 = lag(Interactions,10),
               Interactions_L11 = lag(Interactions,11)) %>% filter(Year >= proj_start_year)
    Tmean_d_mat <- as.matrix(Temp_proj_cov_df_1 %>% dplyr::select(contains("Tmean")))
    TmeanTmean_d_mat <- as.matrix(Temp_proj_cov_df_1 %>% dplyr::select(contains("Interactions")))
    Tmean_cov_mat <- cbind(Tmean_d_mat, TmeanTmean_d_mat)
return(list(Full_projections_data = Temp_proj_cov_df_1, Tmean_cov_mat = Tmean_cov_mat))
}    

## Function to derive climate overlay on inflation rates simulations:
Infla_climate_pressure_func <- function(Projected_Years, Tmean_cov_mat, CPI_coef){
    CPI_impact <- Tmean_cov_mat %*% CPI_coef_vec 
    CPI_climate_impact_data <- data.frame(Year = Projected_Years, CPI_impact = CPI_impact) %>%
        group_by(Year) %>% summarise(Climate_pressure = sum(CPI_impact))
    return(CPI_climate_impact_data$Climate_pressure)
}

## Function to simulate inflation rates and CPI
simulateAR1_Inflation_Clim <- function(mu, phi, sigma, n, start_value, start_value_CPI, Climate_pressure) {
  x <- numeric(n+1)
  x[1] <- start_value
  CPI <- numeric(n+1)
  CPI[1] <- start_value_CPI
  # Generating the mean-reverting AR(1) process
  for (t in 2:(n+1)) {
    x[t] <- mu + phi * (x[t - 1]-mu) + rnorm(1, mean = 0, sd = sigma)
  }
  x_clim <- x+c(0,Climate_pressure[1:n])
  for (t in 2:(n+1)) {
    CPI[t] <- CPI[t-1]*(1+x_clim[t-1])
  }
  return(list(Inflation_rate = x_clim[2:(n+1)], CPI = CPI[2:(n+1)]))
}

simulate_SR_AR1 <- function(intercept, phi, sigma, n, start_value) {
  x <- numeric(n+1)
  x[1] <- start_value
  for (t in 2:(n+1)) {
    x[t] <- intercept + phi * (x[t - 1]-intercept) + rnorm(1, mean = 0, sd = sigma)
  }
  return(x[2:(n+1)])
}

###################################
### Interest rates: 
###################################
simulate_SR_AR1 <- function(intercept, phi, sigma, n, start_value) {
  x <- numeric(n+1)
  x[1] <- start_value
  for (t in 2:(n+1)) {
    x[t] <- intercept + phi * (x[t - 1]-intercept) + rnorm(1, mean = 0, sd = sigma)
  }
  return(x[2:(n+1)])
}

```

The following functions are defined for simulating the insurance cost:

- `simulate_Pois_GLM `: function to simulate the frequency of CAT events based on the Poisson distribution.
- `sim_LN_CAT_losses_func`: function to simulate the total insurance losses from CAT events based on the Poisson frequency and Log-Normal severity. 
- `sim_tweedie`: function to simulate the non-catastrophe loss based on the Tweedie distribution.


```{r Functions for simulations: Insurance losses}

###################################
### Catastrophe losses:
###################################
simulate_Pois_GLM <- function(model, newdata){
    pred_freq <- predict(model, newdata, type = "response")
    simulate_response <- rpois(nrow(newdata), pred_freq)
    newdata$simulate_response = simulate_response
    data_yy <- newdata %>% group_by(Year) %>% summarise(Count = sum(simulate_response))
    return(list(Count_mm = simulate_response, Count_yy = data_yy$Count))
}

sim_LN_CAT_losses_func <- function(LN_model, newdata, CAT_count, interval_year){
    pred_mu_LN <- predict(LN_model, newdata = newdata, type = "response", what = "mu")
    pred_sigma_LN <- predict(LN_model, newdata = newdata, type = "response", what = "sigma")
    #CAT_loss_individual <- list()
    CAT_loss_aggregate <- c()
    ## Obtain the simulations
    for(i in 1:nrow(newdata)){
        ## Simulate the LN losses:
        sim_LN <- rlnorm(CAT_count[i], meanlog  = pred_mu_LN[i], sdlog  = pred_sigma_LN[i])
        ## Simulate the CAT losses:
        CAT_loss <- sim_LN 
        #CAT_loss_individual[[i]] <- CAT_loss
        CAT_loss_aggregate[i] <- sum(CAT_loss)
        
    }
    newdata$CAT_loss_aggregate_raw = CAT_loss_aggregate
    data_agg <- newdata %>% group_by(Year) %>% summarise(CAT_loss_aggregate = sum(CAT_loss_aggregate_raw))
    if(interval_year == T){
    return(data_agg$CAT_loss_aggregate)
    } else{
        return(CAT_loss_aggregate)
    }
}

###################################
### Non-Catastrophe losses:
###################################
sim_tweedie <- function(mu, phi, power){
    sim_tw <- c()
    for(i in 1:length(mu)){
        sim_tw[i] <- rtweedie(1, mu = mu[i], phi = phi, power = power)
    }
    return(sim_tw)
}
```

The following functions are defined for calculating the premiums:

- `pLN_mix`: function to calculate the CDF values of the mixture Log-Normal distribution.
- `mixLN.discr.func`: function to discretise the density of the mixture Log-Normal distribution.
- `calc_aggr_dist`: function to derive the CDF, expected value and the variance of the aggregate CAT loss distribution (i.e., compound Poisson Log-Normal distribution) by considering any excess and limit applied. 
- `calc_re_premium`: function to calculate the reinsurance premiums based on the user-defined excess and limits.
- `premium_S_func_alt`: function to calculate the reinsurance cycle loading on the reinsurnace premiums.


```{r Functions for premiums calculation}
#########################################
## Density and CDF of mixture Log-Normal:
#########################################
dLN_mix <- function(x, mu, sigma, weights) {
  ## Calculate the mixture density:
  mix_dt <- sapply(x, function(xi) sum(weights * dlnorm(xi, meanlog = mu, sdlog = sigma)))
  mix_dt
}

pLN_mix <- function(q, mu, sigma, weights) {
  ## Calculate the mixture distribution function:
  mix_PL <- sapply(q, function(qi) sum(weights*plnorm(qi, meanlog = mu, sdlog = sigma)))
  mix_PL
}

###########################################################
## Density and CDF of mixture Log-Normal (truncated): 
###########################################################
dtLN_mix <- function(x, mu, sigma, low, weights) {
  PL <- sum(weights*plnorm(low, meanlog = mu, sdlog = sigma))
  ## Calculate the mixture density:
  mix_dt <- sapply(x, function(xi) sum(weights * dlnorm(xi, meanlog = mu, sdlog = sigma)))
  ## Calculate the final truncated density:
  mix_dt/(1 - PL) * (x >= low)
}

ptLN_mix <- function(q, mu, sigma, low, weights) {
  PL <- sum(weights*plnorm(low, meanlog = mu, sdlog = sigma))
  ## Calculate the mixture distribution function:
  mix_PL <- sapply(q, function(qi) sum(weights*plnorm(qi, meanlog = mu, sdlog = sigma)))
  ## Calculate the final truncated distribution function:
  (mix_PL - PL)/(1 - PL) * (q >= low)
}

mixLN.discr.func <- function(x, mu, sigma,  weights, limit, method = "rounding", length = 500){
    stp <- ceiling(limit/length)
    ## Return the unadjusted density:
    dens_unadj <- discretise(pLN_mix(x, mu, sigma, weights), from = 0,
  to = limit, step = stp, method = method)
    if(method == "rounding"){
    ## Adjustment factor for the last mass:
    end_mass_adj <- c(rep(0, (length(dens_unadj) - 1)), 1 - pLN_mix(limit + stp/2, mu, sigma, weights))
    ## Get the density adjusted for tail: 
    dens_unadj + end_mass_adj
    } else if(method == "lower"){
        dens_lower <- dens_unadj[-1]
        dens_lower[length(dens_lower)] <- 1 - pLN_mix(limit, mu, sigma, weights)
        dens_lower
    }
    else{
        ## Force the last mass in the vector to be 1-G(m): 
        dens_unadj[length(dens_unadj)] <- 1 - pLN_mix(limit, mu, sigma, weights)
        dens_unadj
    }
}

#######################################################################
## Derive the aggregate distribution based on the Panjer's recurssion
#######################################################################

calc_aggr_dist <- function(excess, t_y, s, limit_re, length.discr = 1000, market_cap = 1, disc_method = "rounding"){
    t_m_end <- t_y*12
    t_m_start <- t_m_end-11
    ## Define other parameters:
    limit <- excess + limit_re
    stp <- ceiling(limit/length.discr)
    loss_seq <- seq(from = 0, to = limit, by = stp)
    l_index <- which(loss_seq >= excess)[1]
    u_index <- which.max(loss_seq) 
    ########### Storms related ########### 
    fx_storms_mm <- matrix(NA, ncol = 12, nrow = length.discr)
    weights_storms_season <- weights_season_ssp_list[[s]][t_y, ]
    freq_total_storms_yy <- as.numeric(aggr_freq_storm_family_yy[t_y, s+1])
    ## Loop through each month to derive monthly discretised CDF
    for(i in 1:12){
        t_m <- (t_m_start:t_m_end)[i]
        ## Storms parameters:
        mu_storms_mm <- as.numeric(Storm_related_mu_list[[s]][t_m, ])
        sigma_storms <- as.numeric(Storm_related_sigma_list[[s]])
        w_storms_mm <- as.numeric(weights_monthly_ssps_list[[s]][t_m, ])
        freq_total_storms_mm <- aggr_freq_storm_family[t_m, s]
        ## Derive the hazard-weighted discretised CDF for monthly loss:
        mixLN_storm_mm <- mixLN.discr.func(x, mu = log(market_cap) + mu_storms_mm, sigma = sigma_storms, weights = w_storms_mm, limit = limit, length = length.discr, method = disc_method)
        fx_storms_mm[, i] <- mixLN_storm_mm
    }
    ## Derive the monthly weighted discretised CDF for storm related losses:
    fx_storms_yy <- fx_storms_mm %*% weights_storms_season
    
    ########### Bushfire and Floods related ########### 
    ## Bushfire and flood parameters:
    mu_BFFL <- as.numeric(BFFL_related_mu_list[[s]][t_y, ]) + log(market_cap) 
    sigma_BFFL <- as.numeric(as.numeric(BFFL_related_sigma_list[[s]]))
    w_BFFL <- as.numeric(weights_yy_ssps_list[[s]][t_y, ])
    freq_total_BFFL <- aggr_freq_BF_FL[t_y, s]
    ## Derive the discretised CDF:
    mixLN_FLBF <- mixLN.discr.func(x, mu = mu_BFFL, sigma = sigma_BFFL, weights = w_BFFL, limit = limit, length = length.discr, method = disc_method)
    
    ########### Derive the aggregate distribution ########### 
    Total_freq <- Hazard_freq_pred_dat[t_y, s + 1]
    weights_yy_storms_BFFL <- as.numeric(weights_yy_ssps_storms_BFFL_list[[s]][t_y, ])
    fx_aggr_yy <- as.matrix(cbind(fx_storms_yy, mixLN_FLBF)) %*% weights_yy_storms_BFFL
    Fs_aggr_yy <- aggregateDist(method = "recursive", model.freq = "poisson",
      lambda = Total_freq, model.sev = fx_aggr_yy, x.scale = stp, maxit = length.discr)
    fs_aggr_yy <- diff(Fs_aggr_yy)
    ########## Derive the truncated likelihood ########### 
    Fs_u <- cumsum(fs_aggr_yy)[u_index]
    Fs_l <- cumsum(fs_aggr_yy)[l_index]
    ## Derive the left-truncated and right-censored density and CDF: 
    fs_tc_aggr_yy <- fs_aggr_yy/(Fs_u - Fs_l) * ifelse((1:length(loss_seq)) >= l_index, 1, 0) * ifelse((1:length(loss_seq)) <= u_index, 1, 0)
    Fs_tc_aggr_yy <- (cumsum(fs_aggr_yy) - Fs_l)/(Fs_u - Fs_l) * ifelse((1:length(loss_seq)) >= l_index, 1, 0) * ifelse((1:length(loss_seq)) <= u_index, 1, 0) + 1 * ifelse((1:length(loss_seq)) > u_index, 1, 0)
    ## Calculate the expected value of reinsurance payments:
    # Evaluate 1 - F(s) at each s: 
    #Fs_disc <- 1 - Fs_tc_aggr_yy[l_index:u_index]
    Fs_disc <- 1 - cumsum(fs_aggr_yy)[l_index:u_index]
    Fs_full <- 1 - cumsum(fs_aggr_yy)
    #fs_disc <- fs_tc_aggr_yy[l_index:u_index]
    Re_loss_seq <- loss_seq[l_index:u_index]-excess
    n <- length(Fs_disc)
    # Apply the trapezoidal rule to derive the integral: 
    Es_tc <- stp * (0.5 * Fs_disc[1] + sum(Fs_disc[2:(n-1)]) + 0.5 * Fs_disc[n])
    #Es_tc <- stp * (0.5 * Fs_disc[1] + sum(Fs_disc[2:(n-1)]) + 0.5 * Fs_disc[n]) + limit * (1 - Fs_disc[n])
    #Es_tc <- stp * (sum(Fs_disc))
    Es_tc2 <- 2 * stp * (0.5 * Re_loss_seq[1] * Fs_disc[1] + sum(Re_loss_seq[2:(n-1)]*Fs_disc[2:(n-1)]) + 0.5*Re_loss_seq[n]*Fs_disc[n])
    Var_tc <- Es_tc2 - Es_tc^2
    
return(list(fs_aggr = fs_aggr_yy, fs_aggr_trunc = fs_tc_aggr_yy, Fs_aggr_trunc = Fs_tc_aggr_yy, Es_aggr_trunc = Es_tc, Var_aggr_trunc = Var_tc))
}

### Write a function to calculate reinsurance premium based on the Panjer's recursion: 
calc_re_premium <- function(excess_list, s, horizon,
                               limit_re_list, market_cap = 1){
        E_Re_aggr <- c()
        Var_Re_aggr <- c()
    for(t in 1:horizon){
        aggr_dist <- calc_aggr_dist(excess = excess_list[t], t_y = t, s = s, limit_re = limit_re_list[t], market_cap = market_cap, length.discr = 1000, disc_method = "rounding")
        # Calculated the expected reinsurance loss:
        E_Re_aggr[t] <- aggr_dist$Es_aggr_trunc
        Var_Re_aggr[t] <-aggr_dist$Var_aggr_trunc
    }
return(list(Es_aggr_trunc = E_Re_aggr, Var_aggr_trunc = Var_Re_aggr))
}

#######################################################################
## Set up the assumptions and function for the reinsurance cycle model
#######################################################################
k1_default <- 0.667
S_ref_default <- 1
P_re_floor <- 0.5 
k3_default <- 0 ## Dividend payout ratio
## Write the functions
premium_S_func_alt <- function(P_ref, S_prev, S_ref = S_ref_default, k1 = k1_default, floor_param = P_re_floor){
    return(pmax(P_ref*exp(-k1*(S_prev - S_ref)), floor_param))
}
dividends_payout_func <- function(K_exDiv, P_t, S_0, k3 = k3_default){
    Div <- pmax(0, k3 * (K_exDiv - S_0 * P_t))
    return(Div)
}
```

The following functions is defined for calculating the surplus:

- `calc_surplus_func_full_cycle`: function to calculate the insurers' surplus based on the simulated insurance cost, investment income and premiums. 

```{r Functions for surplus calculation}
calc_surplus_func_full_cycle <- function(Risk_free_weight, 
                              Brown_weight, 
                              market_cap,
                              reinsurance_excess,
                              reinsurance_limit_multiple, 
                              rates_constrain = FALSE,
                              Loading_cap = NULL, 
                              rates_stability = FALSE,
                              stability_cap = NULL,
                              Re_rates_constrain = FALSE, 
                              Loading_cap_Re = NULL,
                              S_init, 
                              S_init_norm, 
                              scenario_index, 
                              Expected_Non_CAT_Loss,
                              dispersion_nonCAT,
                              Expected_Non_CAT_Loss_other, 
                              dispersion_nonCAT_other,
                              Exposure_pred_other_full, 
                              Power_Non_CAT_loss, 
                              Expected_CAT_Loss,
                              Var_CAT_loss, 
                              risk_aversion, 
                              CPI_pred_mat, CPI_start = CPI_start, 
                              GDP_pred_mat, GDP_start = Real_GDP_start, 
                              Exposure_start_market = Exposure_start, 
                              Exposure_pred_mat,
                              simulation_DFA_list_all, 
                              equities_output, 
                              equities_output_brown,
                              n.sims, 
                              horizon,
                              k1_param = k1_default,
                              S0_param = S_ref_default,
                              k3_param = k3_default, 
                              assumption_alternative = FALSE){
    
     ## Obtain the CPI and GDP corresponding to each scenario:
     CPI_pred <- CPI_pred_mat[1:horizon, scenario_index]
     GDP_pred <- GDP_pred_mat[1:horizon, scenario_index]
     Exposure_pred <- Exposure_pred_mat[1:horizon, scenario_index]*market_cap
     Exposure_pred_other <- Exposure_pred_other_full[1:horizon]*market_cap
     Exposure_start <- Exposure_start_market * market_cap
    
     ## Calculate the gross premium:
     Premium_loading <- (sqrt(Var_CAT_loss[1:horizon, scenario_index])/Expected_CAT_Loss[1:horizon, scenario_index])*risk_aversion
     Var_nonCAT <- dispersion_nonCAT*Expected_Non_CAT_Loss^Power_Non_CAT_loss
     CoV_nonCAT <- sqrt(Var_nonCAT)/Expected_Non_CAT_Loss
     Premium_nonCAT_loading <- CoV_nonCAT*risk_aversion
     
     Var_nonCAT_other <- dispersion_nonCAT_other*Expected_Non_CAT_Loss_other^Power_Non_CAT_loss
     CoV_nonCAT_other <- sqrt(Var_nonCAT_other)/Expected_Non_CAT_Loss_other
     Premium_nonCAT_other_loading <- CoV_nonCAT_other*risk_aversion
     
     Premium_loading_adjusted <- if(rates_constrain == FALSE){
         Premium_loading
     } else{pmin(Premium_loading, Loading_cap)}
     Premium_gross_CAT_norm_raw <- (1 +  Premium_loading_adjusted)*(Expected_CAT_Loss[1:horizon, scenario_index])
     
     Premium_gross_CAT_norm <- if(rates_stability == FALSE){
        Premium_gross_CAT_norm_raw
    } else{
        Premium_gross_CAT_norm_adj <- c()
        Premium_gross_CAT_norm_adj[1] <- Premium_gross_CAT_norm_raw[1]
        for(p in 2:length(Premium_gross_CAT_norm_raw)){
            rates_rise <- Premium_gross_CAT_norm_raw[p]/Premium_gross_CAT_norm_adj[p-1] - 1
            Premium_gross_CAT_norm_adj[p] <- Premium_gross_CAT_norm_adj[p-1]*(1 + min(rates_rise, stability_cap))
        }
        Premium_gross_CAT_norm_adj
    }

     Premium_gross_CAT <- market_cap*Premium_gross_CAT_norm*(CPI_pred/CPI_start)*(GDP_pred/GDP_start)
     Premium_gross_Non_CAT <- Expected_Non_CAT_Loss*(1 + Premium_nonCAT_loading)*(CPI_pred/CPI_start)*Exposure_pred
     Premium_gross_Non_CAT_norm <- Expected_Non_CAT_Loss*(1 + Premium_nonCAT_loading)*Exposure_start
     
      ## Also derive the non-CAT: other premiums:
     Premium_gross_Non_CAT_other <- Expected_Non_CAT_Loss_other*(1 + Premium_nonCAT_other_loading)*(CPI_pred/CPI_start)*Exposure_pred_other
     Premium_gross_Non_CAT_other_norm <- Expected_Non_CAT_Loss_other*(1 + Premium_nonCAT_other_loading)*Exposure_pred_other
     
     
     ## Calculate the reinsurance premium: 
     ### Adjust the reinsurance excess and limit:
     reinsurance_excess_adj <- reinsurance_excess*(CPI_pred/CPI_start)*(GDP_pred/GDP_start)
     Limit_adj <- reinsurance_excess_adj * reinsurance_limit_multiple
     Limit <- reinsurance_excess*reinsurance_limit_multiple
     ### Calculate the reinsurance premium using Panjer's recursion: 
     Re_loss_CAT <- calc_re_premium(excess_list = rep(reinsurance_excess, horizon), s = scenario_index, horizon = horizon, limit_re_list = rep(reinsurance_excess*reinsurance_limit_multiple, horizon), market_cap = market_cap)
     
     Re_premium_loading <- risk_aversion * sqrt(Re_loss_CAT$Var_aggr_trunc)/Re_loss_CAT$Es_aggr_trunc
     Re_premium_loading_adj <- if(Re_rates_constrain == FALSE){
         Re_premium_loading
     } else{pmin(Re_premium_loading, Loading_cap_Re)}
    
     Re_premium_CAT_base <- Re_loss_CAT$Es_aggr_trunc *(CPI_pred/CPI_start) * (GDP_pred/GDP_start)
     Re_premium_CAT <- Re_loss_CAT$Es_aggr_trunc * (1 + Re_premium_loading_adj) *(CPI_pred/CPI_start) * (GDP_pred/GDP_start)
     Re_premium_CAT_norm <- Re_loss_CAT$Es_aggr_trunc * (1 + Re_premium_loading_adj)
     simulation_DFA_list <- simulation_DFA_list_all[[scenario_index]]
     ## Define a empty matrices to store the capital level and underwriting loss: 
     Capital_sims_mat <- matrix(NA, nrow = length(CPI_pred), ncol = n.sims)
     Re_Capital_sims_mat <- matrix(NA, nrow = length(CPI_pred), ncol = n.sims)
     #Capital_sims_normalised_mat <- matrix(NA, nrow = length(CPI_pred), ncol = n.sims)
     UL_results <- c()
     Premium_net_sims_mat <- matrix(NA, nrow = length(CPI_pred), ncol = n.sims)
     Premium_re_sims_mat <- matrix(NA, nrow = length(CPI_pred), ncol = n.sims)
     
     Claims_cost_list <- c()
     Investment_returns_list <- c()
     #UL_results_norm <- c()
     for(i in 1:n.sims){
     
     ################ Calculate the claims cost ################ ################ 
     ## Get the non-catastrophe losses
     Non_CAT_losses_withInflation <- simulation_DFA_list$Liabilities_module$Non_CAT_loss_nominal[1:horizon, i]*market_cap
     Non_CAT_losses_withInflation_other <- simulation_DFA_list$Liabilities_module$Non_CAT_other_loss_nominal[1:horizon, i]*market_cap
     Non_CAT_losses_norm <- simulation_DFA_list$Liabilities_module$Non_CAT_loss[1:horizon, i]*Exposure_start/Exposure_pred*market_cap
      Non_CAT_losses_norm_other <- simulation_DFA_list$Liabilities_module$Non_CAT_other_loss[1:horizon, i]*market_cap
     ## Get the gross CAT losses:
     CAT_losses_gross <- simulation_DFA_list$Liabilities_module$CAT_nominal_loss[1:horizon, i]*market_cap
     CAT_losses_gross_norm <- simulation_DFA_list$Liabilities_module$CAT_normalised_loss[1:horizon, i]*market_cap
     
     ## Get the net CAT losses:
     CAT_losses_net <- CAT_losses_gross-pmax(pmin(CAT_losses_gross-reinsurance_excess_adj, Limit_adj), 0)
     CAT_losses_net_norm <- CAT_losses_gross_norm - pmax(pmin(CAT_losses_gross_norm - reinsurance_excess, Limit), 0)
     
     ## Get the reinsurance loss:
     Re_CAT_loss <- pmax(pmin(CAT_losses_gross-reinsurance_excess_adj, Limit_adj), 0)
     
     ## Get the total claims cost:
     claims_cost_total <- Non_CAT_losses_withInflation + CAT_losses_net + Non_CAT_losses_withInflation_other
     claims_cost_total_norm <- Non_CAT_losses_norm + CAT_losses_net_norm + Non_CAT_losses_norm_other
     
     ################ Calculate the investment return ################ ################
     ## Get the norminal risk-free rates:
     Short_rates_nominal <-  if(assumption_alternative == FALSE){simulation_DFA_list$MEV_module$Nominal_rates[1:horizon, i]} else{simulation_DFA_list$MEV_module$Nominal_rates_alt[1:horizon, i]}
     Short_rates_real <- if(assumption_alternative == FALSE){simulation_DFA_list$MEV_module$Real_rates[1:horizon, i]} else{simulation_DFA_list$MEV_module$Real_rates_alternative[1:horizon, i]}
     ## Get the brown and other excess return:
     brown_return <- equities_output[[scenario_index]][1:horizon, i]
     other_return <- equities_output_brown[[scenario_index]][1:horizon, i]
     
     
     ## Get the excess equity return:
     equity_return_excess <- Brown_weight*brown_return + (1-Brown_weight)*other_return
     #### Get the nominal equity return: 
     equity_return <- pmax(equity_return_excess + Short_rates_nominal, -1)
     equity_return_real <- pmax(equity_return_excess + Short_rates_real, -1)
     ## Get the total return
     investment_return <- Risk_free_weight*Short_rates_nominal + (1-Risk_free_weight)*equity_return 
     investment_return_real <- Risk_free_weight*Short_rates_real + (1-Risk_free_weight)*equity_return_real
     
     ### Calculate the simulated surpluses ####:
     capital_projections <- c()
     capital_projections[1] <- S_init
     
     re_capital_projections <- c()
     re_capital_projections[1] <- S0_param * Re_premium_CAT[1]
      
     re_premium_projections <- c()
     re_premium_projections[1] <- Re_premium_CAT[1]
     
     Premium_net <- c()
     Premium_net[1] <- Premium_gross_CAT[1] + Premium_gross_Non_CAT[1] + Premium_gross_Non_CAT_other[1] - Re_premium_CAT[1]
     for(k in 2:(length(Premium_gross_CAT)+1)){
         S_prev_Re <- if(re_capital_projections[k-1] <= 0){0}else{re_capital_projections[k-1]/(re_premium_projections[k-1])}
         P_re_sim <- premium_S_func_alt(P_ref = Re_premium_CAT[k -1], S_prev = S_prev_Re, S_ref = S0_param, k1 = k1_param, floor_param = Re_premium_CAT[k - 1])
         ## Calculate the net premium:
         P_net <- Premium_gross_CAT[k-1] + Premium_gross_Non_CAT[k-1] + Premium_gross_Non_CAT_other[k-1] - P_re_sim
         ## Calculate the dividend payout:
         Re_capital_exDiv <- (re_capital_projections[k-1] + P_re_sim)*(1 + investment_return[k-1]) -  Re_CAT_loss[k-1]
         Div_rev <- dividends_payout_func(K_exDiv = Re_capital_exDiv, P_t = P_re_sim, S_0 = S0_param, k3 = k3_param)
         ## Calculate the reinsurance capital:
         re_capital_projections[k] <- ifelse(re_capital_projections[k - 1] <= 0, 0, Re_capital_exDiv - Div_rev)
         ## Calculate the insurer capital:
         capital_projections[k] <- ifelse(capital_projections[k-1] <= 0, 0, (capital_projections[k-1] + P_net)*(1 + investment_return[k-1]) - claims_cost_total[k-1])
         ## Assign the net premium:
         Premium_net[k] <- P_net
         ## Assign the reinsurance premium: 
         re_premium_projections[k] <- P_re_sim
     }
     Capital_sims_mat[, i] <- capital_projections[2:(length(Premium_gross_CAT)+1)]
     UL_results[i] <- claims_cost_total[1]/(1 + investment_return[1]) - Premium_net[1]
     Premium_net_sims_mat[, i] <- Premium_net[2:(length(Premium_gross_CAT)+1)]
     Premium_re_sims_mat[, i] <- re_premium_projections[2:(length(Premium_gross_CAT)+1)]
     Re_Capital_sims_mat[, i] <- re_capital_projections[2:(length(Premium_gross_CAT)+1)]
     
     Claims_cost_list[i] <- claims_cost_total[1]
     Investment_returns_list[i] <- investment_return[1]
    }
return(list(Capital_sims = Capital_sims_mat, Premiums = Premium_net_sims_mat, Premiums_Re = Premium_re_sims_mat, Capital_sims_re = Re_Capital_sims_mat, Premiums_Re_Unadj = Re_premium_CAT, Underwriting_Loss_sims = UL_results))
}
```



The following functions is defined for calculating the net CAT insurance costs:

- `calc_reinsurance_recoveries`: function to calculate the net CAT insurance costs by considering the reinsurance recoverables
- `calc_reinsurance_recoveries_norm`: function to calculate the normalised value of net CAT insurance loss

```{r Function for net CAT loss calculation and reinsurance recoverables}
calc_reinsurance_recoveries <- function(horizon, market_cap, scenario_index, reinsurance_excess, reinsurance_limit_multiple, CPI_pred_mat,  GDP_pred_mat, CPI_start, GDP_start){
     ## Get the CPI and GDP exposure: 
     CPI_pred <- CPI_pred_mat[1:horizon, scenario_index]
     GDP_pred <- GDP_pred_mat[1:horizon, scenario_index]
     
     ## Index the reinsurance excess and limit: 
     reinsurance_excess_adj <- reinsurance_excess*(CPI_pred/CPI_start)*(GDP_pred/GDP_start)
     Limit_adj <- reinsurance_excess_adj * reinsurance_limit_multiple
     
     Re_recoveries_mat <- matrix(NA, nrow = horizon, ncol = n.sims)
     CAT_loss_mat <- matrix(NA, nrow = horizon, ncol = n.sims)
     for(i in 1:n.sims){
     ## Get the gross CAT losses:
     CAT_losses_gross <- DFA_components_all_scenarios[[scenario_index]]$Liabilities_module$CAT_nominal_loss[1:horizon, i]*market_cap
     ## Get the net CAT losses:
     Re_recoveries_mat[, i] <-  pmax(pmin(CAT_losses_gross-reinsurance_excess_adj, Limit_adj), 0)
     CAT_loss_mat[, i] <- CAT_losses_gross - pmax(pmin(CAT_losses_gross-reinsurance_excess_adj, Limit_adj), 0)
     }
     return(list(Reinsurance_recov = Re_recoveries_mat, CAT_loss_net = CAT_loss_mat))
}
calc_reinsurance_recoveries_norm <- function(horizon, market_cap, scenario_index, reinsurance_excess, reinsurance_limit_multiple){
     Re_recoveries_mat <- matrix(NA, nrow = horizon, ncol = n.sims)
     CAT_loss_mat <- matrix(NA, nrow = horizon, ncol = n.sims)
     for(i in 1:n.sims){
     ## Get the gross CAT losses:
     CAT_losses_gross <- DFA_components_all_scenarios[[scenario_index]]$Liabilities_module$CAT_normalised_loss[1:horizon, i]*market_cap
     ## Get the net CAT losses:
     Re_recoveries_mat[, i] <-  pmax(pmin(CAT_losses_gross-reinsurance_excess, reinsurance_excess*reinsurance_limit_multiple), 0)
     CAT_loss_mat[, i] <- CAT_losses_gross - pmax(pmin(CAT_losses_gross-reinsurance_excess, reinsurance_excess*reinsurance_limit_multiple), 0)
     }
     return(list(Reinsurance_recov = Re_recoveries_mat, CAT_loss_net = CAT_loss_mat))
}
```


Define other miscellaneous functions used in the paper: 

- `output_stressed_cal`: function to calculate the impact of reduced oil and gas production on the operating and net income of a representative firm.
- `barplot_rotate`: function to produce a nice bar plot
- `plot_quantiles_scenario` and `plot_quantiles_scenario_2`: function to plot the central path and the uncertainty bound of the simulated variable under all climate scenarios. 
- `extract_coefs_clim_var`: function to extract the coefficients of bias-correction.
- `find_elements_file`: function to find a file name that matches specific patterns from a list of file names. 


```{r Miscellaneous functions}
`%nin%` = Negate(`%in%`)
## Function to derive the transition stress: 
output_stressed_cal <- function(output_stressed, data){
    colnames(data)[2] = "Original.value"
    ## Calculate the stressed revenue: 
    stressed_sales <- data[data$Items == "Sales", ]$Original.value*(1-output_stressed)
    
    ## Calculate the stressed COGs: 
    stressed_COGs <- data[data$Items == "COGS excluding D&A", ]$Original.value*(1-output_stressed)
    
    ## Calculate the stressed Gross incomes: 
    stressed_gross_income <- stressed_sales-stressed_COGs-data[data$Items == "Depreciation & Amortization Expense", ]$Original.value
    
    ## Calculate the stressed EBIT and net income: 
    stressed_EBIT <- stressed_gross_income - data[data$Items == "SG&A Expense", ]$Original.value
    stressed_net_income <- (stressed_EBIT + data[data$Items == "Nonoperating Income - Net", ]$Original.value-data[data$Items == "Interest Expense", ]$Original.value)*(1-0.3)
    
    ## Create a column with stressed financial output: 
    stressed_fins <- c(stressed_sales, 
                          stressed_COGs,
                          data[data$Items == "Depreciation & Amortization Expense", ]$Original.value,
                          stressed_gross_income, 
                          data[data$Items == "SG&A Expense", ]$Original.value, 
                          stressed_EBIT, 
                          data[data$Items == "Nonoperating Income - Net", ]$Original.value,
                          data[data$Items == "Interest Expense", ]$Original.value, 
                          stressed_net_income)
    
    data <- data %>%
        mutate(Stressed.Values = stressed_fins) %>% mutate(Change = (stressed_fins-Original.value)/Original.value)
    scalar_impact_NI <- -data[data$Items == "Net income", ]$Change/output_stressed 
    scalar_impact_EBIT <- -data[data$Items == "EBIT (Operating Income)", ]$Change/output_stressed 
    return(list(Scalar_NI = scalar_impact_NI, Scalar_EBIT = scalar_impact_EBIT))
}

## Function for plotting barplot:
barplot_rotate <- function(data, column_to_plot, labels_vec, rot_angle, title, x_label, y_label) {
    plt <- barplot(data[[column_to_plot]], col='steelblue', xaxt="n", main = title, xlab = x_label, ylab = y_label)
    text(plt, par("usr")[3], labels = labels_vec, srt = rot_angle, adj = c(1.1,1.1), xpd = TRUE, cex=0.6) 
}

## Function for plotting quantiles:
plot_quantiles_scenario <- function(quantiles_list, forecast_periods, var_name, main, y_range, colour_list, horizon, transparency, label_percents = F, perc_step = 0.05, LOG = FALSE){
    if(label_percents == T){
    # Plotting
    plot(forecast_periods, quantiles_list[[1]][2, 1:horizon], type = "n", ylim = y_range,
         xlab = "Forecast Period", ylab = var_name, main = main, yaxt = "n")
    axis(2, at = seq(y_range[1], y_range[2], by = perc_step), labels = paste0(seq(y_range[1], y_range[2], by = perc_step) * 100, '%'), las = 1)
    } else{
        plot(forecast_periods, quantiles_list[[1]][2, 1:horizon], type = "n", ylim = y_range,
         xlab = "Forecast Period", ylab = var_name, main = main)
    }
    if(LOG == FALSE){
    for(j in 1:length(quantiles_list)){
        colour = colour_list[[j]]
        # Fill areas between quantiles
        polygon(c(forecast_periods, rev(forecast_periods)), 
                c(quantiles_list[[j]][1, 1:horizon], rev(quantiles_list[[j]][3, 1:horizon])), 
                col = alpha(colour, transparency), border = NA) 
        
        # Add quantile lines
        lines(forecast_periods, quantiles_list[[j]][2, 1:horizon], col = colour, lwd = 2, lty = 1) # 50% quantile (median)
        lines(forecast_periods, quantiles_list[[j]][1, 1:horizon], col = colour, lty = 2) # Lower quantile
        lines(forecast_periods, quantiles_list[[j]][3, 1:horizon], col = colour, lty = 2) # Upper quantile
    }} else{
        for(j in 1:length(quantiles_list)){
        colour = colour_list[[j]]
        # Fill areas between quantiles
        polygon(c(forecast_periods, rev(forecast_periods)), 
                log(c(quantiles_list[[j]][1, 1:horizon], rev(quantiles_list[[j]][3, 1:horizon]))), 
                col = alpha(colour, transparency), border = NA) 
        
        # Add quantile lines
        lines(forecast_periods, log(quantiles_list[[j]][2, 1:horizon]), col = colour, lwd = 2, lty = 1) # 50% quantile (median)
        lines(forecast_periods, log(quantiles_list[[j]][1, 1:horizon]), col = colour, lty = 2) # Lower quantile
        lines(forecast_periods, log(quantiles_list[[j]][3, 1:horizon]), col = colour, lty = 2) # Upper quantile
            }
        }
}

## Alternative function for plotting quantiles:
plot_quantiles_scenario_2 <- function(quantiles_list, forecast_periods, var_name, main, y_range, colour_list, horizon, transparency, label_percents = F, lty_mean = 1, lwd_mean = 2, step_plot_perc = 0.05, quantile_lines = T){
    if(label_percents == T){
    # Plotting
    plot(forecast_periods, quantiles_list[[1]][2, 1:horizon], type = "n", ylim = y_range,
         xlab = "Forecast Period", ylab = var_name, main = main, yaxt = "n")
    axis(2, at = seq(y_range[1], y_range[2], by = step_plot_perc), labels = paste0(seq(y_range[1], y_range[2], by = step_plot_perc) * 100, '%'), las = 1)
    } else{
        plot(forecast_periods, quantiles_list[[1]][2, 1:horizon], type = "n", ylim = y_range,
         xlab = "Forecast Period", ylab = var_name, main = main)
    }
    
    for(j in 1:length(quantiles_list)){
        colour = colour_list[[j]]
        # Fill areas between quantiles
        polygon(c(forecast_periods, rev(forecast_periods)), 
                c(quantiles_list[[j]][1, 1:horizon], rev(quantiles_list[[j]][3, 1:horizon])), 
                col = alpha(colour, transparency), border = NA) 
        
        if(quantile_lines == T){
        # Add quantile lines
        lines(forecast_periods, quantiles_list[[j]][2, 1:horizon], col = colour, lwd = lwd_mean, lty = lty_mean) # 50% quantile (median)
        lines(forecast_periods, quantiles_list[[j]][1, 1:horizon], col = colour, lty = 2) # Lower quantile
        lines(forecast_periods, quantiles_list[[j]][3, 1:horizon], col = colour, lty = 2) # Upper quantile
        } else{lines(forecast_periods, quantiles_list[[j]][2, 1:horizon], col = colour, lwd = lwd_mean, lty = lty_mean)}
    }
}


## Function to extract coefficients of bias-correction:
extract_coefs_clim_var <- function(ensemble_hist_dat, var_name, Model_list, round_digits = 3){
    lm_bias_coef1 <- c()
    lm_bias_coef2 <- c()
    sd_noise_list <- c()
    for(i in 1:length(Model_list)){
        lm_bias <- lm(sort(ensemble_hist_dat[, var_name]) ~ sort(ensemble_hist_dat[, Model_list[i]]))
        hist_pred <- lm_bias$coefficients[1] + lm_bias$coefficients[2]*ensemble_hist_dat[, Model_list[i]]
        noise_hist <- ensemble_hist_dat[, var_name] - hist_pred 
        sd_noise <- sd(noise_hist)
        lm_bias_coef1[i] <- lm_bias$coefficients[1]
        lm_bias_coef2[i] <- lm_bias$coefficients[2]
        sd_noise_list[i] <- sd_noise
    }
    Model_adjustment_dat <- data.frame(Model_names = Model_list, Bias_coef1 = round(lm_bias_coef1, round_digits), Bias_coef2 = round(lm_bias_coef2, round_digits), Noise_sd = round(sd_noise_list, round_digits))
    return(Model_adjustment_dat)
}

## Function to calculate quantiles of statistics: 
multi_func <- function(x, quantile_low = 0.05, quantile_high = 0.95){
    c(quantile_05 = quantile(x, quantile_low), mean = mean(x), quantile_95 = quantile(x,quantile_high))
}

multi_func_withMed <- function(x, quantile_low = 0.05, quantile_high = 0.95){
    c(quantile_05 = quantile(x, quantile_low), median = median(x), quantile_95 = quantile(x,quantile_high))
}

## Function to find an element in a list of file names: 
find_elements_file <- function(filenames, element_1, element_2) {
  matching_elements <- filenames[sapply(filenames, function(x) {
    grepl(element_1, x) & grepl(element_2, x)
  })]
  return(matching_elements)
}
```




# Import and cleanse of datasets

## Economic data

```{r Import of economic data dataset, warning=FALSE}
############################################################
## Import of population dataset
############################################################
World_pop_raw <- read_excel(pop_dat_path)[, -c(1:3)]
World_pop <- pivot_longer(World_pop_raw, contains("YR"), names_to = "Year", values_to = "Population") %>%
    separate(Year, into = c("Year", "Description"), sep = " ") %>%
    dplyr::select(Year, `Country Code`, Population) %>% rename(ISO = `Country Code`)
World_pop$Year = as.numeric(World_pop$Year)
World_pop$Population = as.numeric(World_pop$Population)
AU_pop <- World_pop %>% filter(ISO == "AUS") %>% dplyr::select(Year, Population)

############################################################
## Import the GDP data:
############################################################

AU_GDP_raw <- read_excel(GDP_dat_path, sheet = "Data")[1:3, ]
AU_consumption_raw <- read_excel(Consumption_dat_path, sheet = "Data")[1, ]

AU_GDP_real <- AU_GDP_raw %>% filter(`Series Name` == "GDP (constant LCU)") %>%
    pivot_longer(cols = contains("YR"), names_to = "Year", values_to = "Real_GDP") %>%
    separate(Year, into = c("Year", "Description"), sep = " ") %>%
    dplyr::select(Year, Real_GDP)

AU_consumption_real <- AU_consumption_raw %>% 
    pivot_longer(cols = contains("YR"), names_to = "Year", values_to = "Real_Consumption") %>%
    separate(Year, into = c("Year", "Description"), sep = " ") %>%
    dplyr::select(Year, Real_Consumption) %>%
    mutate(Consumption_growth = Real_Consumption/lag(Real_Consumption)-1)

AU_GDP_deflator <- AU_GDP_raw %>% filter(`Series Name` == "GDP deflator (base year varies by country)") %>%
    pivot_longer(cols = contains("YR"), names_to = "Year", values_to = "GDP deflator (2022)") %>%
    separate(Year, into = c("Year", "Description"), sep = " ") %>%
    dplyr::select(Year, `GDP deflator (2022)`)
    
AU_GDP_nominal <- AU_GDP_raw %>% filter(`Series Name` == "GDP (current LCU)") %>%
    pivot_longer(cols = contains("YR"), names_to = "Year", values_to = "Nominal_GDP") %>%
    separate(Year, into = c("Year", "Description"), sep = " ") %>%
    dplyr::select(Year,Nominal_GDP)

AU_GDP_annual <- inner_join(AU_GDP_real, AU_GDP_nominal, by = "Year") %>%
  mutate(Real_GDP_growth = Real_GDP/lag(Real_GDP) - 1,
         Nominal_GDP_growth = Nominal_GDP/lag(Nominal_GDP)-1) %>%
    filter(Year > 1960 & Year <= 2023)
AU_GDP_annual$Year = as.numeric(AU_GDP_annual$Year)

############################################################
## Import the inflation data: 
############################################################

Au_inflation <-readxl::read_excel(CPI_dat_path , sheet = "Data1")[-c(1:9), ]
colnames(Au_inflation)[1] <- "Time"
Au_inflation$Time = as.Date(as.numeric(Au_inflation$Time), origin = "1899-12-30")
Au_inflation$`Index Numbers ;  All groups CPI ;  Australia ;` = as.numeric(Au_inflation$`Index Numbers ;  All groups CPI ;  Australia ;`)
### Convert the quarterly inflation data to annually:
Au_inflation_annual <- Au_inflation %>%
  mutate(Year = year(Time),
         Month = month(Time)) %>%
  filter(Month == 12) %>% ## Only select the CPI values at the end of the year
  dplyr::select(Year, Month, `Index Numbers ;  All groups CPI ;  Australia ;`) %>%
  rename(CPI = `Index Numbers ;  All groups CPI ;  Australia ;`) %>%
  mutate(Inflation_rate = (CPI/lag(CPI) - 1))

############################################################
# Import the interest rate data: 
############################################################

AU_short_rates_1 <- read_excel(Interest_dat_path_1, skip = 1)[-c(1:9), ]
AU_short_rates_2 <- read_excel(Interest_dat_path_2, skip = 1)[-c(1:9), ]
colnames(AU_short_rates_1)[1] = "Time"
colnames(AU_short_rates_2)[1] = "Time"

## Cleanse the data: 
AU_short_rates_1$Time = as.Date(as.numeric(AU_short_rates_1$Time), origin = "1899-12-30")
AU_short_rates_2$Time = as.Date(as.numeric(AU_short_rates_2$Time), origin = "1899-12-30")

AU_short_rates_1 <- AU_short_rates_1 %>% dplyr::select(Time, `Interbank Overnight Cash Rate`)
AU_short_rates_2 <- AU_short_rates_2 %>% dplyr::select(Time, `Interbank Overnight Cash Rate`)

AU_short_rates <- na.omit(rbind(AU_short_rates_1, AU_short_rates_2)) %>%
    mutate(Year = year(Time), Month = month(Time)) %>%
    filter(Month == 12) %>%
    group_by(Year, Month) %>%
    summarise(Short_rates = last(`Interbank Overnight Cash Rate`)) ## to be consistent with the long-rate data, we take the reported rate at 31 December as the annually reported short rate
AU_short_rates$Short_rates = as.numeric(AU_short_rates$Short_rates)/100

## Join with the inflation rate data to get the real short rates: 
AU_interest_rates <- left_join(AU_short_rates, Au_inflation_annual, by = c("Year", "Month")) %>%
    mutate(Short_rates_real = Short_rates-Inflation_rate)

############################################################
# Import the potential growth data from World Bank:
############################################################
GDP_potential_growth_AU_raw <- read_excel(potential_growth_dat_path, sheet = "PotentialGrowthMeasures")
GDP_potential_growth_AU <- GDP_potential_growth_AU_raw %>% dplyr::select(Country, Year, `MVF: Potential real GDP growth (percent)`) %>% filter(Country == "Australia") %>% mutate(GDP_potential_growth = `MVF: Potential real GDP growth (percent)`/100) %>% dplyr::select(-c(`MVF: Potential real GDP growth (percent)`))
## Join with the interest rate data:
AU_interest_rates_GDP_potential <- left_join(GDP_potential_growth_AU, AU_interest_rates, by = "Year")
```


## Hazard data

```{r Import and cleanse the ICA data}
############################################################
## Import the raw ICA data
############################################################
CAT_dat_ICA <- readxl::read_excel(ICA_dat_path, skip = 8) 
CAT_dat_ICA_rm <- CAT_dat_ICA %>% filter(CAT_Name != "Undeclared") %>%
    filter(Type %nin% c("Arson", "Earthquake", "Gas Distruption", "Other"))

############################################################
## Join with GDP and CPI data, and derive the normalised losses: 
############################################################
CAT_dat_ICA_ERA5 <- CAT_dat_ICA_rm  %>% left_join(AU_GDP_annual, by = "Year") %>% left_join(Au_inflation_annual, by = "Year") %>% filter(Year <= 2022) %>% rename(CAT_loss_original = `ORIGINAL LOSS VALUE`) %>% dplyr::select(CAT_Name, State, Type, Year, CAT_loss_original, Real_GDP, Nominal_GDP, Real_GDP_growth, Nominal_GDP_growth, Month, CPI, Inflation_rate, CAT_Event_Start) %>% drop_na()
## Find the 2022 CPI and GDP levels:
CPI_2022 <- CAT_dat_ICA_ERA5[CAT_dat_ICA_ERA5$Year == 2022, ]$CPI[1]
Real_GDP_2022 <- CAT_dat_ICA_ERA5[CAT_dat_ICA_ERA5$Year == 2022, ]$Real_GDP[1]
## Calculate normalised losses:
CAT_dat_ICA_ERA5_1 <- CAT_dat_ICA_ERA5 %>% mutate(CAT_loss_normalised_GDP = CAT_loss_original*CPI_2022/CPI*Real_GDP_2022/Real_GDP)
CAT_dat_ICA_ERA5_1 <- CAT_dat_ICA_ERA5_1[order(CAT_dat_ICA_ERA5_1$Year), ]

############################################################
## Derive annual and monthly summarisation of events count and size
############################################################
## Count the events and calculate the total normalised losses:
CAT_dat_ICA_ERA5_aggregate_byType_raw <- CAT_dat_ICA_ERA5_1 %>% group_by(Year, Type) %>% summarise(Normalised_loss_aggregate = sum(CAT_loss_normalised_GDP), CAT_count = n())
## Get all combinations of event types and years:
full_CAT_types <- expand.grid(Year = min(CAT_dat_ICA_ERA5_1$Year, na.rm = T):max(CAT_dat_ICA_ERA5_1$Year, na.rm = T), Type = unique(CAT_dat_ICA_ERA5_1$Type))
## Impute 0 for each event that does not show up in a given year: 
CAT_dat_ICA_ERA5_aggregate_byType <- left_join(full_CAT_types, CAT_dat_ICA_ERA5_aggregate_byType_raw, by = c("Year", "Type")) %>% 
    replace_na(list(CAT_count = 0, Normalised_loss_aggregate = 0))
## Get the monthly count: 
full_CAT_types_ym <- expand.grid(Year = min(CAT_dat_ICA_ERA5_1$Year, na.rm = T):max(CAT_dat_ICA_ERA5_1$Year, na.rm = T), Month = 1:12, Type = unique(CAT_dat_ICA_ERA5_1$Type))
CAT_dat_ICA_ERA5_aggregate_byType_raw_mm <- CAT_dat_ICA_ERA5_1 %>% dplyr::select(-Month) %>% mutate(Month = month(CAT_Event_Start)) %>% group_by(Year, Month, Type) %>% summarise(CAT_count = n())
CAT_dat_ICA_ERA5_aggregate_byType_mm <- left_join(full_CAT_types_ym, CAT_dat_ICA_ERA5_aggregate_byType_raw_mm, by = c("Year","Month", "Type")) %>% 
    replace_na(list(CAT_count = 0))
```

## Climate data

```{r Import Historical data}
############################################################
## Precipitation: 
############################################################
Total_precipitation_ERA5_hist <- read_excel(Prep_hist_dat_path, sheet = "pr")[, -c(1,2)]
rx1day_ERA5_hist <- read_excel(Prep_hist_dat_path, sheet = "rx1day")[, -c(1,2)]
rx5day_ERA5_hist <- read_excel(Prep_hist_dat_path, sheet = "rx5day")[, -c(1,2)]

############################################################
## Fire Weather Index: 
############################################################
## Import the historical FWI data: 
nc_data_FWI_40_49 <- brick(find_elements_file(filenames_FWI, "40", "49"))
nc_data_FWI_50_59 <- brick(find_elements_file(filenames_FWI, "50", "59"))
nc_data_FWI_60_65 <- brick(find_elements_file(filenames_FWI, "60", "65"))
nc_data_FWI_66_75 <- brick(find_elements_file(filenames_FWI, "66", "75"))
nc_data_FWI_76_87 <- brick(find_elements_file(filenames_FWI, "76", "87"))
nc_data_FWI_85_95 <- brick(find_elements_file(filenames_FWI, "85", "95"))
nc_data_FWI_96_05 <-brick(find_elements_file(filenames_FWI, "96", "05"))
nc_data_FWI_06_15 <- brick(find_elements_file(filenames_FWI, "06", "15"))
nc_data_FWI_16_22 <- brick(find_elements_file(filenames_FWI, "16", "22"))

############################################################
## SST and MSLP: 
############################################################
## Define the range of Tropical Cyclone basin: 
Lon_cyc_range <- c(90, 160)
Lat_cyc_range <- c(-40, 0)

ERA5_hist_MSLP_mm_list <- list()
ERA5_hist_SST_mm_list <- list()
## Loop through each file:
for(i in 1:length(filenames_Sea)){
    ERA5_hist_MSLP <- brick(filenames_Sea[i], varname = "msl")
    ERA5_hist_SST <- brick(filenames_Sea[i], varname = "sst")
    ## Store the transformed data: 
    ERA5_hist_MSLP_mm_list[[i]] <-  transform_nc_MSLP_era5_func(ERA5_hist_MSLP, lon_range = Lon_cyc_range, lat_range = Lat_cyc_range, Monthly = T)
    ERA5_hist_SST_mm_list[[i]] <-  transform_nc_SST_era5_func(ERA5_hist_SST, lon_range = Lon_cyc_range, lat_range = Lat_cyc_range, Monthly = T)
}
## Combine the list into data frame:
ERA5_hist_MSLP_mm_data <- rbindlist(ERA5_hist_MSLP_mm_list) 
ERA5_hist_SST_mm_data <- rbindlist(ERA5_hist_SST_mm_list)

############################################################
## Near-surface temperature and air temperature: 
############################################################

if(clean_data_required == T){tas_ERA5_hist_dat <- read.csv("Data/Near-surface temperature/tas_ERA5_hist_dat.csv")}else{tas_ERA5_hist_dat <- transform_nc_tas_mm_func(australia_shape, nc_tas_data_ERA5_hist) %>%
    rename(NS_temperature = Temperature)}
## Get the annual average temperature data:
Temp_hist_dat_ERA5 <- tas_ERA5_hist_dat %>% group_by(Year) %>% summarise(Temperature= mean(NS_temperature))

if(clean_data_required == T){ta_ERA5_hist_dat <- read.csv("Data/Air temperature/ta_ERA5_hist_dat.csv")} else{ta_ERA5_hist_dat <- transform_nc_tas_mm_func(australia_shape, nc_ta_data_ERA5_hist) %>% rename(Air_temperature = Temperature) %>% filter(Year >= 1950 & Year <= 2023)}
```

```{r}
if(clean_data_required == T){tas_ERA5_hist_dat <- read.csv("Data/Near-surface temperature/tas_ERA5_hist_dat.csv")}else{tas_ERA5_hist_dat <- transform_nc_tas_mm_func(australia_shape, nc_tas_data_ERA5_hist) %>%
    rename(NS_temperature = Temperature)}
## Get the annual average temperature data:
Temp_hist_dat_ERA5 <- tas_ERA5_hist_dat %>% group_by(Year) %>% summarise(Temperature= mean(NS_temperature))

if(clean_data_required == T){ta_ERA5_hist_dat <- read.csv("Data/Air temperature/ta_ERA5_hist_dat.csv")} else{ta_ERA5_hist_dat <- transform_nc_tas_mm_func(australia_shape, nc_ta_data_ERA5_hist) %>% rename(Air_temperature = Temperature) %>% filter(Year >= 1950 & Year <= 2023)}
```


## Financial data:

```{r Import of financial data, message=FALSE, warning=FALSE}
############################################################
## Total returns index for All-Ordinaries Share
############################################################
TR_all_ord_shares_yy_data <- read_excel(stock_dat_path, sheet = "Price History", skip = 2) %>% dplyr::select(Date, `Total Return (Gross, Unhedged)`) %>% mutate(Year = year(Date), Month = month(Date)) %>% rename(Total_index = `Total Return (Gross, Unhedged)`) %>% dplyr::select(Year, Month, Total_index) %>%
    group_by(Year, Month) %>%
    summarise(Total_index = last(Total_index)) %>% ## Find the price index at each month end
    filter(Month == 12) %>% left_join(AU_interest_rates, by = c("Year", "Month")) %>% ungroup() %>%
    mutate(Total_return_raw = Total_index/lag(Total_index)-1) %>%
    mutate(Total_return = Total_index/lag(Total_index)-1-Short_rates) %>% drop_na()
## Join with real GDP and consumption data: 
TR_all_ord_shares_yy_data_1 <- left_join(TR_all_ord_shares_yy_data, AU_GDP_annual, by = "Year")
AU_consumption_real$Year = as.numeric(AU_consumption_real$Year)
TR_all_ord_shares_yy_data_2 <- left_join(TR_all_ord_shares_yy_data, AU_consumption_real, by = "Year") 
TR_all_ord_shares_yy_data_3 <- TR_all_ord_shares_yy_data_2 %>% filter(Year != 2020)


############################################################
## Import the corporate earning data
############################################################
AU_corporates_dat_raw <- readxl::read_excel(corporate_earnings_path, sheet = "Data1")[-c(1:9), ]
colnames(AU_corporates_dat_raw)[1] <- "Time"
AU_corporates_dat_raw$Time = as.Date(as.numeric(AU_corporates_dat_raw$Time), origin = "1899-12-30")
AU_corporates_dat_raw$`Gross Operating Profits ;  Total (State) ;  Total (Industry) ;  Current Price ;  CORP ;...49` = as.numeric(AU_corporates_dat_raw$`Gross Operating Profits ;  Total (State) ;  Total (Industry) ;  Current Price ;  CORP ;...49`)*1000000
### Cleanse the data: 
AU_corporates_dat_raw <- AU_corporates_dat_raw %>%
    dplyr::select(Time, `Gross Operating Profits ;  Total (State) ;  Total (Industry) ;  Current Price ;  CORP ;...49`) %>%
    rename(Gross_operating_profits = `Gross Operating Profits ;  Total (State) ;  Total (Industry) ;  Current Price ;  CORP ;...49`) %>% drop_na()

############################################################
## Import the planned oil and gas production data: 
############################################################

## Import the gas production data: 
OECD_Gas_SSP <- read_excel(gas_production_dat_path, sheet = "data")[1:5, 1:16] %>%
    pivot_longer(cols = starts_with("2"), names_to = "Year", values_to = "Gas.Production") %>% 
    dplyr::select(Scenario, Year, Gas.Production) 
## Import the oil production data: 
OECD_Oil_SSP <- read_excel(oil_production_dat_path, sheet = "data")[1:5, 1:16] %>%
     pivot_longer(cols = starts_with("2"), names_to = "Year", values_to = "Oil.Production") %>% 
    dplyr::select(Scenario, Year, Oil.Production)
## Join the two dataset: 
OECD_Oil_Gas_SSP <- left_join(OECD_Gas_SSP, OECD_Oil_SSP,  by = c("Scenario", "Year")) %>%
    mutate(Value_sum = Gas.Production+Oil.Production)
OECD_Oil_Gas_SSP$Year = as.numeric(OECD_Oil_Gas_SSP$Year)
```


## Market statistics:


```{r Import Market statistics}
############################################################
## GI statistics:
############################################################
GI_development_statistics_database <-read_excel(GI_dev_stats_input, sheet = "Data")

## Select the Property and motor claims cost:
GI_development_statistics_PropertyandMotor <- GI_development_statistics_database %>%
    filter(`Class of business` %in% c("Houseowners and householders", "Commercial motor vehicle",
                                      "Domestic motor vehicle", "Compulsory third party (CTP) motor vehicle", "Fire and industrial special risks (ISR)"), `Data Item` %in% c("Gross ultimate cost (inflated and undiscounted) by class of business","Number of claims reported by class of business", "Number of risks in force by class of business"))

## Also derive the other claims cost:
GI_development_statistics_other <- GI_development_statistics_database %>%
    filter(`Class of business` %nin% c("Houseowners and householders", "Commercial motor vehicle",
                                      "Domestic motor vehicle", "Compulsory third party (CTP) motor vehicle", "Fire and industrial special risks (ISR)"), `Data Item` %in% c("Gross ultimate cost (inflated and undiscounted) by class of business","Number of claims reported by class of business", "Number of risks in force by class of business"))
```



# Section 3.1 Data and calibration

This section calibrates the model parameters based on historical data as per Section 3.1 in the paper. 

## Hazard module

Test the distribution assumption for CAT losses:

- This produces the Table C.1 in Appendix C


```{r Test distribution assumptions for normalised CAT losses, message=FALSE, warning=FALSE}
fit_LN_Norm_loss <- summary(fitdist(data = CAT_dat_ICA_ERA5_1$CAT_loss_normalised_GDP, distr = "lnorm", method = "mle"))
fit_weibull_Norm_loss <- summary(fitdist(data = CAT_dat_ICA_ERA5_1$CAT_loss_normalised_GDP, distr = "weibull", method = "mle"))
fit_pareto_Norm_loss <- vglm(CAT_dat_ICA_ERA5_1$CAT_loss_normalised_GDP ~ 1, family = paretoII)
#fit_lgamma_Norm_loss <- summary(fitdist(data = CAT_dat_ICA_ERA5_1$CAT_loss_normalised_GDP, distr = "lgamma", method = "mle"))
fit_cauchy_Norm_loss <- summary(fitdist(data = CAT_dat_ICA_ERA5_1$CAT_loss_normalised_GDP, distr = "cauchy", method = "mle"))
## Summarise the performance:
distribution_fit_summary_dat <- data.frame(Distributions = c("Log-Normal", "Weibull", "Pareto", "Cauchy"), AIC = c(fit_LN_Norm_loss$aic, fit_weibull_Norm_loss$aic, AICvlm(fit_pareto_Norm_loss, k = 2), fit_cauchy_Norm_loss$aic), BIC = c(fit_LN_Norm_loss$aic, fit_weibull_Norm_loss$bic, AICvlm(fit_pareto_Norm_loss, k = log(length(CAT_dat_ICA_ERA5_1$CAT_loss_normalised_GDP))),  fit_cauchy_Norm_loss$bic)) %>% mutate(AIC_rank = rank(AIC), BIC_rank = rank(BIC))
distribution_fit_summary_dat
#write.csv(distribution_fit_summary_dat, "Plotting/Hazards/Distribution_fit_summary_data.csv")
```


### Flood

Join the historical precipitation information data to the flood losses dataset: 

```{r Join with flood loss data}
## Store in a dataset:
Precipitation_ERA5_hist_data <- data.frame(Year = as.numeric(substr(names(Total_precipitation_ERA5_hist), start = 1, stop = 4)), Precipitation = as.numeric(Total_precipitation_ERA5_hist),
                                           rx1day = as.numeric(rx1day_ERA5_hist),
                                           rx5day = as.numeric(rx5day_ERA5_hist))
CAT_food_ICA_loss <- CAT_dat_ICA_ERA5_1 %>% filter(Type == "Flooding") %>% left_join(Precipitation_ERA5_hist_data, by = "Year")
CAT_food_ICA_count <- CAT_dat_ICA_ERA5_aggregate_byType %>% filter(Type == "Flooding") %>% left_join(Precipitation_ERA5_hist_data, by = "Year")
```

#### Flood frequency

Fit different flood frequency models with various covariates:

- The tables shown below contribute to Table 3.1 and produce Table B in Appendix C.2. 

```{r Varioable selection Flood frequency}
### Fit Poisson regression on hazard count and all possible combinations of variables:
glm_Flood_freq_1 <- glm(CAT_count ~ Precipitation, family = poisson(link = "log"), data = CAT_food_ICA_count)
glm_Flood_freq_2 <- glm(CAT_count ~ rx1day, family = poisson(link = "log"), data = CAT_food_ICA_count)
glm_Flood_freq_3 <- glm(CAT_count ~ rx5day, family = poisson(link = "log"), data = CAT_food_ICA_count)
glm_Flood_freq_4 <- glm(CAT_count ~ Precipitation + rx1day, family = poisson(link = "log"), data = CAT_food_ICA_count)
glm_Flood_freq_5 <- glm(CAT_count ~ Precipitation + rx5day, family = poisson(link = "log"), data = CAT_food_ICA_count)
glm_Flood_freq_6 <- glm(CAT_count ~ rx1day + rx5day, family = poisson(link = "log"), data = CAT_food_ICA_count)

summary_flood_freq_model_data <- data.frame(Model = 1:6, Covariates = c("Precipitation", "rx1day","rx5day",
                                                                   "Precipitation + rx1day",
                                                                   "Precipitation + rx5day",
                                                                   "rx1day + rx5day"),
                                       AIC = c(glm_Flood_freq_1$aic, glm_Flood_freq_2$aic, glm_Flood_freq_3$aic, glm_Flood_freq_4$aic, glm_Flood_freq_5$aic, glm_Flood_freq_6$aic),
           BIC = c(BIC(glm_Flood_freq_1),
                   BIC(glm_Flood_freq_2),
                   BIC(glm_Flood_freq_3),
                   BIC(glm_Flood_freq_4),
                   BIC(glm_Flood_freq_5),
                   BIC(glm_Flood_freq_6))) %>% mutate(Rank_AIC = rank(AIC), Rank_BIC = rank(BIC))

summary_flood_freq_model_data 
```

```{r Model summary of flood frequency model}
modelsummary(list("Model 1" = glm_Flood_freq_1, "Model 2" = glm_Flood_freq_2, "Model 3*" = glm_Flood_freq_3, "Model 4" = glm_Flood_freq_4, "Model 5" = glm_Flood_freq_5, "Model 6" = glm_Flood_freq_6), statistic = "p.value", stars = TRUE, gof_omit = "Num.Obs.|RMSE")
```

#### Flood severity


As a double check, the Log-Normal distribution still shows the best goodness-of-fit for the flood severity data based on both AIC and BIC values: 

```{r Test distribution assumptions for flood severity}
## Fit other distribution assumptions:
fit_LN_Norm_loss_FL <- summary(fitdist(data = CAT_food_ICA_loss$CAT_loss_normalised_GDP, distr = "lnorm", method = "mle"))
fit_weibull_Norm_loss_FL <- summary(fitdist(data = CAT_food_ICA_loss$CAT_loss_normalised_GDP, distr = "weibull", method = "mle"))
fit_pareto_Norm_loss_FL <- vglm(CAT_food_ICA_loss$CAT_loss_normalised_GDP ~ 1, family = paretoII)
## Summarise the performance:
data.frame(Distributions = c("Log-Normal", "Weibull", "Pareto"), AIC = c(fit_LN_Norm_loss_FL$aic, fit_weibull_Norm_loss_FL$aic, AICvlm(fit_pareto_Norm_loss_FL, k = 2)), BIC = c(fit_LN_Norm_loss_FL$aic, fit_weibull_Norm_loss_FL$bic, AICvlm(fit_pareto_Norm_loss_FL, k = log(length(CAT_food_ICA_loss$CAT_loss_normalised_GDP))))) %>% mutate(AIC_rank = rank(AIC), BIC_rank = rank(BIC))
```

We fit a Log-Normal model to the flood severity based on different combination of precipitation covariates:

- The tables shown below contribute to Table 3.1 and produce Table C in Appendix C.2. 

```{r Varaioable selection Flood severity}
### Fit Log-Normal model on normalised losses:
glm_Flood_sev_1 <- gamlss(formula =  CAT_loss_normalised_GDP ~ Precipitation, data = CAT_food_ICA_loss, family = LOGNO(mu.link="identity", sigma.link="log"), trace = FALSE)
glm_Flood_sev_2 <- gamlss(formula =  CAT_loss_normalised_GDP ~ rx1day, data = CAT_food_ICA_loss, family = LOGNO(mu.link="identity", sigma.link="log"), trace = FALSE)
glm_Flood_sev_3 <- gamlss(formula =  CAT_loss_normalised_GDP ~ rx5day, data = CAT_food_ICA_loss, family = LOGNO(mu.link="identity", sigma.link="log"), trace = FALSE)
glm_Flood_sev_4 <- gamlss(formula =  CAT_loss_normalised_GDP ~ Precipitation + rx1day, data = CAT_food_ICA_loss, family = LOGNO(mu.link="identity", sigma.link="log"), trace = FALSE)
glm_Flood_sev_5 <- gamlss(formula =  CAT_loss_normalised_GDP ~ Precipitation + rx5day, data = CAT_food_ICA_loss, family = LOGNO(mu.link="identity", sigma.link="log"), trace = FALSE)
glm_Flood_sev_6 <- gamlss(formula =  CAT_loss_normalised_GDP ~ rx1day + rx5day, data = CAT_food_ICA_loss, family = LOGNO(mu.link="identity", sigma.link="log"), trace = FALSE)

## summary the model performance: 
summary_flood_sev_model_data <- data.frame(Model = 1:6, Covariates = c("Precipitation", "rx1day","rx5day",
                                                                   "Precipitation + rx1day",
                                                                   "Precipitation + rx5day",
                                                                   "rx1day + rx5day"),
                                       AIC = c(glm_Flood_sev_1$aic, glm_Flood_sev_2$aic, glm_Flood_sev_3$aic, glm_Flood_sev_4$aic, glm_Flood_sev_5$aic, glm_Flood_sev_6$aic),
           BIC = c(BIC(glm_Flood_sev_1),
                   BIC(glm_Flood_sev_2),
                   BIC(glm_Flood_sev_3),
                   BIC(glm_Flood_sev_4),
                   BIC(glm_Flood_sev_5),
                   BIC(glm_Flood_sev_6))) %>% mutate(Rank_AIC = rank(AIC), Rank_BIC = rank(BIC))

summary_flood_sev_model_data
```


```{r Model summary of flood severity model}
modelsummary(list("Model 1" = glm_Flood_sev_1, "Model 2" = glm_Flood_sev_2, "Model 3*" = glm_Flood_sev_3, "Model 4" = glm_Flood_sev_4, "Model 5" = glm_Flood_sev_5, "Model 6" = glm_Flood_sev_6), statistic = "p.value", stars = TRUE, coef_omit = "Intercept", gof_omit = "Num.Obs.|RMSE")
```



### Bushfire

Transform the FWI data from `nc` to `data frame` format: 

```{r Transform the FWI nc data to data frame format}
if(clean_data_required == T){FWI_hist_data <- read.csv("Data/FWI/FWI_hist_data.csv")} else{
    FWI_40_49_data <- transform_nc_FWI_func(australia_shape, nc_data_FWI_40_49)
    FWI_50_59_data <- transform_nc_FWI_func(australia_shape, nc_data_FWI_50_59)
    FWI_60_65_data <- transform_nc_FWI_func(australia_shape, nc_data_FWI_60_65)
    FWI_66_75_data <- transform_nc_FWI_func(australia_shape, nc_data_FWI_66_75)
    FWI_76_87_data <- transform_nc_FWI_func(australia_shape, nc_data_FWI_76_87)
    FWI_85_95_data <- transform_nc_FWI_func(australia_shape, nc_data_FWI_85_95)
    FWI_96_05_data <- transform_nc_FWI_func(australia_shape, nc_data_FWI_96_05)
    FWI_06_15_data <- transform_nc_FWI_func(australia_shape, nc_data_FWI_06_15)
    FWI_16_22_data <- transform_nc_FWI_func(australia_shape, nc_data_FWI_16_22)
    FWI_hist_data <- rbind(FWI_40_49_data, FWI_50_59_data, FWI_60_65_data, FWI_66_75_data, FWI_76_87_data, FWI_85_95_data, FWI_96_05_data, FWI_06_15_data, FWI_16_22_data) %>% rename(Lat = x, Lon = y)
}
```


We use the following statistic derived from raw FWI data for bushfire modelling: 

- Extreme value of the FWI (`fwixx`): annual maximum value of the FWI

Since our focus is on the frequency and severity of bushfire risk at the national level, rather than on a gridded basis as in some studies, we are exploring both mean and maximum aggregations of the FWI index. This approach yields two statistics:

- Mean extreme value of the FWI (`mfwixx`): average of  `fwixx` across all grided cells in Australia
- Maximum extreme value of the FWI (`xfwixx`): maximum of `fwixx` across all grided cells in Australia

Summarise and calculate the annual indicators of FWI: 

```{r Calculate the FWI statistics}
FWI_hist_stats <- FWI_hist_data %>% group_by(Lat, Lon, Year) %>% summarise(fwixx = max(FWI))

FWI_hist_stats_sum <- FWI_hist_stats %>% group_by(Year) %>% summarise(mfwixx = mean(fwixx),
                                                                      xfwixx = max(fwixx))
```

Join FWI with Bushfire loss data: 

```{r Join FWI with Bushfire loss data}
CAT_Bushfire_ICA_loss <- CAT_dat_ICA_ERA5_1 %>% filter(Type == "Bushfire") %>% left_join(FWI_hist_stats_sum, by = "Year")
CAT_Bushfire_ICA_count <- CAT_dat_ICA_ERA5_aggregate_byType %>% filter(Type == "Bushfire") %>% left_join(FWI_hist_stats_sum, by = "Year")
```




#### Bushfire frequency

We fit a Poisson GLM to the bushfire frequency based on different combination of FWI statistics: 


```{r Build bushfire frequency model}
glm_Bush_freq_1 <- glm(CAT_count ~ mfwixx, data = CAT_Bushfire_ICA_count, family = poisson(link = "log"))
glm_Bush_freq_2 <- glm(CAT_count ~ xfwixx, data = CAT_Bushfire_ICA_count, family = poisson(link = "log"))
glm_Bush_freq_3 <- glm(CAT_count ~ mfwixx + xfwixx, data = CAT_Bushfire_ICA_count, family = poisson(link = "log"))

summary_Bush_freq_model_data <- data.frame(Model = 1:3, Covariates = c("mfwixx", "xfwixx","mfwixx + xfwixx"),
                                       AIC = c(glm_Bush_freq_1$aic, glm_Bush_freq_2$aic, glm_Bush_freq_3$aic),
           BIC = c(BIC(glm_Bush_freq_1),
                   BIC(glm_Bush_freq_2),
                   BIC(glm_Bush_freq_3))) %>% mutate(Rank_AIC = rank(AIC), Rank_BIC = rank(BIC))

summary_Bush_freq_model_data
```



#### Bushfire severity

First we test the distribution assumptions on the normalised per-event severity:

- The Log-Normal distribution still has the best goodness-of-fit based on both AIC and BIC values.

```{r Test distribution assumptions for bushfire severity}
## Fit other distribution assumptions:
fit_LN_Norm_loss_BF <- summary(fitdist(data = CAT_Bushfire_ICA_loss$CAT_loss_normalised_GDP, distr = "lnorm", method = "mle"))
fit_weibull_Norm_loss_BF <- summary(fitdist(data = CAT_Bushfire_ICA_loss$CAT_loss_normalised_GDP, distr = "weibull", method = "mle"))
fit_pareto_Norm_loss_BF <- vglm(CAT_Bushfire_ICA_loss$CAT_loss_normalised_GDP ~ 1, family = paretoII)
## Summarise the performance:
data.frame(Distributions = c("Log-Normal", "Weibull", "Pareto"), AIC = c(fit_LN_Norm_loss_BF$aic, fit_weibull_Norm_loss_BF$aic, AICvlm(fit_pareto_Norm_loss_BF, k = 2)), BIC = c(fit_LN_Norm_loss_BF$aic, fit_weibull_Norm_loss_BF$bic, AICvlm(fit_pareto_Norm_loss_BF, k = log(length(CAT_Bushfire_ICA_loss$CAT_loss_normalised_GDP))))) %>% mutate(AIC_rank = rank(AIC), BIC_rank = rank(BIC))
```


We fit a Log-Normal GLM to the bushfire severity based on different combination of FWI statistics: 


```{r Build bushfire severity model}
glm_Bush_sev_0 <- gamlss(formula =  CAT_loss_normalised_GDP ~ 1, data = CAT_Bushfire_ICA_loss, family = LOGNO(mu.link="identity", sigma.link="log"), trace = FALSE)
glm_Bush_sev_1 <- gamlss(formula =  CAT_loss_normalised_GDP ~ mfwixx, data = CAT_Bushfire_ICA_loss, family = LOGNO(mu.link="identity", sigma.link="log"), trace = FALSE)
glm_Bush_sev_2 <- gamlss(formula =  CAT_loss_normalised_GDP ~ xfwixx, data = CAT_Bushfire_ICA_loss, family = LOGNO(mu.link="identity", sigma.link="log"), trace = FALSE)
glm_Bush_sev_3 <- gamlss(formula =  CAT_loss_normalised_GDP ~ xfwixx + mfwixx, data = CAT_Bushfire_ICA_loss, family = LOGNO(mu.link="identity", sigma.link="log"), trace = FALSE)

## Summary of the severity model performance: 
summary_Bush_sev_model_data <- data.frame(Model = 0:3, Covariates = c("Stationary","mfwixx", "xfwixx","mfwixx + xfwixx"),
                                       AIC = c(glm_Bush_sev_0$aic, glm_Bush_sev_1$aic, glm_Bush_sev_2$aic, glm_Bush_sev_3$aic),
           BIC = c(BIC(glm_Bush_sev_0), 
                   BIC(glm_Bush_sev_1),
                   BIC(glm_Bush_sev_2),
                   BIC(glm_Bush_sev_3))) %>% mutate(Rank_AIC = rank(AIC), Rank_BIC = rank(BIC))

summary_Bush_sev_model_data
```


#### Other exploration: Duration of extreme fire weather

To explore the impact of duration of extreme fire weather on bushfire risk, we also calculate: 

- Number of days with extreme fire weather (`fwixd`): annual number of days above the threshold defined as the $95^{th}$ percentile of the FWI over the reference period.

```{r Transform the FWI nc data in reference period to data frame format}
if(clean_data_required == T){FWI_ref_data <- read.csv("Data/FWI/FWI_ref_data.csv")} else{FWI_ref_data <- rbind(FWI_40_49_data, FWI_50_59_data, FWI_60_65_data ) %>% rename(Lat = x, Lon = y)}
```

Calculate the $95^{th}$ percentile of the FWI over the reference period:

```{r Calculate 95 quantile of FWI in the reference period}
FWI_ref_q95 <- FWI_ref_data %>% group_by(Lat, Lon) %>% summarise(FWI_q95 = quantile(FWI, 0.95))
```

```{r Calculate fwixd}
FWI_hist_ref_stats <- left_join(FWI_hist_data, FWI_ref_q95, by = c("Lat", "Lon")) %>% 
    group_by(Lat, Lon, Year) %>%
    summarise(fwixd = sum(FWI >= FWI_q95))
FWI_q95_stats <- FWI_hist_ref_stats %>% group_by(Year) %>% summarise(xfwixd = max(fwixd), mfwixd = mean(fwixd))
```


```{r Join with the loss and count data fwixd}
CAT_Bushfire_ICA_loss <- left_join(CAT_Bushfire_ICA_loss, FWI_q95_stats, by = "Year")
CAT_Bushfire_ICA_count_1 <- left_join(CAT_Bushfire_ICA_count, FWI_q95_stats, by = "Year")
```

Refit frequency model based on ``fwixd`` statistics: 

```{r Refit frequency model based on fwixd statistics}
glm_Bush_freq_4 <- glm(formula = CAT_count ~ mfwixd, family = poisson(link = "log"), 
    data = CAT_Bushfire_ICA_count_1)
glm_Bush_freq_5 <- glm(formula = CAT_count ~ xfwixd, family = poisson(link = "log"), 
    data = CAT_Bushfire_ICA_count_1)
glm_Bush_freq_6 <- glm(formula = CAT_count ~ xfwixd + mfwixd, family = poisson(link = "log"), 
    data = CAT_Bushfire_ICA_count_1)
```

Refit severity model based on ``fwixd`` statistics: 

```{r Refit severity model based on fwixd statistics}
glm_Bush_sev_4 <- gamlss(formula =  CAT_loss_normalised_GDP ~ mfwixd, data = CAT_Bushfire_ICA_loss, family = LOGNO(mu.link="identity", sigma.link="log"), trace = FALSE)
glm_Bush_sev_5 <- gamlss(formula =  CAT_loss_normalised_GDP ~ xfwixd, data = CAT_Bushfire_ICA_loss, family = LOGNO(mu.link="identity", sigma.link="log"), trace = FALSE)
glm_Bush_sev_6 <- gamlss(formula =  CAT_loss_normalised_GDP ~ xfwixd + mfwixd, data = CAT_Bushfire_ICA_loss, family = LOGNO(mu.link="identity", sigma.link="log"), trace = FALSE)
```

Compare with the other models:

```{r}
summary_Bush_sev_model_data_2 <- data.frame(Model = 0:6, Covariates = c("Stationary","mfwixx", "xfwixx","mfwixx + xfwixx", "mfwixd", "xfwixd", "xfwixd + mfwixd"),
                                       AIC = c(glm_Bush_sev_0$aic, glm_Bush_sev_1$aic, glm_Bush_sev_2$aic, glm_Bush_sev_3$aic, glm_Bush_sev_4$aic, glm_Bush_sev_5$aic, glm_Bush_sev_6$aic),
           BIC = c(BIC(glm_Bush_sev_0), 
                   BIC(glm_Bush_sev_1),
                   BIC(glm_Bush_sev_2),
                   BIC(glm_Bush_sev_3),
                   BIC(glm_Bush_sev_4),
                   BIC(glm_Bush_sev_5),
                   BIC(glm_Bush_sev_6))) %>% mutate(Rank_AIC = rank(AIC), Rank_BIC = rank(BIC))

summary_Bush_sev_model_data_2
```

#### Bushfire model summary

The following results generate Table D and E in Appendix C.3, and contribute to the Table 3.1:

```{r Model summary of Bushfire frequency model}
modelsummary(list("Model 1*" = glm_Bush_freq_1, "Model 2" = glm_Bush_freq_2, "Model 3" = glm_Bush_freq_3, "Model 4" = glm_Bush_freq_4, "Model 5" = glm_Bush_freq_5, "Model 6" = glm_Bush_freq_6), statistic = "p.value", stars = TRUE, coef_omit = "Intercept", gof_omit = "RMSE|Num.Obs.")
```

```{r Model summary of Bushfire severity model}
modelsummary(list("Model 1" = glm_Bush_sev_1, "Model 2" = glm_Bush_sev_2, "Model 3" = glm_Bush_sev_3, "Model 4" = glm_Bush_sev_4, "Model 5" = glm_Bush_sev_5, "Model 6" = glm_Bush_sev_6), statistic = "p.value", stars = TRUE, coef_omit = "Intercept", gof_omit = "RMSE|Num.Obs.")
```

### Tropical cyclone


Check the data coverage:

```{r Check the data coverage of MSLP and SST}
ERA5_hist_SST <- brick(filenames_Sea[1], varname = "sst")
SST_dat <- as.data.frame(ERA5_hist_SST, xy = TRUE) %>% filter(x>=Lon_cyc_range[1] & x<=Lon_cyc_range[2]) %>% filter(y>=Lat_cyc_range[1] & y<=Lat_cyc_range[2])
#pdf("Plotting/Climate/SST_AU_basin.pdf")
## Plot SST:
ggplot(SST_dat, aes(x, y, fill = X2006.01.01)) + 
  geom_raster() + 
  coord_fixed(expand = FALSE) + scale_fill_gradient(low = "yellow",
  high = "red") + labs(x = "Longtitudes", y = "Latitudes") + ggtitle("Snapshot of Sea Surface Temperature (in Kelvin's)") + guides(fill=guide_legend(title="Temperature (in Kelvin's)"))
#dev.off()
```

Join with Cyclone loss data to obtain monthly level data: 

```{r Join with Cyclone loss data to obtain monthly level data}
CAT_Cyclone_ICA_loss_mm <- CAT_dat_ICA_ERA5_1 %>% filter(Type == "Cyclone") %>% dplyr::select(-Month) %>% mutate(Month = month(CAT_Event_Start)) %>% left_join(ERA5_hist_SST_mm_data, by = c("Year", "Month")) %>% left_join(ERA5_hist_MSLP_mm_data, by = c("Year", "Month"))

CAT_Cyclone_ICA_count_mm <- CAT_dat_ICA_ERA5_aggregate_byType_mm %>% filter(Type == "Cyclone") %>% left_join(ERA5_hist_SST_mm_data , by = c("Year", "Month")) %>% left_join(ERA5_hist_MSLP_mm_data, by = c("Year", "Month"))
## Derive the annual count: 
CAT_Cyclone_ICA_count <- CAT_dat_ICA_ERA5_aggregate_byType %>% filter(Type == "Cyclone") 
```



#### TC frequency

Following a similar approach, we fitted a Poisson GLM to monthly cyclone frequency using various combinations of covariates:

```{r Monthly cyclone frequency}
glm_Cyc_mm_freq_1 <- glm(CAT_count ~ SST_mean_mm, data = CAT_Cyclone_ICA_count_mm, family = poisson(link = "log"))
glm_Cyc_mm_freq_2 <- glm(CAT_count ~ MSLP_mm, data = CAT_Cyclone_ICA_count_mm, family = poisson(link = "log"))
glm_Cyc_mm_freq_3 <- glm(CAT_count ~ SST_mean_mm + MSLP_mm, data = CAT_Cyclone_ICA_count_mm, family = poisson(link = "log"))

summary_Cyc_mm_perf <- data.frame(Models = 1:3, Covariates = c("SST (Monthly average)", "MSLP (Monthly average)", "SST+MSLP"), AIC = c(glm_Cyc_mm_freq_1$aic, glm_Cyc_mm_freq_2$aic, glm_Cyc_mm_freq_3$aic), BIC = c(BIC(glm_Cyc_mm_freq_1), BIC(glm_Cyc_mm_freq_2), BIC(glm_Cyc_mm_freq_3))) %>% mutate(Rank_AIC = rank(AIC), Rank_BIC = rank(BIC))
summary_Cyc_mm_perf
```


#### TC severity

Based on the non-linear relationship observed between normalised severity and monthly average SST (and also between normalised severity and monthly average MSLP), the following models are fitted:

- Model 1: $Severity \sim SST+SST^2$
- Model 2: $Severity \sim MSLP+MSLP^2$
- Model 3: $Severity \sim cs(SST)$, where cs denotes cubic spline
- Model 4: $Severity \sim cs(SST) + cs(MSLP)$

- Among the four models defined above, Model 1 has the best performance based on both AIC and BIC. However, it still does not out-perform the model with stationary assumption

```{r Monthly cyclone severity modelling}
glm_Cyc_sev_0 <- gamlss(formula =  CAT_loss_normalised_GDP ~ 1, data = CAT_Cyclone_ICA_loss_mm, family = LOGNO(mu.link="identity", sigma.link="log"), trace = FALSE)
glm_Cyc_sev_mm_1 <- gamlss(formula =  CAT_loss_normalised_GDP ~ SST_mean_mm + I(SST_mean_mm^2), data = CAT_Cyclone_ICA_loss_mm, family = LOGNO(mu.link="identity", sigma.link="log"), trace = FALSE)
glm_Cyc_sev_mm_2 <- gamlss(formula =  CAT_loss_normalised_GDP ~ MSLP_mm + I(MSLP_mm^2), data = CAT_Cyclone_ICA_loss_mm, family = LOGNO(mu.link="identity", sigma.link="log"), trace = FALSE)
glm_Cyc_sev_mm_3 <- gamlss(formula =  CAT_loss_normalised_GDP ~ cs(SST_mean_mm, 3), data = CAT_Cyclone_ICA_loss_mm, family = LOGNO(mu.link="identity", sigma.link="log"), trace = FALSE)
glm_Cyc_sev_mm_4 <- gamlss(formula =  CAT_loss_normalised_GDP ~ cs(SST_mean_mm, 3) + cs(MSLP_mm, 3), data = CAT_Cyclone_ICA_loss_mm, family = LOGNO(mu.link="identity", sigma.link="log"), trace = FALSE)

summary_Cyc_sev_MM_model_perf <- data.frame(Models = 0:4, Covaritaes = c("Stationary", "SST+SST^2", "MSLP+MSLP^2", "cs(SST)", "cs(SST)+cs(MSLP)"), AIC = c(glm_Cyc_sev_0$aic, glm_Cyc_sev_mm_1$aic, glm_Cyc_sev_mm_2$aic, glm_Cyc_sev_mm_3$aic, glm_Cyc_sev_mm_4$aic), BIC = c(BIC(glm_Cyc_sev_0), BIC(glm_Cyc_sev_mm_1), BIC(glm_Cyc_sev_mm_2), BIC(glm_Cyc_sev_mm_3), BIC(glm_Cyc_sev_mm_4))) %>% mutate(Rank_AIC = rank(AIC)) %>% mutate(Rank_BIC = rank(BIC))
summary_Cyc_sev_MM_model_perf
```


#### TC model summary

The following results generate Table F and G in Appendix C.4, and contribute to the Table 3.1:

```{r Model summary of Cyclone frequency model}
modelsummary(list("Model 1" = glm_Cyc_mm_freq_1, "Model 2" = glm_Cyc_mm_freq_2, "Model 3*" = glm_Cyc_mm_freq_3), statistic = "p.value", stars = TRUE,  gof_omit = "RMSE|Num.Obs.", coef_rename = c("Intercept","SST","MSLP"))
```

```{r Model summary of Cyclone severity model}
modelsummary(list("Model 1" = glm_Cyc_sev_mm_1, "Model 2" = glm_Cyc_sev_mm_2, "Model 3" = glm_Cyc_sev_mm_3, "Model 4" = glm_Cyc_sev_mm_4), coef_rename = c("SST","SST^2", "MSLP", "MSLP^2", "cs(SST)", "cs(MSLP)"), statistic = "p.value", stars = TRUE, coef_omit = "Intercept", gof_omit = "RMSE|Num.Obs.")
```


### Storm

- As discussed in the preliminary in the previous section, both storm and cyclone formulation are driven by the same climate drivers, though their intensity tends to be different
- Therefore, we will also employ SST and MSLP as the covariates to model storm frequency and severity in this section

First we join the storm loss data to the historical monthly average of SST and MSLP data cleansed in the previous section: 

```{r Join with Storm loss data to obtain monthly level data}
CAT_Storm_ICA_loss_mm <- CAT_dat_ICA_ERA5_1 %>% filter(Type == "Storm"|Type == "Storm Flooding") %>% dplyr::select(-Month) %>% mutate(Month = month(CAT_Event_Start)) %>% left_join(ERA5_hist_SST_mm_data, by = c("Year", "Month")) %>% left_join(ERA5_hist_MSLP_mm_data, by = c("Year", "Month"))

CAT_Storm_ICA_count_mm <- CAT_dat_ICA_ERA5_aggregate_byType_mm %>% filter(Type == "Storm"|Type == "Storm Flooding") %>% left_join(ERA5_hist_SST_mm_data , by = c("Year", "Month")) %>% left_join(ERA5_hist_MSLP_mm_data, by = c("Year", "Month"))

CAT_Storm_ICA_count <- CAT_dat_ICA_ERA5_aggregate_byType %>%filter(Type == "Storm"|Type == "Storm Flooding")
```


#### Storm frequency

Similarly, we fit a Poission GLM on the monthly count of storms as a function of different combinations of covariates:

```{r Monthly strom frequency}
glm_Storm_mm_freq_1 <- glm(CAT_count ~ SST_mean_mm, data = CAT_Storm_ICA_count_mm, family = poisson(link = "log"))
glm_Storm_mm_freq_2 <- glm(CAT_count ~ MSLP_mm, data = CAT_Storm_ICA_count_mm, family = poisson(link = "log"))
glm_Storm_mm_freq_3 <- glm(CAT_count ~ SST_mean_mm + MSLP_mm, data = CAT_Storm_ICA_count_mm, family = poisson(link = "log"))

summary_Storm_mm_perf <- data.frame(Models = 1:3, Covariates = c("SST (Monthly average)", "MSLP (Monthly average)", "SST+MSLP"), AIC = c(glm_Storm_mm_freq_1$aic, glm_Storm_mm_freq_2$aic, glm_Storm_mm_freq_3$aic), BIC = c(BIC(glm_Storm_mm_freq_1), BIC(glm_Storm_mm_freq_2), BIC(glm_Storm_mm_freq_3))) %>% mutate(Rank_AIC = rank(AIC), Rank_BIC = rank(BIC))
summary_Storm_mm_perf
```


#### Storm severity

Fit a Log-Normal model on normalised storm severity with different combination of covariates:

```{r Monthly storm severity modelling}
## Fit different storm severity models: 
glm_Storm_sev_0 <- gamlss(formula =  CAT_loss_normalised_GDP ~ 1, data = CAT_Storm_ICA_loss_mm, family = LOGNO(mu.link="identity", sigma.link="log"), trace = FALSE)
glm_Storm_sev_mm_1 <- gamlss(formula =  CAT_loss_normalised_GDP ~ SST_mean_mm, data = CAT_Storm_ICA_loss_mm, family = LOGNO(mu.link="identity", sigma.link="log"), trace = FALSE)
glm_Storm_sev_mm_2 <- gamlss(formula =  CAT_loss_normalised_GDP ~ MSLP_mm, data = CAT_Storm_ICA_loss_mm, family = LOGNO(mu.link="identity", sigma.link="log"), trace = FALSE)
glm_Storm_sev_mm_3 <- gamlss(formula =  CAT_loss_normalised_GDP ~ MSLP_mm + SST_mean_mm, data = CAT_Storm_ICA_loss_mm, family = LOGNO(mu.link="identity", sigma.link="log"), trace = FALSE)

summary_Storm_sev_MM_model_perf <- data.frame(Models = 0:3, Covaritaes = c("Stationary", "SST", "MSLP", "SST+MSLP"), AIC = c(glm_Storm_sev_0$aic, glm_Storm_sev_mm_1$aic, glm_Storm_sev_mm_2$aic, glm_Storm_sev_mm_3$aic), BIC = c(BIC(glm_Storm_sev_0), BIC(glm_Storm_sev_mm_1), BIC(glm_Storm_sev_mm_2), BIC(glm_Storm_sev_mm_3))) %>% mutate(Rank_AIC = rank(AIC)) %>% mutate(Rank_BIC = rank(BIC))
summary_Storm_sev_MM_model_perf
```

#### Storm model summary

The following results generate Table H and I in Appendix C.5, and contribute to the Table 3.1:

```{r Model summary of Storm frequency model}
modelsummary(list("Model 1*" = glm_Storm_mm_freq_1, "Model 2" = glm_Storm_mm_freq_2, "Model 3" = glm_Storm_mm_freq_3), statistic = "p.value", stars = TRUE,  gof_omit = "RMSE|Num.Obs.", coef_rename = c("Intercept","SST","MSLP"))
```

```{r Model summary of Storm severity model}
modelsummary(list("Model 1*" = glm_Storm_sev_mm_1, "Model 2" = glm_Storm_sev_mm_2, "Model 3" = glm_Storm_sev_mm_3), statistic = "p.value", stars = TRUE,  gof_omit = "RMSE|Num.Obs.", coef_omit = "Intercept", coef_rename = c("SST","MSLP"))
```

### East coast low:


We first filter the historical sea surface temperature data bounded by the ECL region ($24-41^{\circ} S; \;\;148-162^{\circ} E$). To calculate the SST gradients, the ECL region is further divided into two regions based on longtitudes:

```{r Define ECL and reference region}
## Define the Lontitude  range of ECL region:
Lat_ECL_range <- c(-41, -24)
Lon_ECL_range <- c(148, 162)
Lon_ECL_range_R1 <- c(148, 155)
Lon_ECL_range_R2 <- c(160, 165)
```

Obtain the historical ERA SST data in the ECL identification region: 

```{r Import the historical ERA5 SST data in the ECL region, include=FALSE}
ERA5_hist_SST_list_ECL <- list()
## Loop through each file:
for(i in 1:length(filenames_Sea)){
    ERA5_hist_SST <- brick(filenames_Sea[i], varname = "sst")
    ## Store the transformed data: 
    ERA5_hist_SST_list_ECL[[i]] <-  transform_nc_SST_era5_func(ERA5_hist_SST, lon_range = Lon_ECL_range, lat_range = Lat_ECL_range, Monthly = T)
}
## Combine the list into data frame:
ERA5_hist_SST_ECL_data <- rbindlist(ERA5_hist_SST_list_ECL)
```

Calculate the SST gradients in the ECL identification region: 

```{r Derive the SST gradients data, include=FALSE}

ERA5_hist_SST_diff_list_ECL <- list()
ERA5_hist_SST_diff_list_byLat <- list()

## Loop through each file:
for(i in 1:length(filenames_Sea)){
    ERA5_hist_SST <- brick(filenames_Sea[i], varname = "sst")
    SST_ECL_R1 <- as.data.frame(ERA5_hist_SST, xy = TRUE) %>% filter(x>=Lon_ECL_range_R1[1] & x<=Lon_ECL_range_R1[2]) %>% filter(y>=Lat_ECL_range[1] & y<=Lat_ECL_range[2])
    SST_ECL_R2 <- as.data.frame(ERA5_hist_SST, xy = TRUE) %>% filter(x>=Lon_ECL_range_R2[1] & x<=Lon_ECL_range_R2[2]) %>% filter(y>=Lat_ECL_range[1] & y<=Lat_ECL_range[2])
    SST_ECL_R1_Lat <- as.data.frame(na.omit(pivot_longer(SST_ECL_R1, cols = starts_with("X", ignore.case = FALSE), names_to = "Dates", values_to = "SST"))) %>% mutate(Year = as.numeric(substr(Dates, start = 2, stop = 5))) %>% mutate(Month = as.numeric(substr(Dates, start = 7, stop = 8))) %>% group_by(y, Year, Month) %>% summarise(SST_R1 = mean(SST-273.15))
    SST_ECL_R2_Lat <- as.data.frame(na.omit(pivot_longer(SST_ECL_R2, cols = starts_with("X", ignore.case = FALSE), names_to = "Dates", values_to = "SST"))) %>% mutate(Year = as.numeric(substr(Dates, start = 2, stop = 5))) %>% mutate(Month = as.numeric(substr(Dates, start = 7, stop = 8))) %>% group_by(y, Year, Month) %>% summarise(SST_R2 = mean(SST-273.15))
    ## Calculate the SST gradients:
    SST_ECL_diff <- left_join(SST_ECL_R1_Lat, SST_ECL_R2_Lat, by = c("Year", "Month", "y")) %>% mutate(SST_diff = SST_R2-SST_R1) %>% group_by(Year, Month) %>% summarise(SST_diff_mean = mean(SST_diff))
    
    SST_ECL_diff_byLat <- left_join(SST_ECL_R1_Lat, SST_ECL_R2_Lat, by = c("Year", "Month", "y")) %>% mutate(SST_diff = SST_R2-SST_R1) %>% mutate(Dates = ymd(paste(Year, Month, "01", sep = "-")))
    
    ERA5_hist_SST_diff_list_ECL[[i]] <- SST_ECL_diff 
    ERA5_hist_SST_diff_list_byLat[[i]] <- SST_ECL_diff_byLat
}
## Combine the list into data frame:
ERA5_hist_SST_diff_ECL_data <- rbindlist(ERA5_hist_SST_diff_list_ECL)
ERA5_hist_SST_diff_byLat_all_data <- rbindlist(ERA5_hist_SST_diff_list_byLat)
```

Check the data coverage:

- The SST data covers the ECL identification region

```{r Check the ECL data coverage of SST}
SST_ECL_dat <- as.data.frame(ERA5_hist_SST, xy = TRUE) %>% filter(x>=Lon_ECL_range[1] & x<=Lon_ECL_range[2]) %>% filter(y>=Lat_ECL_range[1] & y<=Lat_ECL_range[2])
SST_ref_dat <- as.data.frame(ERA5_hist_SST, xy = TRUE)  %>% filter(x>=Lon_ECL_range[1] & x<=Lon_ECL_range[2]) %>% filter(y>=Lat_ECL_range[1] & y<=Lat_ECL_range[2])
## Plot SST:
ggplot(SST_ECL_dat, aes(x, y, fill = X1994.01.01)) + 
  geom_raster() + 
  coord_fixed(expand = FALSE) + scale_fill_gradient(low = "yellow",
  high = "red") + labs(x = "Longtitudes", y = "Latitudes") + ggtitle("Sea Surface Temperature (Time snapshot; in Kelvin)")
```


First we join the SST gradients data with ECL losses and frequency data: 

```{r Join with ECL loss data to obtain monthly level data}
CAT_ECL_ICA_loss_mm <- CAT_dat_ICA_ERA5_1 %>% filter(Type == "East Coast Low") %>% dplyr::select(-Month) %>% mutate(Month = month(CAT_Event_Start)) %>% left_join(ERA5_hist_SST_ECL_data, by = c("Year", "Month")) %>% left_join(ERA5_hist_SST_diff_ECL_data, by = c("Year", "Month"))
CAT_ECL_ICA_count_mm <- CAT_dat_ICA_ERA5_aggregate_byType_mm %>% filter(Type == "East Coast Low") %>% left_join(ERA5_hist_SST_ECL_data, by = c("Year", "Month")) %>% left_join(ERA5_hist_SST_diff_ECL_data, by = c("Year", "Month"))
CAT_ECL_ICA_count <- CAT_dat_ICA_ERA5_aggregate_byType %>%filter(Type == "East Coast Low")
```


#### ECL frequency

We fit Poisson GLMs on the monthly count of ECL in the ICA dataset between 1960 and 2022:


```{r ECL frequency}
glm_ECL_freq_1 <- glm(CAT_count ~ SST_mean_mm, data = CAT_ECL_ICA_count_mm, 
                      family = poisson(link = "log"))
glm_ECL_freq_2 <- glm(CAT_count ~ SST_diff_mean, data = CAT_ECL_ICA_count_mm,
                      family = poisson(link = "log"))
print("===========Coefficients under Model 1======================")
summary(glm_ECL_freq_1)$coefficients
print("===========Coefficients under Model 2======================")
summary(glm_ECL_freq_2)$coefficients
```


#### ECL severity

Given the limited amount of data and the high uncertainty in the projected intensity of ECL, a stationary distribution assumption is made on the normalised severity of ECL:

```{r ECL severity}
glm_ECL_sev_0 <- gamlss(formula = CAT_loss_normalised_GDP ~ 1, family = LOGNO(mu.link = "identity",  
    sigma.link = "log"), data = CAT_ECL_ICA_loss_mm, trace = FALSE) 
```

#### ECL model summary

The following results generate Table J in Appendix C.6, and contribute to the Table 3.1:

```{r Model summary of ECL frequency model}
modelsummary(list("Model 1" = glm_ECL_freq_1, "Model 2*" = glm_ECL_freq_2), statistic = "p.value", stars = TRUE,  gof_omit = "RMSE|Num.Obs.", coef_rename = c("Intercept","SST", "SST gradients"))
```

### Hailstorm

Join with the hailstorm loss and frequency data:

```{r Join with hailstorm loss data to obtain monthly level data}
CAT_HL_ICA_loss_mm <- CAT_dat_ICA_ERA5_1 %>% filter(Type == "Hailstorm"|Type == "Hail") %>% dplyr::select(-Month) %>% mutate(Month = month(CAT_Event_Start)) %>% left_join(tas_ERA5_hist_dat, by = c("Year", "Month")) %>% left_join(ta_ERA5_hist_dat, by = c("Year", "Month"))

CAT_HL_ICA_count_mm <- CAT_dat_ICA_ERA5_aggregate_byType_mm %>% filter(Type == "Hailstorm"|Type == "Hail") %>% left_join(tas_ERA5_hist_dat, by = c("Year", "Month")) %>% left_join(ta_ERA5_hist_dat, by = c("Year", "Month"))

CAT_HL_ICA_count <- CAT_dat_ICA_ERA5_aggregate_byType %>% filter(Type == "Hailstorm"|Type == "Hail")
```



### Hailstorm frequency

We fitted a Poisson GLM to the monthly count of hail storms using mid-troposphere air temperature and near-surface temperature as covariates, based on their influence on hailstorm dynamics:

```{r Hail storm frequency model}
glm_HS_freq <- glm(CAT_count ~ Air_temperature  + NS_temperature, data = CAT_HL_ICA_count_mm, family = poisson(link = "log"))
summary(glm_HS_freq)$coefficients
```


#### Hailstorm severity

The non-stationary Log-Normal model (which uses air temperature as the covariate) does not out-perform the stationary Log-Normal model:

```{r Hail storm severity model}
glm_HS_sev_0 <- gamlss(formula = CAT_loss_normalised_GDP ~ 1, family = LOGNO(mu.link = "identity",  
    sigma.link = "log"), data = CAT_HL_ICA_loss_mm,  trace = FALSE)
glm_HS_sev_1 <- gamlss(formula = CAT_loss_normalised_GDP ~ Air_temperature + NS_temperature, family = LOGNO(mu.link = "identity", sigma.link = "log"), data = CAT_HL_ICA_loss_mm,  trace = FALSE)

data.frame(Models =0:1, Covariates = c("Stationary", "Atmospheric temperature + NS temperature"), AIC = c(glm_HS_sev_0$aic, glm_HS_sev_1$aic), BIC = c(BIC(glm_HS_sev_0), BIC(glm_HS_sev_1))) %>% mutate(AIC_rank = rank(AIC), BIC_rank = rank(BIC))
```

#### Model summary of hailstorm

The following results generate Table K and Table L in Appendix C.7, and contribute to the Table 3.1:

```{r Model summary of HS frequency and severity model}
modelsummary(list("Model 1" = glm_HS_freq), statistic = "p.value", stars = TRUE,  gof_omit = "RMSE|Num.Obs.", coef_rename = c("Intercept","Atmospheric temperature", "Near-surface temperature"))

modelsummary(list("Model 1" = glm_HS_sev_1), statistic = "p.value", stars = TRUE,  gof_omit = "RMSE|Num.Obs.", coef_rename = c("Atmospheric temperature", "Near-surface temperature"),  coef_omit = "Intercept")
```



## MEV module

### Inflation rates

This part calibrates the parameters of the baseline inflation rates model as shown in Table 3.2. 

Check assumption of inflation rates:

- Based on PACF and ACF plot, the commonly used A1(1) assumption for inflation rates is suitable here. 

```{r}
pacf(na.omit(Au_inflation_annual$Inflation_rate))
acf(na.omit(Au_inflation_annual$Inflation_rate))
```

Fit the benchmark inflation rates model with A1(1) process: 

```{r}
## Fit the benchmark inflation model:
inflation_mod_bench <- arima(x = Au_inflation_annual$Inflation_rate, order=c(1, 0, 0))
```


### Interest rates

This part calibrates the parameters of the interet rates model as shown in Table 3.2. 

First we regress real short-rates over the growth rate of potential GDP:

- The coefficient of potential GDP growth is positive and statistically significant

```{r Fit the potential GDP dependent interest rates model}
lm_SR_GDP_potential <- lm(Short_rates_real ~ GDP_potential_growth, data = AU_interest_rates_GDP_potential)
summary(lm_SR_GDP_potential)
```

Fit a AR(1) model on the residuals:

```{r}
SR_mod_AR1_potentialGDP <- sarima(lm_SR_GDP_potential$residuals, 1, 0, 0, details  = F)
paste("Mu:", SR_mod_AR1_potentialGDP$fit$coef[2])
paste("Phi:", SR_mod_AR1_potentialGDP$fit$coef[1])
paste("Sigma:", sqrt(SR_mod_AR1_potentialGDP$fit$sigma2))
```


Provide a model summary of the interest rate model with potential GDP:

- This contributes to Table 3.2 in the paper. 

```{r Model summary of Real Short rates and Real GDP growth}
modelsummary(list("lm (Short-rates)" = lm_SR_GDP_potential), statistic = "p.value", stars = TRUE, gof_omit = "RMSE|R2|R2 Adj.|RMSE|Num.Obs.|F")
```


## Liabilities module (Non-Catastrophe claims)

### Extract the non-catastrophe loss: 


Aggregate the data for ultimate claim cost, Number of claims reported, and the number of risks in force: 

```{r Summarise the ultimate claims amount}
## Summarise the ultimate claims: 
GI_development_statistics_PropertyandMotor_UC <- GI_development_statistics_PropertyandMotor %>%
    filter(`Data Item` == "Gross ultimate cost (inflated and undiscounted) by class of business") %>%
    mutate(Calendar_Year = `Accident/underwriting year` + `Development year`) %>%
    filter(Calendar_Year == 2022) %>%
    group_by(`Accident/underwriting year`) %>% 
    summarise(Ultimate_Cost = sum(Value))

## Also summarise the ultimate claims for all LoBs:
GI_development_statistics_all_UC <- GI_development_statistics_database %>%
    filter(`Data Item` == "Gross ultimate cost (inflated and undiscounted) by class of business") %>%
    mutate(Calendar_Year = `Accident/underwriting year` + `Development year`) %>%
    filter(Calendar_Year == 2022) %>%
    group_by(`Accident/underwriting year`) %>% 
    summarise(Ultimate_Cost = sum(Value))

## Summarise the count of reported claims: 
GI_development_statistics_PropertyandMotor_NC <- GI_development_statistics_PropertyandMotor %>%
    filter(`Data Item` == "Number of claims reported by class of business") %>%
     mutate(Calendar_Year = `Accident/underwriting year` + `Development year`) %>%
    filter(Calendar_Year == 2022) %>%
    group_by(`Accident/underwriting year`) %>% 
    summarise(Claims_count_total = sum(Value))

## Summarise the risk exposures: 

GI_development_statistics_PropertyandMotor_Exposure <- GI_development_statistics_PropertyandMotor %>%
    filter(`Data Item` == "Number of risks in force by class of business") %>%
    group_by(`Accident/underwriting year`) %>% 
    summarise(Exposure = sum(Value))

## Summarise the risk exposures for all BoLs:
GI_development_statistics_all_Exposure <- GI_development_statistics_database %>%
    filter(`Data Item` == "Number of risks in force by class of business") %>%
    group_by(`Accident/underwriting year`) %>% 
    summarise(Exposure = sum(Value))


## Join the three datasets:
GI_development_statistics_PropertyandMotor_agg <- left_join(GI_development_statistics_PropertyandMotor_Exposure, GI_development_statistics_PropertyandMotor_NC , by = "Accident/underwriting year") %>% left_join(GI_development_statistics_PropertyandMotor_UC, by = "Accident/underwriting year") %>% rename(Year = `Accident/underwriting year`)
```


```{r Summarise the aggregate claims cost for whole industry, fig.height = 7}
GI_development_statistics_total_agg_Exposure <- left_join(GI_development_statistics_all_Exposure, GI_development_statistics_PropertyandMotor_Exposure, by = "Accident/underwriting year") %>% rename(Exposure_all = Exposure.x, Exposure_PM_ISR = Exposure.y) %>% mutate(Exposure_other = Exposure_all - Exposure_PM_ISR) %>% mutate(Exposure_PM_ISR_prop = Exposure_PM_ISR/Exposure_all)

GI_development_statistics_total_agg_UC <- left_join(GI_development_statistics_all_UC, GI_development_statistics_PropertyandMotor_UC, by = "Accident/underwriting year") %>% rename(Claims_cost_PM_ISR = Ultimate_Cost.y, Claims_cost_total = Ultimate_Cost.x) %>% mutate(Claims_cost_other = Claims_cost_total - Claims_cost_PM_ISR) %>% mutate(PM_ISR_proportion = Claims_cost_PM_ISR/Claims_cost_total)

GI_Ultimate_Losses_other <- left_join(GI_development_statistics_total_agg_Exposure, GI_development_statistics_total_agg_UC, by = "Accident/underwriting year") %>% rename(Year = `Accident/underwriting year`) %>% left_join(Au_inflation_annual, by = "Year") %>% dplyr::select(Year, Exposure_other, Claims_cost_other, CPI)
```


Get the annual catastrophe losses data: 

```{r Summarise the annual catastrophe losses data}
CAT_dat_annual_losses <- CAT_dat_ICA_ERA5 %>%
    group_by(Year) %>%
    summarise(CAT_loss_annual = sum(CAT_loss_original, na.rm = T), CPI = mean(CPI))
```

Join with the ultimate losses data to derive the non-catastrophe losses: 

```{r Join with the ultimate losses data}
GI_Ultimate_Losses_PM <- left_join(GI_development_statistics_PropertyandMotor_agg, CAT_dat_annual_losses, by = "Year") %>% mutate(ratio = CAT_loss_annual/Ultimate_Cost) %>%
    mutate(Non_CAT_loss = Ultimate_Cost-CAT_loss_annual,
           Average_Claim_cost = Non_CAT_loss/Exposure)
```


### Calibration of non-catastrophe model

Due to the limited number of data points, we do not explicitely model the CPI as a covariate; instead, we model the normalised claim cost by incorporating the precipitation information as the covariate: 

```{r Adding normalised non-CAT losses to home and property lines}
CPI_2022 <- GI_Ultimate_Losses_PM[GI_Ultimate_Losses_PM$Year == 2022, ]$CPI
GI_Ultimate_Losses_PM <- GI_Ultimate_Losses_PM %>%
    mutate(Non_CAT_loss_2022 = Non_CAT_loss*CPI_2022/CPI)
```

```{r Adding normalised non-CAT losses to other lines}
GI_Ultimate_Losses_other <- GI_Ultimate_Losses_other %>% mutate(Non_CAT_other_2022 = Claims_cost_other*CPI_2022/CPI)
#GI_Ultimate_Losses_other
```

Fit a model on the non-CAT losses of the Property, Motor, Fire and ISR line:

```{r Fit a stationary non-catastrophe loss model}
tw_PM_0 <-  gam(Non_CAT_loss_2022/Exposure ~ 1, data = GI_Ultimate_Losses_PM , family = tw, method = "ML")
```

Also fit a model on the non-CAT losses of other business lines: 

```{r Fit a model on non-CAT losses: other LoBs}
tw_other_0 <- gam(Non_CAT_other_2022/Exposure_other ~ 1, data = GI_Ultimate_Losses_other, family = tw, method = "ML")
#summary(tw_other_0 )
```


To extrapolate the exposure data to the future projection period, we calibrate the relationship between risk exposure and population level: 

```{r Explore the relationship between exposure and population}
#pdf("Plotting/Liabilities/Exposure_vs_Population.pdf")
GI_exposure_pop_data <- GI_development_statistics_PropertyandMotor_Exposure %>% rename(Year = `Accident/underwriting year`) %>% left_join(AU_pop, by = "Year")
GI_other_exposure_pop_data <- GI_Ultimate_Losses_other %>% left_join(AU_pop, by = "Year")
```

We fit a linear model between exposure to Property, Motor, Fire and ISR line and population:

- The fit is decent as the population could explain $96.81\%$ of variability: this could be explained by the highly linear relationship observed in the plot. 

```{r Fit the relationship between exposure and population}
lm_exposure <- lm(formula = Exposure ~ Population, data = GI_exposure_pop_data)
summary(lm_exposure)
```

As for the other lines of business, due to its relatively weak connection with the population growth, we model its exposure by exponential smoothing: 

```{r}
Exposure_forecasts_other <- expsmooth(GI_other_exposure_pop_data$Exposure_other, plot = FALSE, alpha = 0.5, lead = 78)$pred
```




## Assets module

### Calibration of the asset returns model

The corporate earning data is converted to real terms by joining with the CPI data. The corporate earning data is then joined with the total returns data for further analysis. 

```{r Convert the corporate earning to constant prices}
## Select relevant columns:
Au_inflation_select <- Au_inflation %>%
    dplyr::select(Time, `Index Numbers ;  All groups CPI ;  Australia ;`) %>% rename(CPI = `Index Numbers ;  All groups CPI ;  Australia ;`)
## Convert the nominal profits to real profits: 
CPI_2022 <- Au_inflation_select[Au_inflation_select$Time == ymd("	
2022-12-01"), ]$CPI
AU_corporates_dat <- left_join(AU_corporates_dat_raw, Au_inflation_select, by = "Time") %>%
    mutate(Gross_operating_profits_real = Gross_operating_profits*CPI_2022/CPI)
## Obtain the annual operating profits:
AU_corporates_dat_yy <- AU_corporates_dat %>% mutate(Year = year(Time)) %>%
    group_by(Year) %>% summarise(Gross_OP_real = sum(Gross_operating_profits_real)) %>%
    filter(Year < 2019 & Year > 1994) %>%
    mutate(OP_growth = Gross_OP_real/lag(Gross_OP_real)-1)

## Join with the dividend data:
TR_all_OP_ord_shares_yy_data <- left_join(AU_corporates_dat_yy, TR_all_ord_shares_yy_data_3, by = c("Year")) %>% drop_na()
```


We then calibrate the operating profit growth and the total return model:

- The coefficient attached to operating profit growth is positive and statistically significant, supporting our previous assumption on the relationship between operating profit growth and the consumption growth.
- The calibrated parameter are also shown in Table 3.3 in Section 3 of the paper.

```{r Calibrate the operating profit growth and total return model}
lm_OP_consumption_growth <- lm(OP_growth ~ Consumption_growth, data = TR_all_OP_ord_shares_yy_data)
lm_OP_TR <- lm(Total_return ~ OP_growth, data = TR_all_OP_ord_shares_yy_data, subset = Year %nin% c(2008, 2009, 2012))
## Calibrate the operating profit growth and the total return model: 
summary(lm_OP_consumption_growth)
summary(lm_OP_TR)
```

### Calibration of transition stress on brown stocks

Firstly we import the planned production data for oil and gas in Australia: 

```{r Plot of oil and gas planned produciton data}
## Plot the path
ggplot(as.data.frame(OECD_Oil_Gas_SSP), mapping = aes(x = Year, y = Value_sum)) + geom_line(aes(color = Scenario)) + labs(y = "Oil and Gas production") + ggtitle("OECD Oil and Gas")
## Convert to the wide format: 
OECD_Oil_Gas_SSP_wide <- OECD_Oil_Gas_SSP %>% dplyr::select(Scenario, Year, Value_sum) %>% pivot_wider(names_from = Scenario, values_from = Value_sum)
```

Use spline interpolation to interpolate the data: 

```{r Interpolate of oil and gas production data}
interpolated_years <- min(OECD_Oil_Gas_SSP_wide$Year):max(OECD_Oil_Gas_SSP_wide$Year)

## Interpolate the gas production data: 
Oil_Gas_projections_SSP_interpolate <- data.frame(Year = interpolated_years,
                                              `SSP 1.00` = spline(OECD_Oil_Gas_SSP_wide$Year, OECD_Oil_Gas_SSP_wide$`SSP1-26`, n = length(interpolated_years))$y,
                                              `SSP 2.00` = spline(OECD_Oil_Gas_SSP_wide$Year, OECD_Oil_Gas_SSP_wide$`SSP2-45`, n = length(interpolated_years))$y,
                                              `SSP 3.00` = spline(OECD_Oil_Gas_SSP_wide$Year, OECD_Oil_Gas_SSP_wide$`SSP3-Baseline`, n = length(interpolated_years))$y,
                                               `SSP 4.00` = spline(OECD_Oil_Gas_SSP_wide$Year, OECD_Oil_Gas_SSP_wide$`SSP4-Baseline`, n = length(interpolated_years))$y, 
                                              `SSP 5.00` = spline(OECD_Oil_Gas_SSP_wide$Year, OECD_Oil_Gas_SSP_wide$`SSP5-Baseline`, n = length(interpolated_years))$y) %>% filter(Year >= 2022)
colnames(Oil_Gas_projections_SSP_interpolate) = c("Year","SSP 1.00", "SSP 2.00", "SSP 3.00", "SSP 4.00", "SSP 5.00") 
```

Derive the growth rates for oil and gas production: 

```{r Derive the oil and gas production growth rates}
Brown_production_growth_SSP <- data.frame(Year = Oil_Gas_projections_SSP_interpolate$Year[-1],
                                          `SSP 1.00` = na.omit(Oil_Gas_projections_SSP_interpolate$`SSP 1.00`/lag(Oil_Gas_projections_SSP_interpolate$`SSP 1.00`))-1,
                                          `SSP 2.00` = na.omit(Oil_Gas_projections_SSP_interpolate$`SSP 2.00`/lag(Oil_Gas_projections_SSP_interpolate$`SSP 2.00`))-1,
                                          `SSP 3.00` = na.omit(Oil_Gas_projections_SSP_interpolate$`SSP 3.00`/lag(Oil_Gas_projections_SSP_interpolate$`SSP 3.00`))-1,
                                          `SSP 5.00` = na.omit(Oil_Gas_projections_SSP_interpolate$`SSP 5.00`/lag(Oil_Gas_projections_SSP_interpolate$`SSP 5.00`))-1)
```


Import the financial statements from Woodside Energy Group Ltd (Extracted from FactSet):

```{r Import financial statements from WDS}
WDS_fins_raw_00 <- read_excel(rep_oil_gas_dat_path, sheet = "WDS-AU", skip = 7)
colnames(WDS_fins_raw_00)[1]  = "Items"
WDS_fins_raw_0 <- WDS_fins_raw_00[2:26, ]
#colnames(WDS_fins_raw_0)[1]  = "Items"
WDS_fins_temp <- WDS_fins_raw_0[, -1] %>% mutate_if(is.character,as.numeric)
WDS_fins_raw <- cbind(Items = WDS_fins_raw_0$Items, WDS_fins_temp)

## Select the key financial items: 
WDS_fins <- WDS_fins_raw %>% filter(Items %in% c("Sales", "COGS excluding D&A", "Depreciation & Amortization Expense", "Gross Income", "SG&A Expense", "EBIT (Operating Income)", "Nonoperating Income - Net", "Interest Expense")) 

## Caculate the net income (assume a tax rate of 30%)
net_income = (as.numeric(WDS_fins[WDS_fins$Items == "EBIT (Operating Income)", -1])+as.numeric( WDS_fins[WDS_fins$Items == "Nonoperating Income - Net", -1])-as.numeric(WDS_fins[WDS_fins$Items == "Interest Expense", -1]))*(1-0.3) 
net_income_row = c("Net income", as.numeric(net_income))
WDS_fins_0_1 <- rbind(WDS_fins, net_income_row)
WDS_fins_0_2 <- WDS_fins_0_1[, -1] %>% mutate_if(is.character,as.numeric)
WDS_fins_0 <- cbind(Items = WDS_fins_0_1$Items, WDS_fins_0_2)  
```


Perform the impact calculation for each financial statement between the 2014 and 2023: 

```{r Perform the calculation of impact scalar}
Scalar_NI_list <- c()
Scalar_EBIT_list <- c()
for(i in 2:ncol(WDS_fins_0)){
    data.test <- WDS_fins_0[, c(1,i)]
    Scalar_NI_list[i-1] <- output_stressed_cal(0.01, data.test)$Scalar_NI
    Scalar_EBIT_list[i-1] <- output_stressed_cal(0.01, data.test)$Scalar_EBIT
}
```


Excluding the outlier in 2020, the Net Income scalars and EBIT scalars are generally fluctuate closely around their mean with no obvious temporal pattern:

- We could use the average scalars during the period of 2014 and 2023 (excluding the outlier in 2020) to derive the final impact scalar

```{r Plot the impact scalars}
years = 2023:2014
plot(x = years[-4], y = Scalar_NI_list[-4], type = "p", ylim = c(0, 2.4), xlab = "Year", ylab = "Scalar", main = "Net income impact per 1% output drop")
abline(h = mean(Scalar_NI_list[-4]), col = "red")
legend("bottomright", legend = c("Average scalar"), col = c("red"), lty = 1, cex = 0.6)
text(x = 2015, y = 1.63, labels = round(mean(Scalar_NI_list[-4]), 3), col = "red")

plot(x = years[-4],y = Scalar_EBIT_list[-4], type = "p", ylim = c(0, 2.4), xlab = "Year", ylab = "Scalar", main = "EBIT impact per 1% output drop")
abline(h = mean(Scalar_EBIT_list[-4]), col = "red")
legend("bottomright", legend = c("Average scalar"), col = c("red"), lty = 1, cex = 0.6)
text(x = 2015, y = 1.62, labels = round(mean(Scalar_EBIT_list[-4]), 3), col = "red")
```


```{r Calculate the mean impact scalars}
scalar_impact_WDS_EBIT <- mean(Scalar_EBIT_list[-4])
scalar_impact_WDS_NI <- mean(Scalar_NI_list[-4])
```


# Section 3.2 Key simulation results from individual modules

This section generates the results shown in Section 3.2 in the paper. 

## Define the control variables

```{r Control panel for DFA components simulations}
n.sims <- 10000  ## Define the number of simulations
forecast_horizon <- 78 ## Define the maximum period of forecasting (this is the maximum data points used in climate projections); the paper focuses on the forecasting period up to 38 years. 
start_year_proj <- 2023 ## Define the start of the projection period
start_year <- start_year_proj
run <-TRUE ## If choose run = FALSE, will load the previous model runs results
path_to_results <- "Simulated_results_DFA_components_sims10000_21_Sep_2024.Rdata" ## Enter the path to previous simulation results if choose run = F
seed <- 20240625 ## Set the seeds for running the simulations
Risk_free_weight_default <- 0.6 ## Set the weight allocated to risk-free assets
Brown_weight <- 0.034 ## Specify the weight allocated to the brown portfolio
risk_aversion_default <- 0.55 ## Set risk-aversion parameter for premium loading
Clim_proj_clean <- T ## indicate weather climate projections have been imported and clean; default is true
scenario_list <- c("SSP 2.6", "SSP 4.5", "SSP 7.0", "SSP 8.5") ## Define the list of SSP scenarios
col_list <- c("green", "orange", "red", "brown") ## Define the colour used for plotting the simulations
```




## Import and cleanse climate variables projections

This section imports, cleans, bias-corrects, and calibrates the uncertainty of climate variable projections using the method outlined in Section 2.2.1.


### Near-surface temperature

```{r Import of near-surface temperature projections and backcasts}
if(Clim_proj_clean == T){
dirc <- tas_projections_dirc
## Import of projections
Scenarios_list <- c("ssp126", "ssp245", "ssp370", "ssp585")
tas_ensemble_all_scenarios_list <- list()
for(s in 1:4){
    tas_data <- read.csv(paste0(dirc, "/", Scenarios_list[s], "_ensemble_tas_proj", ".csv")) %>% dplyr::select(-X)
    colnames(tas_data) = gsub("\\.", "-", colnames(tas_data))
    tas_ensemble_all_scenarios_list[[s]] <- tas_data %>% filter(Year >= start_year_proj & Year <= (start_year_proj + forecast_horizon -1))
}
## Import of historical backcasts:
tas_ensemble_hist_data <- read.csv(tas_hist_backcasts_dirc) %>% dplyr::select(-X)
colnames(tas_ensemble_hist_data) = gsub("\\.", "-", colnames(tas_ensemble_hist_data))
Model_names_tas_hist <- colnames(tas_ensemble_hist_data)[-c(1:2)]

## Find the intersect of model names:
#Models_ensemble_tas <- intersect(Models_ensemble_tas_proj, Model_names_tas_hist)
} else {
    source("Miscellaneous data cleaning codes/Near_surface_temperature_import_clean.R")
}
## join the data: 
tas_ERA5_ensemble_hist_dat <- left_join(tas_ensemble_hist_data, tas_ERA5_hist_dat, by = c("Year", "Month")) %>% mutate(Dates = ymd(paste(Year, Month, "01", sep = "-")))
```

```{r Get the model names for NS temperature projections}
## Get the common model names accross all scenarios for projections:
Model_names_tas_proj_list <- list()
for(s in 1:4){
    Model_names_tas_proj_list[[s]] <- colnames(tas_ensemble_all_scenarios_list[[s]])[-1]
}
Models_ensemble_tas_proj <- Reduce(intersect, Model_names_tas_proj_list)

## Find the intersect of model names:
Models_ensemble_tas <- intersect(Models_ensemble_tas_proj, Model_names_tas_hist)
```


```{r Test of relationship and bias correction of tas, fig.height = 7}
par(mfrow = c(4, 5))
for(mod in Models_ensemble_tas[1:17]){
test_lm_tas <- lm(sort(NS_temperature) ~ sort(tas_ERA5_ensemble_hist_dat[, mod]), data = tas_ERA5_ensemble_hist_dat)
test_tas_model_correct <- test_lm_tas$coefficients[1] + test_lm_tas$coefficients[2]*tas_ERA5_ensemble_hist_dat[, mod]
# plot(x = tas_ERA5_ensemble_hist_dat$Year, tas_ERA5_ensemble_hist_dat$NS_temperature, col = "black", ylim = c(20, 30), xlab  = "Year", ylab = "tas", main = paste("tas:", mod))
# lines(x = tas_ERA5_ensemble_hist_dat$Year, tas_ERA5_ensemble_hist_dat[, mod], col = "blue", lty = 1)
# lines(x = tas_ERA5_ensemble_hist_dat$Year, test_tas_model_correct, col = "blue", lty = 2, lwd = 2)
# legend("bottomright", legend = c("Model", "Model (Corrected)", "Observations"), col = c("blue","blue","black"), lty = c(1,2, NA), pch = c(NA, NA, 1))
}
```


```{r Derive the projections of bias-corrected Near-surface temperature (after bias corrections), fig.height = 7}
## Derive the mean of tas after bias correction:
tas_mean_all_scenarios_corrected_list <- list()

for(s in 1:4){
    tas_correct_proj_mat <- matrix(NA, nrow = nrow(tas_ensemble_all_scenarios_list[[s]]), ncol = length(Models_ensemble_tas))
    for(i in 1:length(Models_ensemble_tas)){
        mod <- Models_ensemble_tas[i]
        lm_tas <- lm(sort(NS_temperature) ~ sort(tas_ERA5_ensemble_hist_dat[, mod]), data = tas_ERA5_ensemble_hist_dat)
        tas_model_correct <-  lm_tas$coefficients[1] +  lm_tas$coefficients[2]*tas_ensemble_all_scenarios_list[[s]][, mod]
        ## Correct predictions: 
        tas_correct_proj_mat[, i] <- tas_model_correct
    }
colnames(tas_correct_proj_mat) = Models_ensemble_tas
tas_mean_all_scenarios_corrected_list[[s]] <- tas_correct_proj_mat
}
```


Collect the noise and calculate the standard deviations:

```{r Calculate noise and standard deviations for TAS residuals}
tas_noise_mat <- matrix(NA, nrow = nrow(tas_ERA5_ensemble_hist_dat), ncol = length(Models_ensemble_tas))
tas_quantile_sd <- c()
tas_hist_sd <- c()
for(i in 1:length(Models_ensemble_tas)){
        mod <- Models_ensemble_tas[i]
        lm_tas <- lm(sort(NS_temperature) ~ sort(tas_ERA5_ensemble_hist_dat[, mod]), data = tas_ERA5_ensemble_hist_dat)
        tas_model_correct <- lm_tas$coefficients[1] +  lm_tas$coefficients[2]*tas_ERA5_ensemble_hist_dat[, mod]
        ## Collect the noise of projections: 
        noise_tas <- tas_ERA5_ensemble_hist_dat$NS_temperature - tas_model_correct
        tas_noise_mat[, i] <- noise_tas
        ## Collect the quantile standard deviations:
        tas_quantile_sd[i] <- summary(lm_tas)$sigma
        ## Collect the historical standard deviations:
        tas_hist_sd[i] <- sd(noise_tas)
    }
```

```{r Plot of TAS residuals for each model, fig.height = 7}
par(mfrow = c(3, 6))
for(i in 1:ncol(tas_noise_mat)){
    qqplot(qnorm(ppoints(nrow(tas_noise_mat)), mean = 0, sd = tas_hist_sd[i]), tas_noise_mat[, i], xlab = "Theorectical Quantiles (Normal)", ylab = "Empirical Quantiles", main = "Q-Q plot of Near-Surface Temperature residuals")
    abline(0, 1, col = "red", lwd = 2)
}
```


```{r Perform shapiro normality test for residuals}
p_vals_shapiro_tas <- c()
for(i in 1:ncol(tas_noise_mat)){
    shapiro_test_stats <- shapiro.test(tas_noise_mat[, i])
    p_vals_shapiro_tas[i] <- shapiro_test_stats$p.value
}
plt <- barplot(p_vals_shapiro_tas, col='steelblue', xaxt="n", main = "p-values for Shapiro-Wilk test (NS residuals)", xlab = "Models", ylab = "p-values")
text(plt, par("usr")[3], labels = Models_ensemble_tas, srt = 60, adj = c(1.1,1.1), xpd = TRUE, cex=0.6)
abline(h = 0.05, lty = 2, col = "red")
text("")
```


### Air temperature


```{r Import of air temperature projections and backcasts}
if(Clim_proj_clean == T){
    dirc <- ta_projections_dirc
    ta_ensemble_all_scenarios_list <- list()
    for(s in 1:4){
        ta_data <- read.csv(paste0(dirc, "/", Scenarios_list[s], "_ensemble_ta_proj", ".csv")) %>% dplyr::select(-X)
        colnames(ta_data) = gsub("\\.", "-", colnames(ta_data))
        ta_ensemble_all_scenarios_list[[s]] <- ta_data %>% filter(Year >= start_year_proj & Year <= (start_year_proj + forecast_horizon -1))
    }
   # Models_ensemble_ta_proj <- colnames(ta_data)[-c(1:2)]
    
    ta_ensemble_hist_data  <- read.csv(ta_hist_backcasts_dirc) %>% dplyr::select(-X)
    colnames(ta_ensemble_hist_data) = gsub("\\.", "-", colnames(ta_ensemble_hist_data))
    Model_names_ta_hist <- colnames(ta_ensemble_hist_data)[-c(1:2)]
    ## Find the intersect of model names:
    #Models_ensemble_ta <- intersect(Models_ensemble_ta_proj, Model_names_ta_hist)
    } else{
        source("Miscellaneous data cleaning codes/Air_temperature_import_clean.R")
    }

## Join the data:
ta_ERA5_ensemble_hist_dat <- left_join(ta_ensemble_hist_data, ta_ERA5_hist_dat, by = c("Year", "Month")) %>% mutate(Dates = ymd(paste(Year, Month, "01", sep = "-")))
```

```{r Get the model names for Air temperature projections}
## Get the common model names accross all scenarios for projections:
Model_names_ta_proj_list <- list()
for(s in 1:4){
    Model_names_ta_proj_list[[s]] <- colnames(ta_ensemble_all_scenarios_list[[s]])[-1]
}
Models_ensemble_ta_proj <- Reduce(intersect, Model_names_ta_proj_list)

## Find the intersect of model names:
Models_ensemble_ta <- intersect(Models_ensemble_ta_proj, Model_names_ta_hist)
```

```{r Derive the projections of bias-corrected Air temperature (after bias corrections), fig.height = 7}
## Derive the mean of ta after bias correction:
ta_mean_all_scenarios_corrected_list <- list()

for(s in 1:4){
    ta_correct_proj_mat <- matrix(NA, nrow = nrow(ta_ensemble_all_scenarios_list[[s]]), ncol = length(Models_ensemble_ta))
    for(i in 1:length(Models_ensemble_ta)){
        mod <- Models_ensemble_ta[i]
        lm_ta <- lm(sort(Air_temperature) ~ sort(ta_ERA5_ensemble_hist_dat[, mod]), data = ta_ERA5_ensemble_hist_dat)
        ta_model_correct <-  lm_ta$coefficients[1] +  lm_ta$coefficients[2]*ta_ensemble_all_scenarios_list[[s]][, mod]
        ta_correct_proj_mat[, i] <- ta_model_correct
    }
colnames(ta_correct_proj_mat) = Models_ensemble_ta
ta_mean_all_scenarios_corrected_list[[s]] <- ta_correct_proj_mat
}
```


### Sea-surface temperature and gradients


```{r Import of SST projections and backcast data}
if(Clim_proj_clean == T){
### Import the Sea-surface temperature data
dirc <- SST_projections_dirc
    SST_ensemble_all_scenarios_list <- list()
    for(s in 1:4){
        SST_data <- read.csv(paste0(dirc, "/", Scenarios_list[s], "_ensemble_SST_proj", ".csv")) %>% dplyr::select(-X)
        colnames(SST_data) = gsub("\\.", "-", colnames(SST_data))
        SST_ensemble_all_scenarios_list[[s]] <- SST_data %>% filter(Year >= start_year_proj & Year <= (start_year_proj + forecast_horizon -1))
    }
    Models_ensemble_SST_proj <- colnames(SST_data)[-c(1:2)]
    
    SST_ensemble_hist_data  <- read.csv(SST_hist_backcasts_dirc) %>% dplyr::select(-X)
    colnames(SST_ensemble_hist_data) = gsub("\\.", "-", colnames(SST_ensemble_hist_data))
    Model_names_SST_hist <- colnames(SST_ensemble_hist_data)[-c(1:2)]
      ## Find the intersect of model names:
    Models_ensemble_SST <- intersect(Models_ensemble_SST_proj, Model_names_SST_hist)
    } else{
        source("Miscellaneous data cleaning codes/SST_import_clean.R")
    }
## Join with historical data:
SST_ERA5_ensemble_hist_dat <- left_join(SST_ensemble_hist_data, ERA5_hist_SST_mm_data, by = c("Year", "Month")) %>% mutate(Dates = ymd(paste(Year, Month, "01", sep = "-")))
```

```{r Get the model names for SST projections}
## Get the common model names across all scenarios for projections:
Model_names_SST_proj_list <- list()
for(s in 1:4){
    Model_names_SST_proj_list[[s]] <- colnames(SST_ensemble_all_scenarios_list[[s]])[-1]
}
Models_ensemble_SST_proj <- Reduce(intersect, Model_names_SST_proj_list)

## Find the intersect of model names:
Models_ensemble_SST <- intersect(Models_ensemble_SST_proj, Model_names_SST_hist)
```



```{r Derive the projections of sea-surface temperature (after bias corrections), fig.height = 7}
## Derive the mean of SST after bias correction:
SST_mean_all_scenarios_corrected_list <- list()
for(s in 1:4){
    SST_correct_proj_mat <- matrix(NA, nrow = nrow(SST_ensemble_all_scenarios_list[[s]]), ncol = length(Models_ensemble_SST))
    for(i in 1:length(Models_ensemble_SST)){
        mod <- Models_ensemble_SST[i]
        lm_SST <- lm(sort(SST_mean_mm) ~ sort(SST_ERA5_ensemble_hist_dat[, mod]), data = SST_ERA5_ensemble_hist_dat)
        SST_model_correct <-  lm_SST$coefficients[1] +  lm_SST$coefficients[2]*SST_ensemble_all_scenarios_list[[s]][, mod]
        SST_correct_proj_mat[, i] <- SST_model_correct
    }
colnames(SST_correct_proj_mat) = Models_ensemble_SST
SST_mean_all_scenarios_corrected_list[[s]] <- SST_correct_proj_mat
}
```



```{r Import of SST gradients projections and backcast data}
if(Clim_proj_clean == T){
### Import the Sea-surface temperature data
dirc <- SSTgrad_projections_dirc 
    SST_grad_ensemble_all_scenarios_list <- list()
    for(s in 1:4){
        SST_grad_data <- read.csv(paste0(dirc, "/", Scenarios_list[s], "_ensemble_SST_grad_proj", ".csv")) %>% dplyr::select(-X)
        colnames(SST_grad_data) = gsub("\\.", "-", colnames(SST_grad_data))
        SST_grad_ensemble_all_scenarios_list[[s]] <- SST_grad_data %>% filter(Year >= start_year_proj & Year <= (start_year_proj + forecast_horizon -1))
    }
    #Models_ensemble_SST_grad_proj <- colnames(SST_grad_data)[-c(1:2)]
    
    SST_grad_ensemble_hist_data  <- read.csv(SSTgrad_hist_backcasts_dirc) %>% dplyr::select(-X)
    colnames(SST_grad_ensemble_hist_data) = gsub("\\.", "-", colnames(SST_grad_ensemble_hist_data))
    Model_names_SST_grad_hist <- colnames(SST_grad_ensemble_hist_data)[-c(1:2)]
      ## Find the intersect of model names:
    #Models_ensemble_SST <- intersect(Models_ensemble_SST_grad_proj, Model_names_SST_grad_hist)
    } else{
        source("Miscellaneous data cleaning codes/SST_grad_import_clean.R")
    }
## Join with historical data:
SST_grad_ERA5_ensemble_hist_dat <- left_join(SST_grad_ensemble_hist_data, ERA5_hist_SST_diff_ECL_data, by = c("Year", "Month")) %>% mutate(Dates = ymd(paste(Year, Month, "01", sep = "-")))
```


```{r Derive and plot the projections of mean path of SST gradients (after bias corrections), fig.height = 7}
## Derive the mean of SST_grad after bias correction:
SST_grad_mean_all_scenarios_corrected_list <- list()

for(s in 1:4){
    SST_grad_correct_proj_mat <- matrix(NA, nrow = nrow(SST_grad_ensemble_all_scenarios_list[[s]]), ncol = length(Models_ensemble_SST))
    for(i in 1:length(Models_ensemble_SST)){
        mod <- Models_ensemble_SST[i]
        lm_SST_grad <- lm(sort(SST_diff_mean) ~ sort(SST_grad_ERA5_ensemble_hist_dat[, mod]), data = SST_grad_ERA5_ensemble_hist_dat)
        SST_grad_model_correct <-  lm_SST_grad$coefficients[1] +  lm_SST_grad$coefficients[2]*SST_grad_ensemble_all_scenarios_list[[s]][, mod]
        SST_grad_correct_proj_mat[, i] <- SST_grad_model_correct
    }
colnames(SST_grad_correct_proj_mat) = Models_ensemble_SST
SST_grad_mean_all_scenarios_corrected_list[[s]] <- SST_grad_correct_proj_mat
}
```


Collect the noise and calculate the standard deviations for SST gradients:

```{r Calculate noise and standard deviations for SST_grad residuals}
SST_grad_noise_mat <- matrix(NA, nrow = nrow(SST_grad_ERA5_ensemble_hist_dat), ncol = length(Models_ensemble_SST))
SST_grad_quantile_sd <- c()
SST_grad_hist_sd <- c()
for(i in 1:length(Models_ensemble_SST)){
        mod <- Models_ensemble_SST[i]
        lm_SST_grad <- lm(sort(SST_diff_mean) ~ sort(SST_grad_ERA5_ensemble_hist_dat[, mod]), data = SST_grad_ERA5_ensemble_hist_dat)
        SST_grad_model_correct <- lm_SST_grad$coefficients[1] +  lm_SST_grad$coefficients[2]*SST_grad_ERA5_ensemble_hist_dat[, mod]
        ## Collect the noise of projections: 
        noise_SST_grad <- SST_grad_ERA5_ensemble_hist_dat$SST_diff_mean - SST_grad_model_correct
        SST_grad_noise_mat[, i] <- noise_SST_grad
        ## Collect the quantile standard deviations:
        SST_grad_quantile_sd[i] <- summary(lm_SST_grad)$sigma
        ## Collect the historical standard deviations:
        SST_grad_hist_sd[i] <- sd(noise_SST_grad)
    }
```


```{r QQPlot of SST_grad residuals for each model, fig.height = 7}
par(mfrow = c(3, 3))
for(i in 1:ncol(SST_grad_noise_mat)){
    qqplot(qnorm(ppoints(nrow(SST_grad_noise_mat)), mean = 0, sd = SST_grad_hist_sd[i]), SST_grad_noise_mat[, i], xlab = "Theorectical Quantiles (Normal)", ylab = "Empirical Quantiles", main = "Q-Q plot of SST gradients residuals")
    abline(0, 1, col = "red", lwd = 2)
}
```


```{r Perform shapiro normality test for residuals}
p_vals_shapiro_SST_grad <- c()
for(i in 1:ncol(SST_grad_noise_mat)){
    shapiro_test_stats <- shapiro.test(SST_grad_noise_mat[, i])
    p_vals_shapiro_SST_grad[i] <- shapiro_test_stats$p.value
}
plt <- barplot(p_vals_shapiro_SST_grad, col='steelblue', xaxt="n", main = "p-values for Shapiro-Wilk test (SST gradients residuals)", xlab = "Models", ylab = "p-values")
text(plt, par("usr")[3], labels = Models_ensemble_SST, srt = 60, adj = c(1.1,1.1), xpd = TRUE, cex=0.6)
abline(h = 0.05, lty = 2, col = "red")
text("")
```


### Mean Sea-Level Pressure


```{r Import of MSLP projections and backcast data}
if(Clim_proj_clean == T){
### Import the Sea-surface temperature data
dirc <- MSLP_projections_dirc
    MSLP_ensemble_all_scenarios_list <- list()
    for(s in 1:4){
        MSLP_data <- read.csv(paste0(dirc, "/", Scenarios_list[s], "_ensemble_MSLP_proj", ".csv")) %>% dplyr::select(-X)
        colnames(MSLP_data) = gsub("\\.", "-", colnames(MSLP_data))
        MSLP_ensemble_all_scenarios_list[[s]] <- MSLP_data %>% filter(Year >= start_year_proj & Year <= (start_year_proj + forecast_horizon -1))
    }
    
    MSLP_ensemble_hist_data  <- read.csv(MSLP_hist_backcasts_dirc) %>% dplyr::select(-X)
    colnames(MSLP_ensemble_hist_data) = gsub("\\.", "-", colnames(MSLP_ensemble_hist_data))
    Model_names_MSLP_hist <- colnames(MSLP_ensemble_hist_data)[-c(1:2)]
   } else{
        source("Miscellaneous data cleaning codes/MSLP_import_clean.R")
    }
## Join wiht historical data:
MSLP_ERA5_ensemble_hist_dat <- left_join(MSLP_ensemble_hist_data, ERA5_hist_MSLP_mm_data, by = c("Year", "Month")) %>% mutate(Dates = ymd(paste(Year, Month, "01", sep = "-")))
```

```{r Get the model names for MSLP projections}
## Get the common model names accross all scenarios for projections:
Model_names_MSLP_proj_list <- list()
for(s in 1:4){
    Model_names_MSLP_proj_list[[s]] <- colnames(MSLP_ensemble_all_scenarios_list[[s]])[-1]
}
Models_ensemble_MSLP_proj <- Reduce(intersect, Model_names_MSLP_proj_list)

## Find the intersect of model names:
Models_ensemble_MSLP <- intersect(Models_ensemble_MSLP_proj, Model_names_MSLP_hist)
```


```{r Derive and plot the projections of mean path of MSLP (after bias corrections), fig.height = 7}
## Derive the mean of MSLP after bias correction:
MSLP_mean_all_scenarios_corrected_list <- list()

for(s in 1:4){
    MSLP_correct_proj_mat <- matrix(NA, nrow = nrow(MSLP_ensemble_all_scenarios_list[[s]]), ncol = length(Models_ensemble_MSLP))
    for(i in 1:length(Models_ensemble_MSLP)){
        mod <- Models_ensemble_MSLP[i]
        lm_MSLP <- lm(sort(MSLP_mm) ~ sort(MSLP_ERA5_ensemble_hist_dat[, mod]), data = MSLP_ERA5_ensemble_hist_dat)
        MSLP_model_correct <-  lm_MSLP$coefficients[1] +  lm_MSLP$coefficients[2]*MSLP_ensemble_all_scenarios_list[[s]][, mod]
        MSLP_correct_proj_mat[, i] <- MSLP_model_correct
    }
colnames(MSLP_correct_proj_mat) = Models_ensemble_MSLP
MSLP_mean_all_scenarios_corrected_list[[s]] <- MSLP_correct_proj_mat
}
```


Collect the noise and calculate the standard deviations for MSLP: 

```{r Calculate noise and standard deviations for MSLP residuals}
MSLP_noise_mat <- matrix(NA, nrow = nrow(MSLP_ERA5_ensemble_hist_dat), ncol = length(Models_ensemble_MSLP))
MSLP_quantile_sd <- c()
MSLP_hist_sd <- c()
for(i in 1:length(Models_ensemble_MSLP)){
        mod <- Models_ensemble_MSLP[i]
        lm_MSLP <- lm(sort(MSLP_mm) ~ sort(MSLP_ERA5_ensemble_hist_dat[, mod]), data = MSLP_ERA5_ensemble_hist_dat)
        MSLP_model_correct <- lm_MSLP$coefficients[1] +  lm_MSLP$coefficients[2]*MSLP_ERA5_ensemble_hist_dat[, mod]
        ## Collect the noise of projections: 
        noise_MSLP <- MSLP_ERA5_ensemble_hist_dat$MSLP_mm - MSLP_model_correct
        MSLP_noise_mat[, i] <- noise_MSLP
        ## Collect the quantile standard deviations:
        MSLP_quantile_sd[i] <- summary(lm_MSLP)$sigma
        ## Collect the historical standard deviations:
        MSLP_hist_sd[i] <- sd(noise_MSLP)
    }
```

```{r Plot of MSLP residuals for each model, fig.height = 7}
par(mfrow = c(3, 5))
for(i in 1:ncol(MSLP_noise_mat)){
    qqplot(qnorm(ppoints(nrow(MSLP_noise_mat)), mean = 0, sd = MSLP_hist_sd[i]), MSLP_noise_mat[, i], xlab = "Theorectical Quantiles (Normal)", ylab = "Empirical Quantiles", main = "Q-Q plot of MSLP residuals")
    abline(0, 1, col = "red", lwd = 2)
}
```


```{r Perform shapiro normality test for residuals}
p_vals_shapiro_MSLP <- c()
for(i in 1:ncol(MSLP_noise_mat)){
    shapiro_test_stats <- shapiro.test(MSLP_noise_mat[, i])
    p_vals_shapiro_MSLP[i] <- shapiro_test_stats$p.value
}
plt <- barplot(p_vals_shapiro_MSLP, col='steelblue', xaxt="n", main = "p-values for Shapiro-Wilk test (SST gradients residuals)", xlab = "Models", ylab = "p-values")
text(plt, par("usr")[3], labels = Models_ensemble_SST, srt = 60, adj = c(1.1,1.1), xpd = TRUE, cex=0.6)
abline(h = 0.05, lty = 2, col = "red")
text("")
```



### Fire weather index



```{r Import and clean the FWI data}

if(Clim_proj_clean == T){
### Import the FWI data
Scenarios_list <- c("ssp126", "ssp245", "ssp370", "ssp585")
dirc <- fwi_projections_dirc
    FWI_ensemble_all_scenarios_list_mfwixx <- list()
    FWI_ensemble_all_scenarios_list_xfwixx <- list()
    for(s in 1:4){
        mfwixx_data <- read.csv(paste0(dirc, "/", Scenarios_list[s], "_ensemble_mfwixx_proj", ".csv")) %>% dplyr::select(-X)
        xfwixx_data <- read.csv(paste0(dirc, "/", Scenarios_list[s], "_ensemble_xfwixx_proj", ".csv")) %>% dplyr::select(-X)
        colnames(mfwixx_data) = gsub("\\.", "-", colnames(mfwixx_data))
        colnames(xfwixx_data) = gsub("\\.", "-", colnames(xfwixx_data))
        
        FWI_ensemble_all_scenarios_list_mfwixx[[s]] <- mfwixx_data %>% filter(Year >= start_year_proj & Year <= (start_year_proj + forecast_horizon -1))
        FWI_ensemble_all_scenarios_list_xfwixx[[s]] <- xfwixx_data %>% filter(Year >= start_year_proj & Year <= (start_year_proj + forecast_horizon -1))
    }
    #Models_ensemble_FWI_proj <- colnames(xfwixx_data)[-c(1)]
    ## Import the historical data:
    FWI_ensemble_hist_data_mfwixx <- read.csv(mfwi_hist_backcasts_dirc) %>% dplyr::select(-X)
    FWI_ensemble_hist_data_xfwixx <- read.csv(xfwi_hist_backcasts_dirc) %>% dplyr::select(-X)
    
    colnames(FWI_ensemble_hist_data_mfwixx) = gsub("\\.", "-", colnames(FWI_ensemble_hist_data_mfwixx))
    colnames(FWI_ensemble_hist_data_xfwixx) = gsub("\\.", "-", colnames(FWI_ensemble_hist_data_xfwixx))
    Models_ensemble_FWI_hist <- colnames(FWI_ensemble_hist_data_xfwixx)[-c(1)]
    
     ## Find the intersect of model names:
   #Models_ensemble_FWI <- intersect(Models_ensemble_FWI_proj, Models_ensemble_FWI_hist)
} else{
    source("Miscellaneous data cleaning codes/FWI_import_clean.R")
}
   
 ## Join with historical data:
    FWI_ERA5_ensemble_hist_data_m <- inner_join(FWI_ensemble_hist_data_mfwixx, FWI_hist_stats_sum, by = "Year")
    FWI_ERA5_ensemble_hist_data_x <- inner_join(FWI_ensemble_hist_data_xfwixx, FWI_hist_stats_sum, by = "Year")
```

```{r Get the model names for FWI projections}
## Get the common model names accross all scenarios for projections:
Model_names_FWI_proj_list <- list()
for(s in 1:4){
    Model_names_FWI_proj_list[[s]] <- colnames( FWI_ensemble_all_scenarios_list_mfwixx[[s]])[-1]
}
Models_ensemble_FWI_proj <- Reduce(intersect, Model_names_FWI_proj_list)

## Find the intersect of model names:
Models_ensemble_FWI <- intersect(Models_ensemble_FWI_proj, Models_ensemble_FWI_hist)
```

```{r Test of relationship and bias correction of FWI, fig.height = 7}
#pdf("Plotting/Climate/Bias_correction_FWI.pdf")
par(mfrow = c(3, 6))
for(mod in Models_ensemble_FWI[1:18]){
test_lm_FWI <- lm(sort(mfwixx) ~ sort(FWI_ERA5_ensemble_hist_data_m[, mod]), data = FWI_ERA5_ensemble_hist_data_m)
test_FWI_model_correct <- test_lm_FWI$coefficients[1] + test_lm_FWI$coefficients[2]*FWI_ERA5_ensemble_hist_data_m[, mod]
plot(x = FWI_ERA5_ensemble_hist_data_m$Year, FWI_ERA5_ensemble_hist_data_m$mfwixx, col = "black", ylim = c(30, 135), xlab  = "Year", ylab = "FWI", main = paste("FWI(mfwixx)", mod))
lines(x = FWI_ERA5_ensemble_hist_data_m$Year, FWI_ERA5_ensemble_hist_data_m[, mod], col = "blue", lty = 1)
lines(x = FWI_ERA5_ensemble_hist_data_m$Year, test_FWI_model_correct, col = "blue", lty = 2, lwd = 2)
legend("bottomright", legend = c("Model", "Model (Corrected)", "Observations"), col = c("blue","blue","black"), lty = c(1,2, NA), pch = c(NA, NA, 1))
}
#dev.off()
```


```{r Derive the projections of bias-corrected FWI, fig.height = 7}
## Derive the mean of extreme precipitation after bias correction:
mfwixx_mean_all_scenarios_corrected_list <- list()
for(s in 1:4){
    mfwixx_correct_proj_mat <- matrix(NA, nrow = nrow(FWI_ensemble_all_scenarios_list_mfwixx[[s]]), ncol = length(Models_ensemble_FWI))
    for(i in 1:length(Models_ensemble_FWI)){
        mod <- Models_ensemble_FWI[i]
        lm_mfwixx <- lm(sort(mfwixx) ~ sort(FWI_ERA5_ensemble_hist_data_m[, mod]), data = FWI_ERA5_ensemble_hist_data_m)
        mfwixx_model_correct <-  lm_mfwixx$coefficients[1] +  lm_mfwixx$coefficients[2]*FWI_ensemble_all_scenarios_list_mfwixx[[s]][, mod]
        mfwixx_correct_proj_mat[, i] <- mfwixx_model_correct
    }
colnames(mfwixx_correct_proj_mat) = Models_ensemble_FWI
mfwixx_mean_all_scenarios_corrected_list[[s]] <- mfwixx_correct_proj_mat 
}
```


Collect the noise and calculate the standard deviations for FWI: 

```{r Calculate noise and standard deviations for FWI residuals}
FWI_noise_mat <- matrix(NA, nrow = nrow(FWI_ERA5_ensemble_hist_data_m), ncol = length(Models_ensemble_FWI))
FWI_quantile_sd <- c()
FWI_hist_sd <- c()
for(i in 1:length(Models_ensemble_FWI)){
        mod <- Models_ensemble_FWI[i]
        lm_FWI <- lm(sort(mfwixx) ~ sort(FWI_ERA5_ensemble_hist_data_m[, mod]), data = FWI_ERA5_ensemble_hist_data_m)
        FWI_model_correct <- lm_FWI$coefficients[1] +  lm_FWI$coefficients[2]*FWI_ERA5_ensemble_hist_data_m[, mod]
        ## Collect the noise of projections: 
        noise_FWI <- FWI_ERA5_ensemble_hist_data_m$mfwixx - FWI_model_correct
        FWI_noise_mat[, i] <- noise_FWI
        ## Collect the quantile standard deviations:
        FWI_quantile_sd[i] <- summary(lm_FWI)$sigma
        ## Collect the historical standard deviations:
        FWI_hist_sd[i] <- sd(noise_FWI)
    }
```



```{r QQplot of FWI residuals for each model, fig.height = 7}
par(mfrow = c(3, 6))
for(i in 1:ncol(FWI_noise_mat)){
    qqplot(qnorm(ppoints(nrow(FWI_noise_mat)), mean = 0, sd = FWI_hist_sd[i]), FWI_noise_mat[, i], xlab = "Theorectical Quantiles (Normal)", ylab = "Empirical Quantiles", main = paste("Q-Q plot of FWI residuals:",  Models_ensemble_FWI[i]))
    abline(0, 1, col = "red", lwd = 2)
}
```


```{r Perform shapiro normality test for residuals (FWI)}
p_vals_shapiro_FWI <- c()
for(i in 1:ncol(FWI_noise_mat)){
    shapiro_test_stats <- shapiro.test(FWI_noise_mat[, i])
    p_vals_shapiro_FWI[i] <- shapiro_test_stats$p.value
}

#pdf("Plotting/Climate/p_values_FWI_residuals.pdf")
plt <- barplot(p_vals_shapiro_FWI, col='steelblue', xaxt="n", main = "p-values for Shapiro-Wilk test (FWI residuals)", xlab = "Models", ylab = "p-values")
text(plt, par("usr")[3], labels = Models_ensemble_SST, srt = 60, adj = c(1.1,1.1), xpd = TRUE, cex=0.6)
abline(h = 0.05, lty = 2, col = "red")
text("")
#dev.off()
```






### Extreme precipitation


```{r Enter the input file path names for extreme precipitation}
files_names_rx5day <- excel_sheets(extreme_precipitation_path)
files_names_rx5day_proj <- files_names_rx5day[grep("ssp", files_names_rx5day)]
files_names_rx5day_hist <- files_names_rx5day[grep("historical", files_names_rx5day)]
```

```{r Import the projections of extreme precipitation}
Scenarios_list <- c("ssp126", "ssp245", "ssp370", "ssp585")
rx5day_ensemble_all_scenarios_list <- list()
for(s in 1:length(Scenarios_list)){
    scenario <- Scenarios_list[s]
    files_list <- files_names_rx5day_proj[grep(scenario, files_names_rx5day_proj)]
    rx5day_ensemble_list <- list()
    for(i in 1:length(files_list)){
        model_name <- sub(paste0('.*', paste0(scenario,"_")), '', files_list[i])
        
        rx5day <- read_excel(extreme_precipitation_path, sheet = files_list[i]) %>% dplyr::select(-c(code, name)) %>% pivot_longer(cols = everything(), names_to = "Year", values_to = model_name) %>% mutate(Year = as.numeric(substr(Year, 1, 4))) %>% filter(Year >= start_year_proj & Year <= (start_year_proj + forecast_horizon -1))
     
        rx5day_ensemble_list[[i]] <- rx5day
    }
    rx5day_ensemble_data_0 <- do.call(cbind, rx5day_ensemble_list) %>% dplyr::select(-c(Year))
    rx5day_ensemble_data_1 <- cbind(Year = rx5day$Year, rx5day_ensemble_data_0)
    ## Combine all the ensemble model outputs into a data.frame for each scenario
    rx5day_ensemble_all_scenarios_list[[s]] <- rx5day_ensemble_data_1
}
```

```{r Import the historical backcasts of extreme precipitation}
rx5day_ensemble_hist_list <- list()
for(i in 1:length(files_names_rx5day_hist)){
        ## Get the model names:
        model_name <- sub('.*historical_', '', files_names_rx5day_hist[i])
        ## Obtain the extreme precipitation data:
        rx5day <- read_excel(extreme_precipitation_path, sheet = files_names_rx5day_hist[i]) %>% dplyr::select(-c(code, name)) %>% pivot_longer(cols = everything(), names_to = "Year", values_to = model_name) %>% mutate(Year = as.numeric(substr(Year, 1, 4)))
        ## Store it into a list: 
        rx5day_ensemble_hist_list[[i]] <- rx5day
}
rx5day_ensemble_hist_data_0 <- do.call(cbind, rx5day_ensemble_hist_list) %>% dplyr::select(-c(Year))
rx5day_ensemble_hist_data_1 <- cbind(Year = rx5day_ensemble_hist_list[[1]]$Year, rx5day_ensemble_hist_data_0)
## Join with the historical observations data:
rx5day_ERA5_ensemble_hist_data <- inner_join(Precipitation_ERA5_hist_data, rx5day_ensemble_hist_data_1, by = "Year")
```

```{r Get the model names for rx5day projections}
## Get the common model names accross all scenarios for projections:
Model_names_rx5day_proj_list <- list()
for(s in 1:4){
    Model_names_rx5day_proj_list[[s]] <- colnames(rx5day_ensemble_all_scenarios_list[[s]])[-1]
}
Models_ensemble_rx5day_proj <- Reduce(intersect, Model_names_rx5day_proj_list)
```


```{r Get the intersection model names for rx5day}
Model_names_rx5day_hist_raw <- colnames(rx5day_ensemble_hist_data_1)[-1]

Model_names_rx5day_hist <- case_when(Model_names_rx5day_hist_raw == "access-cm" ~ "access-cm2",
          Model_names_rx5day_hist_raw == "access-es" ~ "access-esm1-5",
          Model_names_rx5day_hist_raw == "bcc-csm2-" ~ "bcc-csm2-mr",
          Model_names_rx5day_hist_raw == "ipsl-cm6a" ~ "ipsl-cm6a-lr",
          Model_names_rx5day_hist_raw == "kace-1-0-" ~ "kace-1-0-g",
          Model_names_rx5day_hist_raw == "mpi-esm1-" ~ "mpi-esm1-2-hr",
          Model_names_rx5day_hist_raw == "mpi-esm1 1"~ "mpi-esm1-2-lr",
          Model_names_rx5day_hist_raw == "mri-esm2-" ~ "mri-esm2-0",
          Model_names_rx5day_hist_raw == "noresm2-l" ~ "noresm2-lm",
          Model_names_rx5day_hist_raw == "noresm2-m" ~ "noresm2-mm",
          T ~ Model_names_rx5day_hist_raw
          )

Models_ensemble_rx5day <- intersect(Models_ensemble_rx5day_proj, Model_names_rx5day_hist)


colnames(rx5day_ERA5_ensemble_hist_data) = c("Year", "Precipitation", "rx1day", "rx5day", Model_names_rx5day_hist)
```





```{r Test of relationship and bias correction of rx5day, fig.height = 7}
par(mfrow = c(4, 5))
for(mod in Models_ensemble_rx5day[1:17]){
test_lm_rx5day <- lm(sort(rx5day) ~ sort(rx5day_ERA5_ensemble_hist_data[, mod]), data = rx5day_ERA5_ensemble_hist_data)
test_rx5day_model_correct <- test_lm_rx5day$coefficients[1] + test_lm_rx5day$coefficients[2]*rx5day_ERA5_ensemble_hist_data[, mod]
plot(x = rx5day_ERA5_ensemble_hist_data$Year, rx5day_ERA5_ensemble_hist_data$rx5day, col = "black", ylim = c(30, 135), xlab  = "Year", ylab = "rx5day", main = paste("rx5day:", mod))
lines(x = rx5day_ERA5_ensemble_hist_data$Year, rx5day_ERA5_ensemble_hist_data[, mod], col = "blue", lty = 1)
lines(x = rx5day_ERA5_ensemble_hist_data$Year, test_rx5day_model_correct, col = "blue", lty = 2, lwd = 2)
legend("bottomright", legend = c("Model", "Model (Corrected)", "Observations"), col = c("blue","blue","black"), lty = c(1,2, NA), pch = c(NA, NA, 1))
}
```



```{r Derive the projections of extreme precipitation (after bias corrections), fig.height = 7}
## Derive the mean of extreme precipitation after bias correction:
rx5day_mean_all_scenarios_corrected_list <- list()

for(s in 1:4){
    rx5day_correct_proj_mat <- matrix(NA, nrow = nrow(rx5day_ensemble_all_scenarios_list[[s]]), ncol = length(Models_ensemble_rx5day))
    for(i in 1:length(Models_ensemble_rx5day)){
        mod <- Models_ensemble_rx5day[i]
        lm_rx5day <- lm(sort(rx5day) ~ sort(rx5day_ERA5_ensemble_hist_data[, mod]), data = rx5day_ERA5_ensemble_hist_data)
        rx5day_model_correct <-  lm_rx5day$coefficients[1] +  lm_rx5day$coefficients[2]*rx5day_ensemble_all_scenarios_list[[s]][, mod]
        rx5day_correct_proj_mat[, i] <- rx5day_model_correct
    }
colnames(rx5day_correct_proj_mat) = Models_ensemble_rx5day
rx5day_mean_all_scenarios_corrected_list[[s]] <- rx5day_correct_proj_mat
}
```


Collect the noise and calculate the standard deviations for rx5day: 

```{r Calculate noise and standard deviations for rx5day residuals}
rx5day_noise_mat <- matrix(NA, nrow = nrow(rx5day_ERA5_ensemble_hist_data), ncol = length(Models_ensemble_rx5day))
rx5day_quantile_sd <- c()
rx5day_hist_sd <- c()
for(i in 1:length(Models_ensemble_rx5day)){
        mod <- Models_ensemble_rx5day[i]
        lm_rx5day <- lm(sort(rx5day) ~ sort(rx5day_ERA5_ensemble_hist_data[, mod]), data = rx5day_ERA5_ensemble_hist_data)
        rx5day_model_correct <- lm_rx5day$coefficients[1] +  lm_rx5day$coefficients[2]*rx5day_ERA5_ensemble_hist_data[, mod]
        ## Collect the noise of projections: 
        noise_rx5day <- rx5day_ERA5_ensemble_hist_data$rx5day - rx5day_model_correct
        rx5day_noise_mat[, i] <- noise_rx5day
        ## Collect the quantile standard deviations:
        rx5day_quantile_sd[i] <- summary(lm_rx5day)$sigma
        ## Collect the historical standard deviations:
        rx5day_hist_sd[i] <- sd(noise_rx5day)
    }
```


```{r QQplot of rx5day residuals for each model, fig.height = 7}
par(mfrow = c(3, 6))
for(i in 1:ncol(rx5day_noise_mat)){
    qqplot(qnorm(ppoints(nrow(rx5day_noise_mat)), mean = 0, sd = rx5day_hist_sd[i]), rx5day_noise_mat[, i], xlab = "Theorectical Quantiles (Normal)", ylab = "Empirical Quantiles", main = "Q-Q plot of rx5day residuals")
    abline(0, 1, col = "red", lwd = 2)
}
```


```{r Perform shapiro normality test for residuals (rx5day)}
p_vals_shapiro_rx5day <- c()
for(i in 1:ncol(rx5day_noise_mat)){
    shapiro_test_stats <- shapiro.test(rx5day_noise_mat[, i])
    p_vals_shapiro_rx5day[i] <- shapiro_test_stats$p.value
}
plt <- barplot(p_vals_shapiro_rx5day, col='steelblue', xaxt="n", main = "p-values for Shapiro-Wilk test (rx5day residuals)", xlab = "Models", ylab = "p-values")
text(plt, par("usr")[3], labels = Models_ensemble_SST, srt = 60, adj = c(1.1,1.1), xpd = TRUE, cex=0.6)
abline(h = 0.05, lty = 2, col = "red")
text("")
```



## Import of economic projections


```{r Import of real GDP projections}
GDP_projections_SSP <- read.csv(GDP_projections_dat_path, skip = 1) %>%
    filter(iso3c == "AUS" & x == "gdp" & version == "original" & unit == "million 2005$PPP") %>%
    dplyr::select(-c(x, version)) %>%
    pivot_longer(cols = contains("X"), names_to = "Year", values_to = "GDP(PPP)") %>%
    mutate(year = as.numeric(substring(Year, 2, nchar(Year)))) %>%
    mutate(value = `GDP(PPP)`*1000000) %>%
    dplyr::select(iso3c, SSP, year, value)

## Since the latest year available for conversion in the package is 2020, we need to manually convert to the 2022 value by using the GDP deflator: 

GDP_scalar_22_20 <- AU_GDP_deflator[AU_GDP_deflator$Year == 2022, ]$`GDP deflator (2022)`/AU_GDP_deflator[AU_GDP_deflator$Year == 2020, ]$`GDP deflator (2022)`

GDP_projections_SSP_converted <- convertGDP(
  gdp = GDP_projections_SSP,
  unit_in = "constant 2005 Int$PPP",
  unit_out = "constant 2020 LCU"
) %>%
    mutate(Real_GDP = value*GDP_scalar_22_20 ) %>%
    dplyr::select(SSP, year, Real_GDP) %>%
    rename(Year = year) %>%
    filter(Year >= 2022) %>%
    pivot_wider(names_from = "SSP", values_from = Real_GDP)

colnames(GDP_projections_SSP_converted) = c("Year","SSP 1.00", "SSP 2.00", "SSP 3.00", "SSP 4.00", "SSP 5.00")


interpolated_years <- 2021:2100
GDP_projections_SSP_interpolate <- data.frame(Year = interpolated_years,
                                              `SSP 1.00` = spline(GDP_projections_SSP_converted$Year, GDP_projections_SSP_converted$`SSP 1.00`, n = length(interpolated_years))$y,
                                              `SSP 2.00` = spline(GDP_projections_SSP_converted$Year, GDP_projections_SSP_converted$`SSP 2.00`, n = length(interpolated_years))$y,
                                              `SSP 3.00` = spline(GDP_projections_SSP_converted$Year, GDP_projections_SSP_converted$`SSP 3.00`, n = length(interpolated_years))$y,
                                               `SSP 4.00` = spline(GDP_projections_SSP_converted$Year, GDP_projections_SSP_converted$`SSP 4.00`, n = length(interpolated_years))$y, 
                                              `SSP 5.00` = spline(GDP_projections_SSP_converted$Year, GDP_projections_SSP_converted$`SSP 5.00`, n = length(interpolated_years))$y) 

colnames(GDP_projections_SSP_interpolate) = colnames(GDP_projections_SSP_converted)


### Construct a GDP growth data:
GDP_projections_SSP_growth <- data.frame(Year = interpolated_years,
                                              `SSP 1.00` = GDP_projections_SSP_interpolate$`SSP 1.00`/lag(GDP_projections_SSP_interpolate$`SSP 1.00`)-1,
                                              `SSP 2.00` = GDP_projections_SSP_interpolate$`SSP 2.00`/lag(GDP_projections_SSP_interpolate$`SSP 2.00`)-1,
                                              `SSP 3.00` = GDP_projections_SSP_interpolate$`SSP 3.00`/lag(GDP_projections_SSP_interpolate$`SSP 3.00`)-1,
                                               `SSP 4.00` = GDP_projections_SSP_interpolate$`SSP 4.00`/lag(GDP_projections_SSP_interpolate$`SSP 4.00`)-1,
                                              `SSP 5.00` = GDP_projections_SSP_interpolate$`SSP 5.00`/lag(GDP_projections_SSP_interpolate$`SSP 5.00`)-1) %>%
    filter(Year >= 2020)

colnames(GDP_projections_SSP_growth) = colnames(GDP_projections_SSP_converted)

```




```{r Import of population projections}
### Import the data: 
AU_pop_projections <- read_excel(Pop_projections_dat_path, sheet = "data") %>% dplyr::select(c(Scenario, contains("2"))) %>% drop_na() %>% pivot_longer(cols = contains("2"), names_to = "Year", values_to = "Population_raw") %>% mutate(Population = Population_raw*1000000) %>% dplyr::select(Year, Scenario, Population) %>% pivot_wider(names_from = Scenario, values_from = Population)
AU_pop_projections$Year = as.numeric(AU_pop_projections$Year)

#### Interpolate the data: 
interpolated_years <- min(AU_pop_projections$Year):2100
Pop_projections_SSP_interpolate <- data.frame(Year = interpolated_years,
                                              `SSP 1.00` = spline(AU_pop_projections$Year, AU_pop_projections$SSP1, n = length(interpolated_years))$y,
                                              `SSP 2.00` = spline(AU_pop_projections$Year, AU_pop_projections$SSP2, n = length(interpolated_years))$y,
                                              `SSP 3.00` = spline(AU_pop_projections$Year, AU_pop_projections$SSP3, n = length(interpolated_years))$y,
                                              `SSP 5.00` = spline(AU_pop_projections$Year, AU_pop_projections$SSP5, n = length(interpolated_years))$y) %>% filter(Year >= 2023)
```


```{r Derive exposure projections based on population projections}
AU_exposure_projections <- data.frame(Year = Pop_projections_SSP_interpolate$Year, Exposure_SSP1 = lm_exposure$coefficients[1] + lm_exposure$coefficients[2]*Pop_projections_SSP_interpolate$SSP.1.00,
Exposure_SSP2 = lm_exposure$coefficients[1] + lm_exposure$coefficients[2]*Pop_projections_SSP_interpolate$SSP.2.00,
Exposure_SSP3 = lm_exposure$coefficients[1] + lm_exposure$coefficients[2]*Pop_projections_SSP_interpolate$SSP.3.00,
Exposure_SSP5 = lm_exposure$coefficients[1] + lm_exposure$coefficients[2]*Pop_projections_SSP_interpolate$SSP.5.00)
colnames(AU_exposure_projections) = colnames(Pop_projections_SSP_interpolate)
```

```{r Plot of exposure projections}
plot(x = AU_exposure_projections$Year, y = AU_exposure_projections[, 2], col = col_list[1], type = "l", ylim = c(4e7, 3e8), xlab = "Years", ylab = "Exposure units")
for(s in 2:4){
    points(x = AU_exposure_projections$Year, y = AU_exposure_projections[, s + 1], col = col_list[s], type = "l")
}
```




## Simulation of all DFA components


### Define the starting values and model version

Inflation rates: 

```{r Import the CPI regression coefficients}
CPI_regress_coef <- read.csv(CPI_regress_coef_path)
colnames(CPI_regress_coef) = c("Covariates", "Coefficients")
## Extract the relevant coefficients required for projections: 
CPI_coef_vec <- filter(CPI_regress_coef, grepl('Tmean_d', Covariates))$Coefficients
#filter(CPI_regress_coef, grepl('Tmean_d', Covariates))
```

```{r Define the historical reference period}
 ## Filter the temperature data within the selected range:
Temp_mean_90_21_m <- tas_ERA5_hist_dat %>% filter(Year >= 1990 & Year <= 2021) %>%
    group_by(Month) %>% summarise(Temperature_mean_hist = mean(NS_temperature))
tas_ERA5_hist_dat_CPI_input <- tas_ERA5_hist_dat %>% rename(Temperature = NS_temperature)
```

```{r Define the Inflation rates start values}
inflation_rate_start <- mean(Au_inflation_annual[Au_inflation_annual$Year >= 2021, ]$Inflation_rate)
CPI_start <-  mean(Au_inflation_annual[Au_inflation_annual$Year >= 2021, ]$CPI)
```

Interest rates:

```{r Define the starting value for interest rates simulations}
z_SR_start <- last(lm_SR_GDP_potential$residuals)
```

```{r Define the range of years used in GDP projections data}
GDP_projections_SSP_growth_select <- GDP_projections_SSP_growth %>%
    filter(Year >= (start_year_proj-1)) %>% filter(Year <= (start_year_proj-1 + forecast_horizon)) %>% dplyr::select(`SSP 1.00`, `SSP 2.00`, `SSP 3.00`, `SSP 5.00`)
```

Hazard module:

```{r Define the selected models for each hazard}
## Flood
glm_Flood_freq_select <- glm_Flood_freq_3
glm_Flood_sev_select <- glm_Flood_sev_3
## Bushfire
glm_BF_freq_select <- glm_Bush_freq_1
glm_BF_sev_select <- glm_Bush_sev_0
## TC
glm_TC_freq_select <- glm_Cyc_mm_freq_3
glm_TC_sev_select <- glm_Cyc_sev_0
## Storm
glm_Storm_freq_select <- glm_Storm_mm_freq_1
glm_Storm_sev_select <- glm_Storm_sev_mm_1
## ECL
glm_ECL_freq_select <- glm_ECL_freq_2
glm_ECL_sev_select <- glm_ECL_sev_0
## Hailstorm
glm_HS_freq_select <- glm_HS_freq
glm_HS_sev_select <- glm_HS_sev_0
```

Liabilities module:

```{r Define the starting GDP level and range of GDP forecasts needed}
Real_GDP_start <- AU_GDP_annual[AU_GDP_annual$Year == 2022, ]$Real_GDP
Real_GDP_forecasts <- GDP_projections_SSP_interpolate %>% filter(Year >= start_year_proj) %>%
    filter(Year <= (start_year_proj + forecast_horizon -1)) %>%
    dplyr::select(`SSP 1.00`, `SSP 2.00`, `SSP 3.00`, `SSP 5.00`)

Real_GDP_forecasts_all <- GDP_projections_SSP_interpolate %>% filter(Year >= (start_year_proj-2)) %>%
    filter(Year <= (start_year_proj + forecast_horizon -1)) %>%
    dplyr::select(`SSP 1.00`, `SSP 2.00`, `SSP 3.00`, `SSP 5.00`)
```

```{r Define the range of exposure forecasts needed}
Exposure_start <- GI_Ultimate_Losses_PM[GI_Ultimate_Losses_PM$Year == 2022, ]$Exposure
Exposure_forecasts <- AU_exposure_projections %>% filter(Year >= start_year_proj) %>%
    filter(Year <= (start_year_proj + forecast_horizon -1)) %>%
    dplyr::select(`SSP.1.00`, `SSP.2.00`, `SSP.3.00`, `SSP.5.00`)
```

```{r Define the non-CAT model selected}
model_nonCAT <- tw_PM_0
model_nonCAT_other <- tw_other_0
power_tw <- 1.99 ## Enter the power index based on the regression output from `model_nonCAT`
```


### Perform the simulations

#### Simulation of climate, hazard, MEV and liabilites modules 

Check the CMIP 6 ensemble inputs: 

```{r Check of CMIP6 model ensembles}
print("============ Check for historical backcasts ============")

    if(sum(Models_ensemble_tas %in% colnames(tas_ERA5_ensemble_hist_dat)[-c(1,2)] == F) == 0){print("TAS: PASS")}else{print("TAS: Fail")}
    if(sum(Models_ensemble_ta %in% colnames(ta_ERA5_ensemble_hist_dat)[-c(1,2)] == F) == 0){print("TA: PASS")}else{print("TA: Fail")}
    if(sum(Models_ensemble_SST %in% colnames(SST_ERA5_ensemble_hist_dat)[-c(1,2)] == F) == 0){print("SST: PASS")}else{print("SST: Fail")}
    if(sum(Models_ensemble_MSLP %in% colnames(MSLP_ERA5_ensemble_hist_dat)[-c(1,2)] == F) == 0){print("MSLP: PASS")}else{print("MSLP: Fail")}
    if(sum(Models_ensemble_FWI %in% colnames(FWI_ERA5_ensemble_hist_data_m)[-c(1)] == F) == 0){print("FWI: PASS")}else{print("FWI: Fail")}
    if(sum(Models_ensemble_rx5day %in% colnames(rx5day_ERA5_ensemble_hist_data)[-c(1)] == F) == 0){print("rx5day: PASS")}else{print("rx5day: Fail")}



print("============ Check for climate projections ============")
for(s in 1:4){
    print(paste("=========== Scenarios", s, "==========="))
    if(sum(Models_ensemble_tas %in% colnames(tas_ensemble_all_scenarios_list[[s]])[-c(1,2)] == F) == 0){print("TAS: PASS")}else{print("TAS: Fail")}
    if(sum(Models_ensemble_ta %in% colnames(ta_ensemble_all_scenarios_list[[s]])[-c(1,2)] == F) == 0){print("TA: PASS")}else{print("TA: Fail")}
    if(sum(Models_ensemble_SST %in% colnames(SST_ensemble_all_scenarios_list[[s]])[-c(1,2)] == F) == 0){print("SST: PASS")}else{print("SST: Fail")}
    if(sum(Models_ensemble_MSLP %in% colnames(MSLP_ensemble_all_scenarios_list[[s]])[-c(1,2)] == F) == 0){print("MSLP: PASS")}else{print("MSLP: Fail")}
    if(sum(Models_ensemble_FWI %in% colnames(FWI_ensemble_all_scenarios_list_mfwixx[[s]])[-c(1)] == F) == 0){print("FWI: PASS")}else{print("FWI: Fail")}
    if(sum(Models_ensemble_rx5day %in% colnames(rx5day_ensemble_all_scenarios_list[[s]])[-c(1)] == F) == 0){print("rx5day: PASS")}else{print("rx5day: Fail")}
}
```


Perform simulations of climate variables, interest rates, inflation rates, frequency and severity of CAT events, and insurance loss: 

```{r Simulation of all DFA components}
set.seed(seed)
if(run == T){
DFA_components_all_scenarios <- list()
for(s in 1:4){
    # s <- 4
    ## Define the economic development input:
    GDP_pred <-  Real_GDP_forecasts[, s]
    GDP_growth_pred <- GDP_projections_SSP_growth_select[, s]
    Exposure_pred <- Exposure_forecasts[, s]
    
    ## Define empty matrices to store the ouptuts
    ##### Climate module ##### 
    NS_Temp_sims_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    Air_Temp_sims_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    MSLP_sims_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    SST_Temp_sims_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    SST_grad_Temp_sims_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    FWI_mfwixx_sims_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    rx5day_sims_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    ##### MEV module ##### 
    simulated_results_inflation_clim <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    Interest_Rates_sim_SR <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    Interest_Rates_sim_SR_alt <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    Interest_Rates_norminal <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    Interest_Rates_norminal_alt <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    #### Hazard module #####
    Flood_count_sims_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    Flood_loss_sims_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    BF_count_sims_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    BF_loss_sims_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    TC_count_sims_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    TC_loss_sims_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    Storm_count_sims_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    Storm_loss_sims_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    ECL_count_sims_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    ECL_loss_sims_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    HS_count_sims_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    HS_loss_sims_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    #### Liabilities module #####
    CAT_norm_loss_sims_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    CAT_nominal_loss_sims_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    CAT_damage_ratio_sims_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    
    Non_CAT_losses_agg <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    Non_CAT_losses_agg_withInflation <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    
    Non_CAT_other_losses_agg <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    Non_CAT_other_losses_agg_withInflation <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
        
    
    for(i in 1:n.sims){
            ########################################################################
            #### #### #### #### Climate module simulations #### #### #### #### #### 
            ########################################################################
            ## Near surface temperature
            NS_Temp_sims <- simulate_clim_var(scenario_index = s, ensemble_hist_dat = tas_ERA5_ensemble_hist_dat, var_name = "NS_temperature", ensemble_all_scenarios_list = tas_ensemble_all_scenarios_list, Model_list = Models_ensemble_tas, n.sims = 1, start_year = start_year_proj, forecast_period = forecast_horizon)
            ## Air temperature
            Air_Temp_sims <- simulate_clim_var(scenario_index = s, ensemble_hist_dat = ta_ERA5_ensemble_hist_dat, var_name = "Air_temperature", ensemble_all_scenarios_list = ta_ensemble_all_scenarios_list, Model_list = Models_ensemble_ta, n.sims = 1, start_year = start_year_proj, forecast_period = forecast_horizon)
            ## Sea-surface temperature
            SST_sims <- simulate_clim_var(scenario_index = s, ensemble_hist_dat = SST_ERA5_ensemble_hist_dat, var_name = "SST_mean_mm", ensemble_all_scenarios_list = SST_ensemble_all_scenarios_list, Model_list = Models_ensemble_SST, n.sims = 1, start_year = start_year_proj, forecast_period = forecast_horizon)
            ## Sea-surface temperature gradients
            SST_grad_sims <- simulate_clim_var(scenario_index = s, ensemble_hist_dat = SST_grad_ERA5_ensemble_hist_dat, var_name = "SST_diff_mean", ensemble_all_scenarios_list = SST_grad_ensemble_all_scenarios_list, Model_list = Models_ensemble_SST, n.sims = 1, start_year = start_year_proj, forecast_period = forecast_horizon)
            ## MSLP
            MSLP_sims <- simulate_clim_var(scenario_index = s, ensemble_hist_dat = MSLP_ERA5_ensemble_hist_dat, var_name = "MSLP_mm", ensemble_all_scenarios_list = MSLP_ensemble_all_scenarios_list, Model_list = Models_ensemble_MSLP, n.sims = 1, start_year = start_year_proj, forecast_period = forecast_horizon)
            ## FWI
            FWI_mfwixx_sims <- simulate_clim_var(scenario_index = s, ensemble_hist_dat = FWI_ERA5_ensemble_hist_data_m, var_name = "mfwixx", ensemble_all_scenarios_list = FWI_ensemble_all_scenarios_list_mfwixx, Model_list = Models_ensemble_FWI, n.sims = 1, start_year = start_year_proj, forecast_period = forecast_horizon)
            ## Extreme precipitation
            rx5day_sims <- simulate_clim_var(scenario_index = s, ensemble_hist_dat = rx5day_ERA5_ensemble_hist_data, var_name = "rx5day", ensemble_all_scenarios_list = rx5day_ensemble_all_scenarios_list, Model_list = Models_ensemble_rx5day, n.sims = 1, start_year = start_year_proj, forecast_period = forecast_horizon)
            
            ########################################################################
    
            
            ########################################################################
            #### #### #### #### Interest rates simulations #### #### #### #### #### 
            ########################################################################
            
            ## Simulate the baseline interest rates:
            simulated_SR_raw <- simulate_SR_AR1(intercept = SR_mod_AR1_potentialGDP$fit$coef[2], phi = SR_mod_AR1_potentialGDP$fit$coef[1], sigma = sqrt(SR_mod_AR1_potentialGDP$fit$sigma2), n = (forecast_horizon + 1), start_value = z_SR_start) + predict(lm_SR_GDP_potential, newdata = data.frame(GDP_potential_growth = GDP_growth_pred))
            simulated_SR <- simulated_SR_raw[-1]
            
            ########################################################################
            
            ########################################################################
            #### #### #### #### Inflation rates simulations #### #### #### #### #### 
            ########################################################################
            full_pred_data <- tas_ensemble_all_scenarios_list[[s]] %>% filter(Year >= start_year_proj) %>% filter(Year <= (start_year_proj + forecast_horizon-1))
            ## Create a data frame with the simulated temperature: 
            Tmean_proj_data <- data.frame(Year = full_pred_data$Year, Month = full_pred_data$Month, Temperature = as.numeric(NS_Temp_sims$sim_var_mm))
            ## Create a data.frame for the inflation rates covariates: 
            Inflation_covariates <- convert_T_to_covariates_func(Tmean_proj_data, Temp_mean_90_21_m, tas_ERA5_hist_dat_CPI_input, proj_start_year = start_year_proj)
            ## Derive the annual climate impact on inflation rates:
            Inflation_climate_impact <- Infla_climate_pressure_func(Projected_Years = full_pred_data$Year, Tmean_cov_mat = Inflation_covariates$Tmean_cov_mat, CPI_coef = CPI_coef_vec)
            ## Calculate the climate-adjusted inflation rates:
             Inflation_rates_sim <- simulateAR1_Inflation_Clim(mu = inflation_mod_bench$coef[2], phi = inflation_mod_bench$coef[1], sigma = sqrt(inflation_mod_bench$sigma2), n = forecast_horizon, start_value = inflation_rate_start, start_value_CPI = CPI_start, Climate_pressure = Inflation_climate_impact)
            
             ########################################################################
            #### #### #### #### Calculate the nominal rates #### #### #### #### #### 
            ########################################################################
             
            ### Calculate the nominal rates:
            simulated_SR_nominal <- pmax(simulated_SR + Inflation_rates_sim$Inflation_rate, 0) ## Floor the nominal rates at zero
    
             
            ########################################################################
            #### #### #### #### Hazard module simulations #### #### #### #### #### 
            ########################################################################
            
            ## Flood
            Flood_count_sim <- simulate_Pois_GLM(glm_Flood_freq_select, newdata = data.frame(Year = start_year_proj:(start_year_proj+forecast_horizon-1), rx5day = rx5day_sims$sim_var_year))
            Flood_total_norm_loss_sim <- sim_LN_CAT_losses_func(glm_Flood_sev_select, newdata = data.frame(Year = start_year_proj:(start_year_proj+forecast_horizon-1), rx5day = rx5day_sims$sim_var_year), CAT_count = Flood_count_sim$Count_yy, interval_year = T)
            
            ## Bushfire:
            BF_count_sim <- simulate_Pois_GLM(glm_BF_freq_select, newdata = data.frame(Year = start_year_proj:(start_year_proj+forecast_horizon-1), mfwixx  = FWI_mfwixx_sims$sim_var_year))
            BF_total_norm_loss_sim <- sim_LN_CAT_losses_func(glm_BF_sev_select, newdata = data.frame(Year = start_year_proj:(start_year_proj+forecast_horizon-1), mfwixx  = FWI_mfwixx_sims$sim_var_year), CAT_count = BF_count_sim$Count_yy, interval_year = T)
            
            ## TC
            TC_count_sim <- simulate_Pois_GLM(glm_TC_freq_select, newdata = data.frame(Year = full_pred_data$Year, SST_mean_mm  = SST_sims$sim_var_mm, MSLP_mm = MSLP_sims$sim_var_mm))
            TC_total_norm_loss_sim <- sim_LN_CAT_losses_func(glm_TC_sev_select, newdata = data.frame(Year = full_pred_data$Year, SST_mean_mm  = SST_sims$sim_var_mm, MSLP_mm = MSLP_sims$sim_var_mm), CAT_count = TC_count_sim$Count_mm, interval_year = T)
            
            ## Storm
            Storm_count_sim <- simulate_Pois_GLM(glm_Storm_freq_select, newdata = data.frame(Year = full_pred_data$Year, SST_mean_mm  = SST_sims$sim_var_mm))
            Storm_total_norm_loss_sim <- sim_LN_CAT_losses_func(glm_Storm_sev_select, newdata = data.frame(Year = full_pred_data$Year, SST_mean_mm  = SST_sims$sim_var_mm), CAT_count = Storm_count_sim$Count_mm, interval_year = T)
            
            ## ECL
            ECL_count_sim <- simulate_Pois_GLM(glm_ECL_freq_select, newdata = data.frame(Year = full_pred_data$Year, SST_diff_mean = SST_grad_sims$sim_var_mm))
            ECL_total_norm_loss_sim <- sim_LN_CAT_losses_func(glm_ECL_sev_select, newdata = data.frame(Year = full_pred_data$Year, SST_diff_mean = SST_grad_sims$sim_var_mm), CAT_count = ECL_count_sim$Count_mm, interval_year = T)
            
            ## Hailstorm
            HS_count_sim <- simulate_Pois_GLM(glm_HS_freq_select, newdata = data.frame(Year = full_pred_data$Year, Air_temperature = Air_Temp_sims$sim_var_mm, NS_temperature = NS_Temp_sims$sim_var_mm))
            HS_total_norm_loss_sim <- sim_LN_CAT_losses_func(glm_HS_sev_select, newdata = data.frame(Year = full_pred_data$Year, Air_temperature = Air_Temp_sims$sim_var_mm, NS_temperature = NS_Temp_sims$sim_var_mm), CAT_count = HS_count_sim$Count_mm, interval_year = T)
             ########################################################################
            #### #### #### #### Liabilities module #### #### #### #### #### 
            ########################################################################
            # CAT losses:
            ## Calculate normalised aggregate losses
             sim_CAT_losses <- Flood_total_norm_loss_sim + BF_total_norm_loss_sim + TC_total_norm_loss_sim + Storm_total_norm_loss_sim + ECL_total_norm_loss_sim + HS_total_norm_loss_sim
            ## Adjust the CAT loss for wealth exposure, and cap the simulated losses at the exposure:
            sim_CAT_losses_GDP_adj <- pmin(sim_CAT_losses*GDP_pred/Real_GDP_start,GDP_pred)
            sim_CAT_damage_ratio <- sim_CAT_losses_GDP_adj/GDP_pred
            ## Also adjust the CAT loss for price level:
            sim_CAT_losses_full_adj <- sim_CAT_losses_GDP_adj*Inflation_rates_sim$CPI/CPI_start
            
            # Non-CAT losses:
            ## Non-CAT losses for property, motor, fire and ISR: 
            sim_Non_CAT_avg <- sim_tweedie(mu = rep(exp(model_nonCAT$coefficients), forecast_horizon), phi = summary(model_nonCAT)$dispersion, power = power_tw)
            sim_Non_CAT_agg <- sim_Non_CAT_avg*Exposure_pred
            sim_Non_CAT_agg_withInflation <- sim_Non_CAT_agg*Inflation_rates_sim$CPI/CPI_start
             ## Non-CAT losses for other BoLs:
            sim_Non_CAT_other_avg <- sim_tweedie(mu = rep(exp(model_nonCAT_other$coefficients), forecast_horizon), phi = summary(model_nonCAT_other)$dispersion, power = power_tw)
            sim_Non_CAT_other_agg <- sim_Non_CAT_other_avg*Exposure_forecasts_other
            sim_Non_CAT_other_agg_withInflation <- sim_Non_CAT_other_agg*Inflation_rates_sim$CPI/CPI_start
            
        
            
        ########################################################################
        ### Store the outputs ####
        ########################################################################
            
        ##### Climate module ##### 
        NS_Temp_sims_mat[, i] <- NS_Temp_sims$sim_var_year
        Air_Temp_sims_mat[, i] <- Air_Temp_sims$sim_var_year
        SST_Temp_sims_mat[, i] <- SST_sims$sim_var_year
        SST_grad_Temp_sims_mat[, i] <- SST_grad_sims$sim_var_year
        MSLP_sims_mat[,i] <- MSLP_sims$sim_var_year
        FWI_mfwixx_sims_mat[, i] <- FWI_mfwixx_sims$sim_var_year
        rx5day_sims_mat[, i] <- rx5day_sims$sim_var_year
        
        ##### MEV module ##### 
        simulated_results_inflation_clim[, i] <- Inflation_rates_sim$Inflation_rate
        Interest_Rates_sim_SR[, i] <- simulated_SR
        Interest_Rates_norminal[, i] <- simulated_SR + Inflation_rates_sim$Inflation_rate
        
        ### Hazard module ### 
            
        Flood_count_sims_mat[, i] <- Flood_count_sim$Count_yy
        Flood_loss_sims_mat[, i] <- Flood_total_norm_loss_sim
        BF_count_sims_mat[, i] <- BF_count_sim$Count_yy
        BF_loss_sims_mat[, i] <- BF_total_norm_loss_sim
        TC_count_sims_mat[, i] <- TC_count_sim$Count_yy
        TC_loss_sims_mat[, i] <- TC_total_norm_loss_sim
        Storm_count_sims_mat[, i] <- Storm_count_sim$Count_yy
        Storm_loss_sims_mat[, i] <- Storm_total_norm_loss_sim
        ECL_count_sims_mat[, i] <- ECL_count_sim$Count_yy
        ECL_loss_sims_mat[, i] <- ECL_total_norm_loss_sim
        HS_count_sims_mat[, i] <- HS_count_sim$Count_yy
        HS_loss_sims_mat[, i] <- HS_total_norm_loss_sim
        
        #### Liabilities module #####
        CAT_norm_loss_sims_mat[, i] <- sim_CAT_losses
        CAT_nominal_loss_sims_mat[, i] <- sim_CAT_losses_full_adj
        CAT_damage_ratio_sims_mat[, i] <- sim_CAT_damage_ratio
        
        Non_CAT_losses_agg[, i] <- sim_Non_CAT_agg
        Non_CAT_losses_agg_withInflation[, i] <- sim_Non_CAT_agg_withInflation
        
        Non_CAT_other_losses_agg[, i] <- sim_Non_CAT_other_agg
        Non_CAT_other_losses_agg_withInflation[, i] <- sim_Non_CAT_other_agg_withInflation
        
        }
    ## Define a list to store the climate module outputs:
    Climate_sims_list <- list(NS_Temp = NS_Temp_sims_mat, Air_Temp = Air_Temp_sims_mat, SST = SST_Temp_sims_mat, SST_gradients = SST_grad_Temp_sims_mat, MSLP = MSLP_sims_mat, mfwixx = FWI_mfwixx_sims_mat,rx5day = rx5day_sims_mat)
    ## Define a list to store the MEV outputs
    MEV_list <- list(Inflation_rates = simulated_results_inflation_clim,
                     Real_rates = Interest_Rates_sim_SR,
                     Nominal_rates = Interest_Rates_norminal)
    ## Define a list to store the Hazard module outputs:
    Hazard_count_list <- list(Flood =  Flood_count_sims_mat,
        Bushfire = BF_count_sims_mat,
        TC = TC_count_sims_mat,
        Storm = Storm_count_sims_mat,
        ECL = ECL_count_sims_mat,
        Hailstorm = HS_count_sims_mat)
    Hazard_severity_list <- list(Flood =  Flood_loss_sims_mat,
        Bushfire = BF_loss_sims_mat,
        TC = TC_loss_sims_mat,
        Storm = Storm_loss_sims_mat,
        ECL = ECL_loss_sims_mat,
        Hailstorm = HS_loss_sims_mat)
    ## Define a list to store the liabilities module outputs
    Liabilities_list <- list(CAT_normalised_loss = CAT_norm_loss_sims_mat,
                             CAT_damage_ratio = CAT_damage_ratio_sims_mat,
                             CAT_nominal_loss = CAT_nominal_loss_sims_mat,
                             Non_CAT_loss = Non_CAT_losses_agg,
                             Non_CAT_loss_nominal = Non_CAT_losses_agg_withInflation,
                             Non_CAT_other_loss = Non_CAT_other_losses_agg,
                             Non_CAT_other_loss_nominal = Non_CAT_other_losses_agg_withInflation)
 
    ## Define a list to store all DFA components:
    DFA_components_list <- list(Climate_module = Climate_sims_list, 
                                MEV_module = MEV_list,
                                Hazard_module_count = Hazard_count_list,
                                Hazard_module_sev = Hazard_severity_list,
                                Liabilities_module = Liabilities_list)
   
     DFA_components_all_scenarios[[s]] <- DFA_components_list
}
} else{
    load(path_to_results)
}
```

```{r}
save(DFA_components_all_scenarios, file = "Simulated_results_DFA_components_sims10000_2_Nov_2024.Rdata")
```


Extract the calibrated coefficients for the climate module: 

```{r Extract coefficients from bias corrected models}
## Near surface temperature temperature
        NS_Temp_mod_coefs <- extract_coefs_clim_var(ensemble_hist_dat = tas_ERA5_ensemble_hist_dat, var_name = "NS_temperature", Model_list = Models_ensemble_tas)
## Air temperature
            Air_Temp_mod_coefs <- extract_coefs_clim_var(ensemble_hist_dat = ta_ERA5_ensemble_hist_dat, var_name = "Air_temperature", Model_list = Models_ensemble_ta)
            ## Sea-surface temperature
            SST_mod_coefs <- extract_coefs_clim_var(ensemble_hist_dat = SST_ERA5_ensemble_hist_dat, var_name = "SST_mean_mm", Model_list = Models_ensemble_SST)
            ## Sea-surface temperature gradients
            SST_grad_mod_coefs <- extract_coefs_clim_var(ensemble_hist_dat = SST_grad_ERA5_ensemble_hist_dat, var_name = "SST_diff_mean", Model_list = Models_ensemble_SST)
            ## MSLP
            MSLP_mod_coefs <- extract_coefs_clim_var(ensemble_hist_dat = MSLP_ERA5_ensemble_hist_dat, Model_list = Models_ensemble_MSLP, var_name = "MSLP_mm")
            ## FWI
            FWI_mfwixx_mod_coefs <- extract_coefs_clim_var(ensemble_hist_dat = FWI_ERA5_ensemble_hist_data_m, var_name = "mfwixx", Model_list = Models_ensemble_FWI)
            ## Extreme precipitation
            rx5day_mod_coefs <- extract_coefs_clim_var(ensemble_hist_dat = rx5day_ERA5_ensemble_hist_data, var_name = "rx5day", Model_list = Models_ensemble_rx5day)
```

```{r Export the coefficients}
# write.csv(NS_Temp_mod_coefs, "Plotting/Climate/Bias correction coefficients/NS_Temp_mod_coefs.csv")
# write.csv(SST_mod_coefs, "Plotting/Climate/Bias correction coefficients/SST_mod_coefs.csv")
# write.csv(Air_Temp_mod_coefs, "Plotting/Climate/Bias correction coefficients/Air_Temp_mod_coefs.csv")
# write.csv(SST_grad_mod_coefs, "Plotting/Climate/Bias correction coefficients/SST_grad_mod_coefs.csv")
# write.csv(MSLP_mod_coefs, "Plotting/Climate/Bias correction coefficients/MSLP_mod_coefs.csv")
# write.csv( FWI_mfwixx_mod_coefs, "Plotting/Climate/Bias correction coefficients/FWI_mod_coefs.csv")
# write.csv(rx5day_mod_coefs, "Plotting/Climate/Bias correction coefficients/rx5day_mod_coefs.csv")
```


#### Simulation of the asset module

Based on the simulated results in the previous section, next we simulate the climate-dependent total equity returns based on the method outlined in Section 2.3.3 in the paper. 

First we estimate the economic damage scalar, which is calculated as the average ratios of uninsured economic damages to insurance losses:

```{r Derive the ratios of insurance losses to economic damage}
suppressWarnings(
CAT_EM_AU <- readxl::read_excel(EM_dat_path, sheet = "EM-DAT Data")
)
CAT_EM_AU <- CAT_EM_AU[is.na(CAT_EM_AU$`Insured Damage, Adjusted ('000 US$)`) == F & is.na(CAT_EM_AU$`Total Damage, Adjusted ('000 US$)`) == F, ]
CAT_EM_AU <- CAT_EM_AU[order(CAT_EM_AU$`Start Year`), ]
## Calculate the ratio of insurance damage to economic damage:
Total_damage_scalar <- (CAT_EM_AU$`Total Damage, Adjusted ('000 US$)`-CAT_EM_AU$`Insured Damage, Adjusted ('000 US$)`)/CAT_EM_AU$`Insured Damage, Adjusted ('000 US$)`
Eco_damage_scalar <- (CAT_EM_AU$`Total Damage, Adjusted ('000 US$)`)/CAT_EM_AU$`Insured Damage, Adjusted ('000 US$)`

Total_damage_scalar_mean <- mean(Total_damage_scalar)
Eco_damage_scalar_mean <- mean(Eco_damage_scalar)
plot(Total_damage_scalar, xlab = "Time", ylab = "Uninsured loss/Insured loss", main = "Ratio of total economic damage to insurance losses")
abline(h = Total_damage_scalar_mean, lty = 2, col = "red")
```



Perform the simulations for both physical risk and transition risk based on method outlined in the paper: 

```{r Simulated the equity return based on the corporate earnings channels}
equity_TR_excess_adj_OP_list <- list()
equity_TR_excess_adj_OP_brown_list <- list()
equity_TR_excess_adj_OP_base_list <- list()
equity_TR_cum_excess_adj_OP_list <- list()
equity_TR_cum_excess_adj_OP_brown_list <- list()
Random_shocks_OP_list <- list()
Random_shocks_TR_op_list <- list()

for(s in 1:4){
## Define the real GDP forecast and change in brown production:
Real_GDP_pred_raw <- Real_GDP_forecasts_all[, s]
Brown_production_change <- Brown_production_growth_SSP[, s + 1]
Real_GDP_pred_base_growth <- na.omit(Real_GDP_pred_raw/lag(Real_GDP_pred_raw)-1)[-1]
## Define empty matrices to store the outputs: 
TR_sims_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
TR_sims_mat_Brown <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
TR_sims_mat_base <-  matrix(NA, nrow = forecast_horizon, ncol = n.sims)
Random_shocks_op_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
Random_shocks_TR_op_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
TR_sims_cum_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
TR_sims_brown_cum_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)

for(i in 1:n.sims){
    ## Get the damage ratios:
    damage_ratio <- pmin(DFA_components_all_scenarios[[s]]$Liabilities_module$CAT_damage_ratio[, i], 0.999)
    ## Constraint the damage ratio to be less than 100% of GDP
    ## Derive the adjusted GDP:
    Real_GDP_pred_adj <- c(Real_GDP_pred_raw[1:2], Real_GDP_pred_raw[3:(3+forecast_horizon-1)]*(1-Total_damage_scalar_mean*damage_ratio))
    Real_GDP_pred_adj_growth <- na.omit(Real_GDP_pred_adj/lag(Real_GDP_pred_adj)-1)[-1]
    ## Derive the total equity returns:
    ### Simulate the random shocks:
    epsilon_op <- rnorm(forecast_horizon, mean = 0, sd = summary(lm_OP_consumption_growth)$sigma)
    epsilon_TR <- rnorm(forecast_horizon, mean = 0, sd = summary(lm_OP_TR)$sigma)
    
    ### Simulate the operating profit growth:
    #### Derive the change in operating profit for the general sector:
    OP <- lm_OP_consumption_growth$coefficients[1] + lm_OP_consumption_growth$coefficients[2]*Real_GDP_pred_adj_growth + epsilon_op
    #### Derive the change in operating profit for the brown sector:
    OP_brown <- OP + Brown_production_change*scalar_impact_WDS_EBIT
    ### Simulate the total excess returns:
    #### General sector:
    TR <- pmax(lm_OP_TR$coefficients[1] + lm_OP_TR$coefficients[2] * OP + epsilon_TR,-1)
    TR_cum <- cumprod(1 + TR) - 1
    #### Brown sector:
    TR_brown <- pmax(lm_OP_TR$coefficients[1] + lm_OP_TR$coefficients[2] * OP_brown + epsilon_TR,-1)
    TR_brown_cum <- cumprod(1 + TR_brown) - 1
    ### Simulate the baseline operating profit growth and excess total equity: 
    OP_base_sims <-  lm_OP_consumption_growth$coefficients[1] + lm_OP_consumption_growth$coefficients[2]*Real_GDP_pred_base_growth + epsilon_op
    TR_base_sims <-  lm_OP_TR$coefficients[1] + lm_OP_TR$coefficients[2] * OP_base_sims + epsilon_TR
    
    ## Store the results into a output:
    TR_sims_mat[, i] <- TR
    TR_sims_mat_Brown[, i] <- TR_brown
    TR_sims_mat_base[, i] <- TR_base_sims
    TR_sims_cum_mat[, i] <- TR_cum
    TR_sims_brown_cum_mat[, i] <- TR_brown_cum 
    Random_shocks_op_mat[, i] <- epsilon_op
    Random_shocks_TR_op_mat[, i] <- epsilon_TR
}
equity_TR_excess_adj_OP_list[[s]] <- TR_sims_mat
equity_TR_excess_adj_OP_brown_list[[s]] <- TR_sims_mat_Brown
equity_TR_excess_adj_OP_base_list[[s]] <- TR_sims_mat_base
equity_TR_cum_excess_adj_OP_list[[s]] <- TR_sims_cum_mat
equity_TR_cum_excess_adj_OP_brown_list[[s]] <- TR_sims_brown_cum_mat
Random_shocks_OP_list[[s]] <- Random_shocks_op_mat
Random_shocks_TR_op_list[[s]] <- Random_shocks_TR_op_mat
}
```


```{r Also derive the adjusted consumption growth}
Adj_consumption_growth_list <- list() 
for(s in 1:4){
    Real_GDP_pred_raw <- Real_GDP_forecasts_all[, s]
    Adj_consumption_growth <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    for(i in 1:n.sims){
    ## Get the damage ratios:
    damage_ratio <- pmin(DFA_components_all_scenarios[[s]]$Liabilities_module$CAT_damage_ratio[, i], 0.999)
    ## Constraint the damage ratio to be less than 100% of GDP
    ## Derive the adjusted GDP:
    Real_GDP_pred_adj <- c(Real_GDP_pred_raw[1:2], Real_GDP_pred_raw[3:(3+forecast_horizon-1)]*(1-Total_damage_scalar_mean*damage_ratio))
    Real_GDP_pred_adj_growth <- na.omit(Real_GDP_pred_adj/lag(Real_GDP_pred_adj)-1)[-1]
    Adj_consumption_growth[, i] <- Real_GDP_pred_adj_growth
    }
Adj_consumption_growth_list[[s]] <- Adj_consumption_growth
}
```

Derive the mean and the uncertainty range of the simulated equity returns: 

```{r Derive the quantiles of total returns based on the corporate earning chanel}
equity_TR_adj_OP_quantiles_list <- list()
equity_TR_adj_OP_quantiles_list_brown <- list()
equity_TR_adj_OP_quantiles_list_base_list <- list()
equity_TR_adj_OP_mins_list <- list()
equity_TR_adj_OP_sd_list <- list()
equity_TR_cum_adj_OP_quantiles_list <- list()
equity_TR_cum_adj_OP_quantiles_list_brown <- list()
for(s in 1:4){
   equity_TR_adj_OP_quantiles_list[[s]] <- apply(equity_TR_excess_adj_OP_list[[s]], MARGIN = 1, FUN = function(x) multi_func(x, quantile_low = 0.005, quantile_high = 0.995))
   equity_TR_adj_OP_quantiles_list_brown[[s]] <- apply(equity_TR_excess_adj_OP_brown_list[[s]], MARGIN = 1, FUN = function(x) multi_func(x, quantile_low = 0.005, quantile_high = 0.995))
   equity_TR_adj_OP_quantiles_list_base_list[[s]] <- apply(equity_TR_excess_adj_OP_base_list[[s]], MARGIN = 1, FUN = function(x) multi_func(x, quantile_low = 0.005, quantile_high = 0.995))
   
   equity_TR_cum_adj_OP_quantiles_list[[s]] <- apply(equity_TR_cum_excess_adj_OP_list[[s]], MARGIN = 1, FUN = function(x) multi_func(x, quantile_low = 0.005, quantile_high = 0.995))
   equity_TR_cum_adj_OP_quantiles_list_brown[[s]] <- apply(equity_TR_cum_excess_adj_OP_brown_list[[s]], MARGIN = 1, FUN = function(x) multi_func(x, quantile_low = 0.005, quantile_high = 0.995))
   
   equity_TR_adj_OP_mins_list[[s]] <- apply(equity_TR_excess_adj_OP_list[[s]], MARGIN = 1, FUN = min) 
   equity_TR_adj_OP_sd_list[[s]] <- apply(equity_TR_excess_adj_OP_list[[s]], MARGIN = 1, FUN = sd) 
}
```


### Insurance market assumptions

This section derives the insurance market assumptions as shown in Table 3.4 in Section 3.1.2. 

First we import the APRA statistics on the premium volume of different general insurers in Australia, and calculate the market share based on the premium volume of individual insurers:

```{r Import APRA company statistics, warning=FALSE}
APRA_market_stats <- read_excel(GI_fins_stats_input, sheet = "Data - Level 2")
## Calculate the average gross earned premiums over the last three years: 
APRA_market_exposure_mean_0 <- APRA_market_stats %>% filter(`Data item` == "Gross earned premium", `Reporting date` >= ymd("2020-12-31")) %>%
    group_by(`Insurance group short name`) %>% summarise(Mean_Gross_earned_premiums = mean(Value)) %>% drop_na()
## Derive the market share based on the gross earned premiums:
APRA_market_exposure_mean <- APRA_market_exposure_mean_0 %>% mutate(Market_share = Mean_Gross_earned_premiums/sum(APRA_market_exposure_mean_0$Mean_Gross_earned_premiums))
APRA_market_exposure_mean <- APRA_market_exposure_mean[order(APRA_market_exposure_mean$Market_share, decreasing = TRUE), ] %>% mutate(Market_share_perc = percent(Market_share, 0.1))
```

Plot the average market share over the last three years: 

- There are four dominating GI insurers (QBE, IAG, Suncorp, Allianz), followed by four medium size insurers, and 12 small insurers. 

```{r Show the APRA statistics}
blank_theme <- theme_minimal()+
  theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid=element_blank(),
  plot.title=element_text(size=14, face="bold")
  )

APRA_market_exposure_mean
ggplot(APRA_market_exposure_mean, aes(x="", y = Market_share, fill=`Insurance group short name`))+
geom_bar(width = 1, stat = "identity")  +blank_theme + coord_polar("y", start=0) + 
  theme(axis.text.x=element_blank())
```

Based on the market share distribution, the market is assumed to be consist of 20 insurers which contains:

- Four large market participants, each with a market share of $20\%$.
- Four medium market participants, each with a market share of $3\%$.
- 12 small participants, each with a market share of $0.7 \%$.

```{r Market shares input to the model}
market_share_large <- mean(sort(APRA_market_exposure_mean$Market_share, decreasing = T)[1:4])
market_share_med <- mean(sort(APRA_market_exposure_mean$Market_share, decreasing = T)[5:8])
market_share_small <- mean(sort(APRA_market_exposure_mean$Market_share, decreasing = T)[9:nrow(APRA_market_exposure_mean)])

market_share_input <- data.frame(Company = c("Large", "Medium", "Small"), Market_shares_raw = c(market_share_large, market_share_med, market_share_small), Numbers = c(4, 4, nrow(APRA_market_exposure_mean) - 8)) %>%
    mutate(Market_shares = c(0.2, 0.03, (1 - 0.2*4 - 0.03*4)/12))

market_share_input
```


Calculate the MCR (Minimum Capital Requirement) multiple based on the quarterly APRA GI performance statistics: 

```{r Import of the APRA statistics, message=FALSE, warning=FALSE}
GI_perf_APRA_dat <- read_excel(GI_perf_data_path , sheet = "Data")
GI_capital_required_APRA_dat <- GI_perf_APRA_dat %>% filter(Category == "Capital and capital requirement") %>%
    filter(`Industry segment group` == "Direct insurer") %>% filter(`Data item` == "Prescribed capital amount") %>%
    group_by(`Reporting date`) %>% summarise(PCA = sum(Value))
GI_capital_eligible_APRA_dat <- GI_perf_APRA_dat %>% filter(Category == "Capital and capital requirement") %>%
    filter(`Industry segment group` == "Direct insurer") %>% filter(`Data item` == "Eligible capital base") %>%
    group_by(`Reporting date`) %>% summarise(Capital_eligible = sum(Value))
GI_capital_stats_APRA <- left_join(GI_capital_required_APRA_dat, GI_capital_eligible_APRA_dat, by = "Reporting date") %>%
    mutate(PCA_multiple = Capital_eligible/PCA)
## Plot the PCA multiple ratios:
PCA_multiple_mean <- mean(GI_capital_stats_APRA$PCA_multiple)
plot(x = GI_capital_stats_APRA$`Reporting date`, y = GI_capital_stats_APRA$PCA_multiple, xlab = "Reporting dates", ylab = "PCA multiples", main = "PCA multiples (APRA statistics)")
abline(h = PCA_multiple_mean, col = "red", lty = 2)
```



### Premium module

This section calculates the gross premium and reinsurance premiums as per the method outlined in Section 2.4.2. 


#### Gross premium


First we derive the expected inflation rates:

```{r Calculate expected inflation rates and CPI}
Inflation_rates_expected_mat <- matrix(NA, nrow = forecast_horizon, ncol = 4)
for(s in 1:4){
   Inflation_rates_expected_mat[, s] <-  apply(DFA_components_all_scenarios[[s]]$MEV_module$Inflation_rates, FUN = mean, MARGIN = 1)
}

CPI_expected_mat <- matrix(NA, nrow = forecast_horizon, ncol = 4)
for(s in 1:4){
    CPI <- c()
    CPI[1] <- CPI_start
    for(t in 2:(forecast_horizon+1)){
        CPI[t] <- CPI[t-1]*(1+Inflation_rates_expected_mat[t-1, s])
    }
CPI_expected_mat[, s] <- CPI[-1]
}
```


Calculate the expected claim frequency and claim cost for flood and bushfire:

```{r Modified calculation of expected claim frequency and claim cost: Precipitation and FWI}
## Flood:
Flood_freq_pred <- matrix(NA, nrow = forecast_horizon, ncol = 4)
Flood_sev_mu_pred <- matrix(NA, nrow = forecast_horizon, ncol = 4)
Flood_sev_sigma_pred <- as.numeric(exp(glm_Flood_sev_select$sigma.coefficients))
Flood_loss_pred <- matrix(NA, nrow = forecast_horizon, ncol = 4)

## Bushfire:
Bushfire_freq_pred <- matrix(NA, nrow = forecast_horizon, ncol = 4)
Bushfire_sev_mu_pred <- matrix(NA, nrow = forecast_horizon, ncol = 4)
Bushfire_sev_sigma_pred <- as.numeric(exp(glm_BF_sev_select$sigma.coefficients))
Bushfire_loss_pred <- matrix(NA, nrow = forecast_horizon, ncol = 4)


for(s in 1:4){
## Flood:
Flood_freq_ind <- matrix(NA, nrow = forecast_horizon, ncol = ncol(rx5day_mean_all_scenarios_corrected_list[[s]]))
Flood_sev_ind <- matrix(NA, nrow = forecast_horizon, ncol = ncol(rx5day_mean_all_scenarios_corrected_list[[s]]))
Flood_loss_ind <- matrix(NA, nrow = forecast_horizon, ncol = ncol(rx5day_mean_all_scenarios_corrected_list[[s]]))

## BF:
BF_freq_ind <- matrix(NA, nrow = forecast_horizon, ncol = ncol(mfwixx_mean_all_scenarios_corrected_list[[s]]))
BF_sev_ind <- matrix(NA, nrow = forecast_horizon, ncol = ncol(mfwixx_mean_all_scenarios_corrected_list[[s]]))
BF_loss_ind <- matrix(NA, nrow = forecast_horizon, ncol = ncol(mfwixx_mean_all_scenarios_corrected_list[[s]]))

    for(j in 1:ncol(rx5day_mean_all_scenarios_corrected_list[[s]])){
            ## Flood:
            Flood_freq <- predict(glm_Flood_freq_select, newdata = data.frame(rx5day = rx5day_mean_all_scenarios_corrected_list[[s]][, j]), type = "response")
            Flood_sev_mu <- predict(glm_Flood_sev_select, newdata = data.frame(rx5day = rx5day_mean_all_scenarios_corrected_list[[s]][, j]), type = "response", what = "mu")
            Flood_sev <- exp(Flood_sev_mu + 0.5*Flood_sev_sigma_pred^2)
            Flood_loss  <- Flood_sev*Flood_freq
            ## Store the outputs:
            Flood_freq_ind[, j] <- Flood_freq
            Flood_sev_ind[, j] <- Flood_sev_mu
            Flood_loss_ind[, j] <- Flood_loss
    }
    for(i in 1:ncol(mfwixx_mean_all_scenarios_corrected_list[[s]])){      
            ## Bushfire:
            Bushfire_freq <- predict(glm_BF_freq_select, newdata = data.frame(mfwixx  = mfwixx_mean_all_scenarios_corrected_list[[s]][, i]), type = "response")
            Bushfire_sev_mu <- predict(glm_BF_sev_select, newdata = data.frame(mfwixx  = mfwixx_mean_all_scenarios_corrected_list[[s]][, i]), type = "response", what = "mu")
            Bushfire_sev <- exp(Bushfire_sev_mu + 0.5*Bushfire_sev_sigma_pred^2)
            Bushfire_loss <- Bushfire_sev*Bushfire_freq
            ## Store the outputs:
            BF_freq_ind[, i] <- Bushfire_freq 
            BF_sev_ind[, i] <- Bushfire_sev_mu
            BF_loss_ind[, i] <- Bushfire_loss
    }
### Calculate the ensemble outputs:
Flood_freq_pred[, s] <- apply(Flood_freq_ind, MARGIN = 1, FUN = mean)
Flood_sev_mu_pred[, s] <- apply(Flood_sev_ind, MARGIN = 1, FUN = mean)
Flood_loss_pred[, s] <- apply(Flood_loss_ind, MARGIN = 1, FUN = mean)

Bushfire_freq_pred[, s] <- apply(BF_freq_ind, MARGIN = 1, FUN = mean)
Bushfire_sev_mu_pred[, s] <- apply(BF_sev_ind, MARGIN = 1, FUN = mean)
Bushfire_loss_pred[, s] <- apply(BF_loss_ind, MARGIN = 1, FUN = mean)
}
```


Calculate the expected claim frequency and claim cost for tropical cyclones, storms, ECL and hailstorms: 

```{r Modified calculation of expected claim frequency and claim cost: Monthly variables}
## TC:
TC_freq_pred <- matrix(NA, nrow = forecast_horizon*12, ncol = 4)
TC_sev_mu_pred <- matrix(NA, nrow = forecast_horizon*12, ncol = 4)
TC_sev_sigma_pred <- as.numeric(exp(glm_TC_sev_select$sigma.coefficients))
TC_loss_pred <- matrix(NA, nrow = forecast_horizon*12, ncol = 4)

## Storm:
Storm_freq_pred <- matrix(NA, nrow = forecast_horizon*12, ncol = 4)
Storm_sev_mu_pred <- matrix(NA, nrow = forecast_horizon*12, ncol = 4)
Storm_sev_sigma_pred <- as.numeric(exp(glm_Storm_sev_select$sigma.coefficients))
Storm_loss_pred <- matrix(NA, nrow = forecast_horizon*12, ncol = 4)

## ECL:
ECL_freq_pred <- matrix(NA, nrow = forecast_horizon*12, ncol = 4)
ECL_sev_mu_pred <- matrix(NA, nrow = forecast_horizon*12, ncol = 4)
ECL_sev_sigma_pred <- as.numeric(exp(glm_ECL_sev_select$sigma.coefficients))
ECL_loss_pred <- matrix(NA, nrow = forecast_horizon*12, ncol = 4)

## Hailstorm:
HS_freq_pred <- matrix(NA, nrow = forecast_horizon*12, ncol = 4)
HS_sev_mu_pred <- matrix(NA, nrow = forecast_horizon*12, ncol = 4)
HS_sev_sigma_pred <- as.numeric(exp(glm_HS_sev_select$sigma.coefficients))
HS_loss_pred <- matrix(NA, nrow = forecast_horizon*12, ncol = 4)

### Generate predictions for TC, storm, ECL and hailstorms:
#### TC:
for(s in 1:4){
    ## Create empty matrices to store the outputs:
    TC_freq_ind <- matrix(NA, nrow = forecast_horizon*12, ncol = ncol(SST_mean_all_scenarios_corrected_list[[s]]))
    TC_sev_mu_ind <- matrix(NA, nrow = forecast_horizon*12, ncol = ncol(SST_mean_all_scenarios_corrected_list[[s]]))
    TC_loss_ind <- matrix(NA, nrow = forecast_horizon*12, ncol = ncol(SST_mean_all_scenarios_corrected_list[[s]]))
    ## Calculate the results:
    for(j in 1:ncol(SST_mean_all_scenarios_corrected_list[[s]])){
        ## TC:
        TC_freq <- predict(glm_TC_freq_select, newdata = data.frame(SST_mean_mm = SST_mean_all_scenarios_corrected_list[[s]][, j], MSLP_mm = MSLP_mean_all_scenarios_corrected_list[[s]][, j]), type = "response")
        TC_sev_mu <- predict(glm_TC_sev_select, newdata = data.frame(SST_mean_mm = SST_mean_all_scenarios_corrected_list[[s]][, j], MSLP_mm = MSLP_mean_all_scenarios_corrected_list[[s]][, j]), type = "response", what = "mu")
        TC_sev <- exp(TC_sev_mu + 0.5*TC_sev_sigma_pred^2)
        TC_loss <- TC_sev*TC_freq
        ## Store the outputs in a matrix:
        TC_freq_ind[, j] <- TC_freq
        TC_sev_mu_ind[, j] <- TC_sev_mu
        TC_loss_ind[, j] <- TC_loss
    }
    TC_freq_pred[,s] <- apply(TC_freq_ind, MARGIN = 1, FUN = mean)
    TC_sev_mu_pred[,s] <- apply(TC_sev_mu_ind, MARGIN = 1, FUN = mean)
    TC_loss_pred[,s] <- apply(TC_loss_ind, MARGIN = 1, FUN = mean)
}

#### Storms:
for(s in 1:4){
    ## Create empty matrices to store the outputs:
    Storm_freq_ind <- matrix(NA, nrow = forecast_horizon*12, ncol = ncol(SST_mean_all_scenarios_corrected_list[[s]]))
    Storm_sev_mu_ind <- matrix(NA, nrow = forecast_horizon*12, ncol = ncol(SST_mean_all_scenarios_corrected_list[[s]]))
    Storm_loss_ind <- matrix(NA, nrow = forecast_horizon*12, ncol = ncol(SST_mean_all_scenarios_corrected_list[[s]]))
    ## Calculate the results:
    for(j in 1:ncol(SST_mean_all_scenarios_corrected_list[[s]])){
        ## TC:
        Storm_freq <- predict(glm_Storm_freq_select, newdata = data.frame(SST_mean_mm = SST_mean_all_scenarios_corrected_list[[s]][, j]), type = "response")
        Storm_sev_mu <- predict(glm_Storm_sev_select, newdata = data.frame(SST_mean_mm = SST_mean_all_scenarios_corrected_list[[s]][, j]), type = "response", what = "mu")
        Storm_sev <- exp(Storm_sev_mu + 0.5*Storm_sev_sigma_pred^2)
        Storm_loss <- Storm_sev*Storm_freq
        ## Store the outputs in a matrix:
        Storm_freq_ind[, j] <- Storm_freq
        Storm_sev_mu_ind[, j] <- Storm_sev_mu
        Storm_loss_ind[, j] <- Storm_loss
    }
    Storm_freq_pred[,s] <- apply(Storm_freq_ind, MARGIN = 1, FUN = mean)
    Storm_sev_mu_pred[,s] <- apply(Storm_sev_mu_ind, MARGIN = 1, FUN = mean)
    Storm_loss_pred[,s] <- apply(Storm_loss_ind, MARGIN = 1, FUN = mean)
}

#### ECLs:
for(s in 1:4){
    ## Create empty matrices to store the outputs:
    ECL_freq_ind <- matrix(NA, nrow = forecast_horizon*12, ncol = ncol(SST_grad_mean_all_scenarios_corrected_list[[s]]))
    ECL_sev_mu_ind <- matrix(NA, nrow = forecast_horizon*12, ncol = ncol(SST_grad_mean_all_scenarios_corrected_list[[s]]))
    ECL_loss_ind <- matrix(NA, nrow = forecast_horizon*12, ncol = ncol(SST_grad_mean_all_scenarios_corrected_list[[s]]))
    ## Calculate the results:
    for(j in 1:ncol(SST_grad_mean_all_scenarios_corrected_list[[s]])){
        ## ECL:
        ECL_freq <- predict(glm_ECL_freq_select, newdata = data.frame(SST_diff_mean = SST_grad_mean_all_scenarios_corrected_list[[s]][, j]), type = "response")
        ECL_sev_mu <- predict(glm_ECL_sev_select, newdata = data.frame(SST_diff_mean = SST_grad_mean_all_scenarios_corrected_list[[s]][, j]), type = "response", what = "mu")
        ECL_sev <- exp(ECL_sev_mu + 0.5*ECL_sev_sigma_pred^2)
        ECL_loss <- ECL_sev*ECL_freq
        ## Store the outputs in a matrix:
        ECL_freq_ind[, j] <- ECL_freq
        ECL_sev_mu_ind[, j] <- ECL_sev_mu
        ECL_loss_ind[, j] <- ECL_loss
    }
    ECL_freq_pred[,s] <- apply(ECL_freq_ind, MARGIN = 1, FUN = mean)
    ECL_sev_mu_pred[,s] <- apply(ECL_sev_mu_ind, MARGIN = 1, FUN = mean)
    ECL_loss_pred[,s] <- apply(ECL_loss_ind, MARGIN = 1, FUN = mean)
}

## Derive the common models used in NS temperature and air temperature projections:
NS_air_temp_model_names <- intersect(colnames(tas_mean_all_scenarios_corrected_list[[1]]), colnames(ta_mean_all_scenarios_corrected_list[[1]]))
#### HS:
for(s in 1:4){
    ## Create empty matrices to store the outputs:
    HS_freq_ind <- matrix(NA, nrow = forecast_horizon*12, ncol = length(NS_air_temp_model_names))
    HS_sev_mu_ind <- matrix(NA, nrow = forecast_horizon*12, ncol = length(NS_air_temp_model_names))
    HS_loss_ind <- matrix(NA, nrow = forecast_horizon*12, ncol = length(NS_air_temp_model_names))
    ## Calculate the results:
    for(j in 1:length(NS_air_temp_model_names)){
        mod <- NS_air_temp_model_names[j]
        ## HS:
        HS_freq <- predict(glm_HS_freq_select, newdata = data.frame(NS_temperature = tas_mean_all_scenarios_corrected_list[[s]][,  mod], Air_temperature = ta_mean_all_scenarios_corrected_list[[s]][, mod]), type = "response")
        HS_sev_mu <- predict(glm_HS_sev_select, newdata = data.frame(NS_temperature = tas_mean_all_scenarios_corrected_list[[s]][,  mod], Air_temperature = ta_mean_all_scenarios_corrected_list[[s]][, mod]), type = "response", what = "mu")
        HS_sev <- exp(HS_sev_mu + 0.5*HS_sev_sigma_pred^2)
        HS_loss <- HS_sev*HS_freq
        ## Store the outputs in a matrix:
        HS_freq_ind[, j] <- HS_freq
        HS_sev_mu_ind[, j] <- HS_sev_mu
        HS_loss_ind[, j] <- HS_loss
    }
    HS_freq_pred[,s] <- apply(HS_freq_ind, MARGIN = 1, FUN = mean)
    HS_sev_mu_pred[,s] <- apply(HS_sev_mu_ind, MARGIN = 1, FUN = mean)
    HS_loss_pred[,s] <- apply(HS_loss_ind, MARGIN = 1, FUN = mean)
}
```

Convert the quantities above into clean formats:

```{r Convert expected losses for hazards into clean formats}
## Flood:
colnames(Flood_freq_pred) = scenario_list
colnames(Flood_sev_mu_pred) = scenario_list
colnames(Flood_loss_pred) = scenario_list

Flood_freq_pred_dat <- as.data.frame(Flood_freq_pred) %>% mutate(Year = start_year_proj:(start_year_proj + forecast_horizon - 1)) %>% dplyr::select(Year, everything())
Flood_sev_mu_pred_dat <- as.data.frame(Flood_sev_mu_pred) %>% mutate(Year = start_year_proj:(start_year_proj + forecast_horizon - 1)) %>% dplyr::select(Year, everything())
Flood_loss_pred_dat <- as.data.frame(Flood_loss_pred) %>% mutate(Year = start_year_proj:(start_year_proj + forecast_horizon - 1)) %>% dplyr::select(Year, everything())

## Bushfire:
colnames(Bushfire_freq_pred) = scenario_list
colnames(Bushfire_sev_mu_pred) = scenario_list
colnames(Bushfire_loss_pred) = scenario_list

Bushfire_freq_pred_dat <- as.data.frame(Bushfire_freq_pred) %>% mutate(Year = start_year_proj:(start_year_proj + forecast_horizon - 1)) %>% dplyr::select(Year, everything())
Bushfire_sev_mu_pred_dat <- as.data.frame(Bushfire_sev_mu_pred) %>% mutate(Year = start_year_proj:(start_year_proj + forecast_horizon - 1)) %>% dplyr::select(Year, everything())
Bushfire_loss_pred_dat <- as.data.frame(Bushfire_loss_pred) %>% mutate(Year = start_year_proj:(start_year_proj + forecast_horizon - 1)) %>% dplyr::select(Year, everything())

## Define a year list and month list
year_list_mm <- rep(start_year_proj:(start_year_proj + forecast_horizon - 1), each = 12)
month_list_mm <- rep(1:12, times = forecast_horizon)



## TC:
colnames(TC_freq_pred) = scenario_list
colnames(TC_sev_mu_pred) = scenario_list
colnames(TC_loss_pred) = scenario_list

TC_freq_pred_dat <- as.data.frame(TC_freq_pred) %>% mutate(Year = year_list_mm, Month = month_list_mm) %>% dplyr::select(Year, everything())
TC_sev_mu_pred_dat <- as.data.frame(TC_sev_mu_pred) %>% mutate(Year = year_list_mm, Month = month_list_mm) %>% dplyr::select(Year, everything())
TC_loss_pred_dat <- as.data.frame(TC_loss_pred) %>% mutate(Year = year_list_mm,  Month = month_list_mm) %>% dplyr::select(Year, everything())

## Storm:
colnames(Storm_freq_pred) = scenario_list
colnames(Storm_sev_mu_pred) = scenario_list
colnames(Storm_loss_pred) = scenario_list

Storm_freq_pred_dat <- as.data.frame(Storm_freq_pred) %>% mutate(Year = year_list_mm, Month = month_list_mm) %>% dplyr::select(Year, everything())
Storm_sev_mu_pred_dat <- as.data.frame(Storm_sev_mu_pred) %>% mutate(Year = year_list_mm, Month = month_list_mm) %>% dplyr::select(Year, everything())
Storm_loss_pred_dat <- as.data.frame(Storm_loss_pred) %>% mutate(Year = year_list_mm,  Month = month_list_mm) %>% dplyr::select(Year, everything())


## ECL:
colnames(ECL_freq_pred) = scenario_list
colnames(ECL_sev_mu_pred) = scenario_list
colnames(ECL_loss_pred) = scenario_list

ECL_freq_pred_dat <- as.data.frame(ECL_freq_pred) %>% mutate(Year = year_list_mm, Month = month_list_mm) %>% dplyr::select(Year, everything())
ECL_sev_mu_pred_dat <- as.data.frame(ECL_sev_mu_pred) %>% mutate(Year = year_list_mm, Month = month_list_mm) %>% dplyr::select(Year, everything())
ECL_loss_pred_dat <- as.data.frame(ECL_loss_pred) %>% mutate(Year = year_list_mm,  Month = month_list_mm) %>% dplyr::select(Year, everything())


## HS:
colnames(HS_freq_pred) = scenario_list
colnames(HS_sev_mu_pred) = scenario_list
colnames(HS_loss_pred) = scenario_list

HS_freq_pred_dat <- as.data.frame(HS_freq_pred) %>% mutate(Year = year_list_mm, Month = month_list_mm) %>% dplyr::select(Year, Month, everything())
HS_sev_mu_pred_dat <- as.data.frame(HS_sev_mu_pred) %>% mutate(Year = year_list_mm, Month = month_list_mm) %>% dplyr::select(Year, Month, everything())
HS_loss_pred_dat <- as.data.frame(HS_loss_pred) %>% mutate(Year = year_list_mm,  Month = month_list_mm) %>% dplyr::select(Year, Month, everything())
```


Convert the monthly frequency data of TC, HS, and ECL loss to annual frequency data: 

```{r Convert monthly hazard frequency data to annual hazard frequency data}
## TC:
TC_freq_pred_dat_yy <- TC_freq_pred_dat %>% group_by(Year) %>% summarise(`SSP 2.6` = sum(`SSP 2.6`), `SSP 4.5` = sum(`SSP 4.5`), `SSP 7.0` = sum(`SSP 7.0`), `SSP 8.5` = sum(`SSP 8.5`)) 
## Storm:
Storm_freq_pred_dat_yy <- Storm_freq_pred_dat %>% group_by(Year) %>% summarise(`SSP 2.6` = sum(`SSP 2.6`), `SSP 4.5` = sum(`SSP 4.5`), `SSP 7.0` = sum(`SSP 7.0`), `SSP 8.5` = sum(`SSP 8.5`)) 
## ECL:
ECL_freq_pred_dat_yy <- ECL_freq_pred_dat %>% group_by(Year) %>% summarise(`SSP 2.6` = sum(`SSP 2.6`), `SSP 4.5` = sum(`SSP 4.5`), `SSP 7.0` = sum(`SSP 7.0`), `SSP 8.5` = sum(`SSP 8.5`)) 
## HS:
HS_freq_pred_dat_yy <- HS_freq_pred_dat %>% group_by(Year) %>% summarise(`SSP 2.6` = sum(`SSP 2.6`), `SSP 4.5` = sum(`SSP 4.5`), `SSP 7.0` = sum(`SSP 7.0`), `SSP 8.5` = sum(`SSP 8.5`))
```



Convert the monthly loss data of TC, HS, and ECL loss to annual loss data: 

```{r Convert monthly hazard data to annual hazard data}
## TC:
TC_loss_pred_dat_yy <- TC_loss_pred_dat %>% group_by(Year) %>% summarise(`SSP 2.6` = sum(`SSP 2.6`), `SSP 4.5` = sum(`SSP 4.5`), `SSP 7.0` = sum(`SSP 7.0`), `SSP 8.5` = sum(`SSP 8.5`)) 
## Storm:
Storm_loss_pred_dat_yy <- Storm_loss_pred_dat %>% group_by(Year) %>% summarise(`SSP 2.6` = sum(`SSP 2.6`), `SSP 4.5` = sum(`SSP 4.5`), `SSP 7.0` = sum(`SSP 7.0`), `SSP 8.5` = sum(`SSP 8.5`)) 
## ECL:
ECL_loss_pred_dat_yy <- ECL_loss_pred_dat %>% group_by(Year) %>% summarise(`SSP 2.6` = sum(`SSP 2.6`), `SSP 4.5` = sum(`SSP 4.5`), `SSP 7.0` = sum(`SSP 7.0`), `SSP 8.5` = sum(`SSP 8.5`)) 
## HS:
HS_loss_pred_dat_yy <- HS_loss_pred_dat %>% group_by(Year) %>% summarise(`SSP 2.6` = sum(`SSP 2.6`), `SSP 4.5` = sum(`SSP 4.5`), `SSP 7.0` = sum(`SSP 7.0`), `SSP 8.5` = sum(`SSP 8.5`)) 
```


Then we derive expected aggregate loss: 

```{r Derive the aggregate expected normalised hazard loss, fig.height = 7}
## Derive the aggregate expected loss:
Hazard_loss_pred_dat <- data.frame(Year = Flood_loss_pred_dat$Year, `SSP 2.6` = Flood_loss_pred_dat$`SSP 2.6` + Bushfire_loss_pred_dat$`SSP 2.6` + TC_loss_pred_dat_yy$`SSP 2.6` + Storm_loss_pred_dat_yy$`SSP 2.6`+ ECL_loss_pred_dat_yy$`SSP 2.6`+ HS_loss_pred_dat_yy$`SSP 2.6`, `SSP 4.5` = Flood_loss_pred_dat$`SSP 4.5` + Bushfire_loss_pred_dat$`SSP 4.5` + TC_loss_pred_dat_yy$`SSP 4.5` + Storm_loss_pred_dat_yy$`SSP 4.5`+ ECL_loss_pred_dat_yy$`SSP 4.5`+ HS_loss_pred_dat_yy$`SSP 4.5`, `SSP 7.0` = Flood_loss_pred_dat$`SSP 7.0` + Bushfire_loss_pred_dat$`SSP 7.0` + TC_loss_pred_dat_yy$`SSP 7.0` + Storm_loss_pred_dat_yy$`SSP 7.0`+ ECL_loss_pred_dat_yy$`SSP 7.0`+ HS_loss_pred_dat_yy$`SSP 7.0`, `SSP 8.5` = Flood_loss_pred_dat$`SSP 8.5` + Bushfire_loss_pred_dat$`SSP 8.5` + TC_loss_pred_dat_yy$`SSP 8.5` + Storm_loss_pred_dat_yy$`SSP 8.5`+ ECL_loss_pred_dat_yy$`SSP 8.5`+ HS_loss_pred_dat_yy$`SSP 8.5`)
colnames(Hazard_loss_pred_dat) = colnames(Flood_loss_pred_dat)
## Derive the aggregate expected frequency: 
Hazard_freq_pred_dat <- data.frame(Year = Flood_freq_pred_dat$Year, `SSP 2.6` = Flood_freq_pred_dat$`SSP 2.6` + Bushfire_freq_pred_dat$`SSP 2.6` + TC_freq_pred_dat_yy$`SSP 2.6` + Storm_freq_pred_dat_yy$`SSP 2.6`+ ECL_freq_pred_dat_yy$`SSP 2.6`+ HS_freq_pred_dat_yy$`SSP 2.6`, `SSP 4.5` = Flood_freq_pred_dat$`SSP 4.5` + Bushfire_freq_pred_dat$`SSP 4.5` + TC_freq_pred_dat_yy$`SSP 4.5` + Storm_freq_pred_dat_yy$`SSP 4.5`+ ECL_freq_pred_dat_yy$`SSP 4.5`+ HS_freq_pred_dat_yy$`SSP 4.5`, `SSP 7.0` = Flood_freq_pred_dat$`SSP 7.0` + Bushfire_freq_pred_dat$`SSP 7.0` + TC_freq_pred_dat_yy$`SSP 7.0` + Storm_freq_pred_dat_yy$`SSP 7.0`+ ECL_freq_pred_dat_yy$`SSP 7.0`+ HS_freq_pred_dat_yy$`SSP 7.0`, `SSP 8.5` = Flood_freq_pred_dat$`SSP 8.5` + Bushfire_freq_pred_dat$`SSP 8.5` + TC_freq_pred_dat_yy$`SSP 8.5` + Storm_freq_pred_dat_yy$`SSP 8.5`+ ECL_freq_pred_dat_yy$`SSP 8.5`+ HS_freq_pred_dat_yy$`SSP 8.5`)
colnames(Hazard_freq_pred_dat) = colnames(Flood_freq_pred_dat)
```

Derive the variance of the aggregate loss for each hazard event type: 

```{r Derive variance of the individual CAT loss, fig.height = 7}
##Flood:
Flood_var_mat <- matrix(NA, nrow = forecast_horizon, ncol = 4)
for(s in 1:4){
    Flood_var_mat[, s] <- Flood_freq_pred[, s]*exp(2*Flood_sev_mu_pred[, s] + Flood_sev_sigma_pred^2)*exp(Flood_sev_sigma_pred^2)
}
colnames(Flood_var_mat) = scenario_list

##Bushfire:
Bushfire_var_mat <- matrix(NA, nrow = forecast_horizon, ncol = 4)
for(s in 1:4){
    Bushfire_var_mat[, s] <- Bushfire_freq_pred[, s]*exp(2*Bushfire_sev_mu_pred[, s] + Bushfire_sev_sigma_pred^2)*exp(Bushfire_sev_sigma_pred^2)
}
colnames(Bushfire_var_mat) = scenario_list

## TC:
TC_var_mat_raw <- matrix(NA, nrow = nrow(TC_loss_pred_dat), ncol = 4)
for(s in 1:4){
    TC_var_mat_raw[, s] <- TC_freq_pred[, s]*exp(2*TC_sev_mu_pred[, s] + TC_sev_sigma_pred^2)*exp(TC_sev_sigma_pred^2)
}
colnames(TC_var_mat_raw) = scenario_list
TC_var_dat <- as.data.frame(TC_var_mat_raw) %>% mutate(Year = TC_loss_pred_dat$Year) %>%
    group_by(Year) %>% summarise(`SSP 2.6` = sum(`SSP 2.6`), `SSP 4.5` = sum(`SSP 4.5`), `SSP 7.0` = sum(`SSP 7.0`), `SSP 8.5` = sum(`SSP 8.5`)) %>% dplyr::select(-c(Year))
TC_var_mat <- as.matrix(TC_var_dat)

## Storm:
Storm_var_mat_raw <- matrix(NA, nrow = nrow(Storm_loss_pred_dat), ncol = 4)
for(s in 1:4){
    Storm_var_mat_raw[, s] <- Storm_freq_pred[, s]*exp(2*Storm_sev_mu_pred[, s] + Storm_sev_sigma_pred^2)*exp(Storm_sev_sigma_pred^2)
}
colnames(Storm_var_mat_raw) = scenario_list
Storm_var_dat <- as.data.frame(Storm_var_mat_raw) %>% mutate(Year = Storm_loss_pred_dat$Year) %>%
    group_by(Year) %>% summarise(`SSP 2.6` = sum(`SSP 2.6`), `SSP 4.5` = sum(`SSP 4.5`), `SSP 7.0` = sum(`SSP 7.0`), `SSP 8.5` = sum(`SSP 8.5`)) %>% dplyr::select(-c(Year))
Storm_var_mat <- as.matrix(Storm_var_dat)


## ECL:
ECL_var_mat_raw <- matrix(NA, nrow = nrow(ECL_loss_pred_dat), ncol = 4)
for(s in 1:4){
    ECL_var_mat_raw[, s] <- ECL_freq_pred[, s]*exp(2*ECL_sev_mu_pred[, s] + ECL_sev_sigma_pred^2)*exp(ECL_sev_sigma_pred^2)
}
colnames(ECL_var_mat_raw) = scenario_list
ECL_var_dat <- as.data.frame(ECL_var_mat_raw) %>% mutate(Year = ECL_loss_pred_dat$Year) %>%
    group_by(Year) %>% summarise(`SSP 2.6` = sum(`SSP 2.6`), `SSP 4.5` = sum(`SSP 4.5`), `SSP 7.0` = sum(`SSP 7.0`), `SSP 8.5` = sum(`SSP 8.5`)) %>% dplyr::select(-c(Year))
ECL_var_mat <- as.matrix(ECL_var_dat)


## Hailstorm:
HS_var_mat_raw <- matrix(NA, nrow = nrow(HS_loss_pred_dat), ncol = 4)
for(s in 1:4){
    HS_var_mat_raw[, s] <- HS_freq_pred[, s]*exp(2*HS_sev_mu_pred[, s] + HS_sev_sigma_pred^2)*exp(HS_sev_sigma_pred^2)
}
colnames(HS_var_mat_raw) = scenario_list
HS_var_dat <- as.data.frame(HS_var_mat_raw) %>% mutate(Year = HS_loss_pred_dat$Year) %>%
    group_by(Year) %>% summarise(`SSP 2.6` = sum(`SSP 2.6`), `SSP 4.5` = sum(`SSP 4.5`), `SSP 7.0` = sum(`SSP 7.0`), `SSP 8.5` = sum(`SSP 8.5`)) %>% dplyr::select(-c(Year))
HS_var_mat <- as.matrix(HS_var_dat)
```

Calculate the variance of the total loss (aggregating across all hazard types): 

```{r Calculate the aggregate variance of all hazards, fig.height = 7}
Hazard_var_agg_mat <- matrix(NA, nrow = forecast_horizon, ncol = 4)
for(s in 1:4){
    Hazard_var_agg_mat[, s] <- Flood_var_mat[, s] + Bushfire_var_mat[, s] + TC_var_mat[, s] + Storm_var_mat[, s]+ ECL_var_mat[, s] + HS_var_mat[, s]
}
colnames(Hazard_var_agg_mat) = Scenarios_list
```


Calculate the coefficients of variance for CAT losses: 

```{r Calculate the coefficient of variance}
CoV_hazard_loss_mat <- matrix(NA, nrow = forecast_horizon, ncol = 4)
#CoV_hazard_loss_mat_sims <- matrix(NA, nrow = forecast_horizon, ncol = 4)
for(s in 1:4){
   CoV_hazard_loss_mat[, s] <- sqrt(Hazard_var_agg_mat[, s])/Hazard_loss_pred_dat[, s + 1]
}
```


Also derive the premium loadings for non-catastrophe losses under the property, motor, fire and ISR (PMFI) line :

```{r CoV and premium loading for non-catastrophe loss}
E_nonCAT <- as.numeric(exp(model_nonCAT$coefficients))
dispersion_nonCAT <- as.numeric(summary(model_nonCAT)$dispersion)
Var_nonCAT <- dispersion_nonCAT*E_nonCAT^power_tw
CoV_nonCAT <- sqrt(Var_nonCAT)/E_nonCAT
data.frame(Tweedie_power = power_tw, CoV_nonCAT = CoV_nonCAT, Implied_Loading = CoV_nonCAT*risk_aversion_default)
```

Calculate the total gross premium and their loadings: 

```{r Calculate the implied total gross premium loadings under both selected and alternative risk aversion}
### Calculate under the selected risk aversion parameter:
gross_premium_loadings_mat <- matrix(NA, nrow = forecast_horizon, ncol = 4)
gross_premium_CAT_norm_mat <- matrix(NA, nrow = forecast_horizon, ncol = 4)
gross_premium_CAT_nominal_mat <- matrix(NA, nrow = forecast_horizon, ncol = 4)
prop_CAT_premium_base_mat <- matrix(NA, nrow = forecast_horizon, ncol = 4)
prop_CAT_premium_total_mat <- matrix(NA, nrow = forecast_horizon, ncol = 4)
gross_premium_exposure_adj_mat <- matrix(NA, nrow = forecast_horizon, ncol = 4)
gross_premium_nominal_mat <- matrix(NA, nrow = forecast_horizon, ncol = 4)
## Perform the calculations:
for(s in 1:4){
    Expected_CAT <-  Hazard_loss_pred_dat[, s + 1]
    Var_CAT <- Hazard_var_agg_mat[, s]
    Exposure <- Exposure_forecasts[, s]
    GDP <- Real_GDP_forecasts[, s]
    CPI_pred <-  CPI_expected_mat[, s]
    ## Calculate the total premiums projections (adjusted for exposure): 
    Premium_CAT <- (Expected_CAT + risk_aversion_default*sqrt(Var_CAT))*GDP/Real_GDP_start
    Premium_nonCAT <- E_nonCAT * (1 + CoV_nonCAT*risk_aversion_default) * Exposure
    
    ## Calculate the base premium based on expected level pricing principal:
    Premium_base <- Expected_CAT*GDP/Real_GDP_start + E_nonCAT*Exposure
    ## Calculate the implied premium loading:
    Gross_premium_loading <- (Premium_CAT + Premium_nonCAT)/Premium_base - 1
    ## Store the results in the matrix:
    gross_premium_CAT_norm_mat[, s] <- Expected_CAT + risk_aversion_default*sqrt(Var_CAT)
    gross_premium_CAT_nominal_mat[, s] <- Premium_CAT*CPI_pred/CPI_start
    gross_premium_loadings_mat[, s] <- Gross_premium_loading
    gross_premium_exposure_adj_mat[, s] <- Premium_CAT + Premium_nonCAT
    gross_premium_nominal_mat[, s] <- (Premium_CAT + Premium_nonCAT)*CPI_pred/CPI_start
    prop_CAT_premium_base_mat[, s] <- (Expected_CAT*GDP/Real_GDP_start)/Premium_base
    prop_CAT_premium_total_mat[, s] <- Premium_CAT/(Premium_CAT + Premium_nonCAT)
}
```



#### Reinsurance premium

This section calculates the risk premium portion of the reinsurance premium based on the method outlined in Section 2.4.3. 

Since the aggregate loss for each hazard type follows a compound Poisson distribution, the total aggregate loss for the portfolio also follows a compound Poisson distribution. The frequency distribution is Poisson with a rate equal to the sum of the individual hazard rates, while the severity distribution is a mixture of the individual severity distributions, weighted by their respective hazard rates.

Firstly, we derive the mixture weights in the compound Poisson distribution allocated to different hazards: 

```{r Derive the weights allocated to different hazards, fig.height = 7}
## Derive the weights for storm-related hazards (monthly resolution):
weights_monthly_ssps_list <- list()
aggr_freq_storm_family <- matrix(NA, nrow = forecast_horizon*12, ncol = 4)
for(s in 1:4){
   aggr_freq <- TC_freq_pred[, s] + Storm_freq_pred[ ,s] + ECL_freq_pred[ ,s] + HS_freq_pred[, s]
   weights_monthly_ssps_list[[s]] <- data.frame(TC_weights = TC_freq_pred[, s]/aggr_freq,
                                                Storm_weights = Storm_freq_pred[ ,s]/aggr_freq,
                                                ECL_weights = ECL_freq_pred[ ,s]/aggr_freq,
                                                HS_weights = HS_freq_pred[, s]/aggr_freq)
   aggr_freq_storm_family[, s] <- aggr_freq
}
## Derive the weights for bushfire and flood (annual resolution):
weights_yy_ssps_list <- list()
aggr_freq_BF_FL <- matrix(NA, nrow = forecast_horizon, ncol = 4)
for(s in 1:4){
    aggr_freq_yy <- Flood_freq_pred_dat[, s + 1] + Bushfire_freq_pred_dat[, s + 1]
    weights_yy_ssps_list[[s]] <- data.frame(FL_weights = Flood_freq_pred_dat[, s + 1]/aggr_freq_yy, BF_weights = Bushfire_freq_pred_dat[, s + 1]/aggr_freq_yy)
    aggr_freq_BF_FL[ ,s] <- aggr_freq_yy
}
## Derive the monthly weights for storm-related hazards:
aggr_freq_storm_family_yy <- as.data.frame(aggr_freq_storm_family) %>%
    mutate(Year = TC_freq_pred_dat$Year) %>% group_by(Year) %>%
    summarise(S1_Total = sum(V1), S2_Total = sum(V2), S3_Total = sum(V3), S4_Total = sum(V4))
aggr_freq_storm_family_dat <- as.data.frame(aggr_freq_storm_family) %>% mutate(Year = TC_freq_pred_dat$Year) %>% dplyr::select(Year, everything()) %>% left_join(aggr_freq_storm_family_yy, by = "Year") %>%
    mutate(S1_w = V1/S1_Total, S2_w = V2/S2_Total, S3_w = V3/S3_Total, S4_w = V4/S4_Total) %>%
    dplyr::select(contains("_w"))
weights_season_ssp_list <- list()
for(s in 1:4){
    weights_season_ssp_list[[s]] <- matrix(aggr_freq_storm_family_dat[ ,s], ncol = 12, nrow = forecast_horizon, byrow = T)
}
## Derive the annual weights for storm-related hazards and the other: 
weights_yy_ssps_storms_BFFL_list <- list()
for(s in 1:4){
   weights_yy_ssps_storms_BFFL_list[[s]] <- data.frame(Storms_w = aggr_freq_storm_family_yy[, s+1]/Hazard_freq_pred_dat[, s + 1], Other_w = aggr_freq_BF_FL[, s]/Hazard_freq_pred_dat[, s + 1])
}

## Derive the annual weights for individual hazards:
weights_yy_ssps_all_hazards_list <- list()
for(s in 1:4){
    weights_yy_ssps_all_hazards_list[[s]] <- data.frame(
        TC_weights = unlist(TC_freq_pred_dat_yy[, s+1]/Hazard_freq_pred_dat[, s + 1]), 
        Storm_weights = unlist(Storm_freq_pred_dat_yy[ ,s+1]/Hazard_freq_pred_dat[, s + 1]),
        ECL_weights = unlist(ECL_freq_pred_dat_yy[ ,s+1]/Hazard_freq_pred_dat[, s + 1]), 
        HS_weights = unlist(HS_freq_pred_dat_yy[, s+1]/Hazard_freq_pred_dat[, s + 1]),
        FL_weights = unlist(Flood_freq_pred_dat[, s + 1]/Hazard_freq_pred_dat[, s + 1]), 
        BF_weights = unlist(Bushfire_freq_pred_dat[, s + 1]/Hazard_freq_pred_dat[, s + 1]))
}

## Store the mu and sigma of all storm related hazards into a single matrix:
Storm_related_mu_list <- list()
Storm_related_sigma_list <- list()
for(s in 1:4){
    Storm_related_mu_list[[s]] <- data.frame(TC_mu = TC_sev_mu_pred[, s], 
               Storm_mu = Storm_sev_mu_pred[, s],
               ECL_mu = ECL_sev_mu_pred[, s],
               HS_mu = HS_sev_mu_pred[, s])
    Storm_related_sigma_list[[s]] <- data.frame(TC_sigma = TC_sev_sigma_pred, 
               Storm_sigma = Storm_sev_sigma_pred,
               ECL_sigma = ECL_sev_sigma_pred,
               HS_sigma = HS_sev_sigma_pred)
}
## Store the mu and sigma for bushfires and floods: 
BFFL_related_mu_list <- list()
BFFL_related_sigma_list <- list()
for(s in 1:4){
    BFFL_related_mu_list[[s]] <- data.frame(FL_mu = Flood_sev_mu_pred[ ,s],
                                            BF_mu = Bushfire_sev_mu_pred[, s])
    BFFL_related_sigma_list[[s]] <- c(Flood_sev_sigma_pred, Bushfire_sev_sigma_pred)
}
```


Before calculating the reinsurance premium, we need to first choose a appropriate reinsurance excess level.

The published statistics on excess and limit level for aggregate catastrophe XoL contract are listed below:

- QBE: aggregate excess level at 500m, with loss limit of 300m; there is also $52.5\%$ quota share on XoL business
- IAG: no excess level on the aggregate CAT losses level; there is a $32.5\%$ quota share applied to all natural hazards (i.e., IAG paid $67.5\%$ of hazard losses while the reinsurer paid the remaining $32.5\%$)
- Suncorp: aggregate excess level at 850m, with loss limit of 400m (Reference: [link](https://www.suncorpgroup.com.au/announcements-pdf/1679416)); there is also a $30\%$ quota share on Queensland home insurance portfolio


Base on the excess level and the quota share statistics below, assume only excess-of-loss reinsurance is purchased, the following excess and limit levels are chosen as the default values in our simulations:

- Excess: 1000M (1e9)
- Limit: 600M (6e8)

```{r Summary of aggregate CAT reinsurance excess statistics}
excess_aggre_stats <- data.frame(Size = rep("Large", 2), Companies = c("QBE", "Suncorp"), Aggregate_Excess = c(500*1e6, 850*1e6), Quota_share = c("52.5% (whole CAT)", "30% (Queensland home)"), Limit = c(300*1e6, 400*1e6)) %>% mutate(Implied_limit_multiples = Limit/Aggregate_Excess)
excess_aggre_stats

## Use the Suncorp aggregate excess statistics: 
excess_default_Large <- 1e9
limit_default_large <- 6e8
```


For other insurers size, we assume their excess and limit levels are proportional to the excess and limit levels of the 
large insurers based on the relative proportion of the market shares (i.e., $\text{Excess for insurer i} = \text{Excess of large insurers} \cdot \frac{\text{Market share of insurer i}}{\text{Market share of large insurers}}$):

```{r}
Reinsurance_structure_input <- data.frame(Size = c("Large", "Medium", "Small"), Aggregate_Excess = c(1e9, 1e9*market_share_input[market_share_input$Company == "Medium", ]$Market_shares/market_share_input[market_share_input$Company == "Large", ]$Market_shares, 3.3e7), Aggregate_Limit = c(6e8, 6e8*market_share_input[market_share_input$Company == "Medium", ]$Market_shares/market_share_input[market_share_input$Company == "Large", ]$Market_shares, 6e8*market_share_input[market_share_input$Company == "Small", ]$Market_shares/market_share_input[market_share_input$Company == "Large", ]$Market_shares))

Reinsurance_structure_input
```

```{r Define the excess and limit levels for different reinsurers' sizes}
excess_default_Large <- Reinsurance_structure_input[Reinsurance_structure_input$Size == "Large", ]$Aggregate_Excess
excess_default_Medium <- Reinsurance_structure_input[Reinsurance_structure_input$Size == "Medium", ]$Aggregate_Excess
excess_default_Small <- Reinsurance_structure_input[Reinsurance_structure_input$Size == "Small", ]$Aggregate_Excess

limit_default_Large <- Reinsurance_structure_input[Reinsurance_structure_input$Size == "Large", ]$Aggregate_Limit
limit_default_Medium <- Reinsurance_structure_input[Reinsurance_structure_input$Size == "Medium", ]$Aggregate_Limit
limit_default_Small <- Reinsurance_structure_input[Reinsurance_structure_input$Size == "Small", ]$Aggregate_Limit
```


Next we calculate the total reinsurance premiums in the market:

```{r Calculate the total reinsurance premiums in the market, warning=FALSE}

reinsurance_base_premium_market_all_scenarios <- matrix(NA, nrow = forecast_horizon, ncol = 4)
reinsurance_premium_market_all_scenarios <- matrix(NA, nrow = forecast_horizon, ncol = 4)
reinsurance_risk_premium_market_all_scenarios <- matrix(NA, nrow = forecast_horizon, ncol = 4)

for(s in 1:4){
    re_premium_calcs_L <- calc_re_premium(excess_list = rep(excess_default_Large, forecast_horizon), s = s, horizon = forecast_horizon, limit_re_list = rep(limit_default_large, forecast_horizon), market_cap = market_share_input[market_share_input$Company == "Large", ]$Market_shares)
    
    re_premium_calcs_M <- calc_re_premium(excess_list = rep(excess_default_Medium, forecast_horizon), s = s, horizon = forecast_horizon, limit_re_list = rep(limit_default_Medium, forecast_horizon), market_cap = market_share_input[market_share_input$Company == "Medium", ]$Market_shares)
    
    re_premium_calcs_S <- calc_re_premium(excess_list = rep(excess_default_Small, forecast_horizon), s = s, horizon = forecast_horizon, limit_re_list = rep(limit_default_Small, forecast_horizon), market_cap = market_share_input[market_share_input$Company == "Small", ]$Market_shares)
    
    ### Calculate the total reinsurance premiums: 
    #### Note: we cap the reinsurance at the limit level; this is because if reinsurance premiums are greater than the limit, the insurers will have no incentive to purchase reinsurance. 
    re_premium_L <- pmin(re_premium_calcs_L$Es_aggr_trunc + risk_aversion_default * sqrt(re_premium_calcs_L$Var_aggr_trunc), limit_default_large)
    re_premium_M <- pmin(re_premium_calcs_M$Es_aggr_trunc + risk_aversion_default * sqrt(re_premium_calcs_M$Var_aggr_trunc), limit_default_Medium)
    re_premium_S <- pmin(re_premium_calcs_S$Es_aggr_trunc + risk_aversion_default * sqrt(re_premium_calcs_S$Var_aggr_trunc), limit_default_Small)
    ### Extract the risk premium part:
    re_risk_premium_L <- risk_aversion_default * sqrt(re_premium_calcs_L$Var_aggr_trunc)
    re_risk_premium_M <- risk_aversion_default * sqrt(re_premium_calcs_M$Var_aggr_trunc)
    re_risk_premium_S <- risk_aversion_default * sqrt(re_premium_calcs_S$Var_aggr_trunc)
    
    ### Derive the total base market reinsurance premium:
    reinsurance_base_premium_market_all_scenarios[, s] <- re_premium_calcs_L$Es_aggr_trunc*market_share_input[market_share_input$Company == "Large", ]$Numbers + re_premium_calcs_M$Es_aggr_trunc*market_share_input[market_share_input$Company == "Medium", ]$Numbers + re_premium_calcs_S$Es_aggr_trunc*market_share_input[market_share_input$Company == "Small", ]$Numbers
    
    ### Derive the total market reinsurance premium:
    reinsurance_premium_market_all_scenarios[, s] <- re_premium_L*market_share_input[market_share_input$Company == "Large", ]$Numbers + re_premium_M*market_share_input[market_share_input$Company == "Medium", ]$Numbers + re_premium_S * market_share_input[market_share_input$Company == "Small", ]$Numbers
    ### Derive the total market reinsurance risk premium (i.e., reinsurance premium loadings): 
    reinsurance_risk_premium_market_all_scenarios[, s] <- re_risk_premium_L*market_share_input[market_share_input$Company == "Large", ]$Numbers + re_risk_premium_M*market_share_input[market_share_input$Company == "Medium", ]$Numbers + re_risk_premium_S * market_share_input[market_share_input$Company == "Small", ]$Numbers
}
```



### Surplus module

This section seeks to simulate the surplus based on method outlined in Section 2.5.


Firstly, we derive the initial capital level for different sizes of insurers:

```{r Derive initial capital level under different management strategies, warning=FALSE}
## Define empty vectors to store the outputs:
init_capital_L <- c()
init_capital_M <- c()
init_capital_S <- c()
## Define empty vectors to store the normalised outputs:
init_capital_L_norm <- c()
init_capital_M_norm <- c()
init_capital_S_norm <- c()


for(s in 1:4){
    ## Initial capital requirement for large size: 
    UL_sims_Large <- calc_surplus_func_full_cycle(Risk_free_weight = Risk_free_weight_default, 
                                  Brown_weight = Brown_weight, 
                                  market_cap = market_share_input[market_share_input$Company == "Large", ]$Market_shares,
                                  reinsurance_excess = Reinsurance_structure_input[Reinsurance_structure_input$Size == "Large", ]$Aggregate_Excess,
                                  reinsurance_limit_multiple = Reinsurance_structure_input[Reinsurance_structure_input$Size == "Large", ]$Aggregate_Limit/Reinsurance_structure_input[Reinsurance_structure_input$Size == "Large", ]$Aggregate_Excess, 
                                  S_init = 0, 
                                  S_init_norm = 0,
                                  scenario_index = s, 
                                  Expected_Non_CAT_Loss = mean(predict(model_nonCAT, type = "response")),
                                  dispersion_nonCAT = dispersion_nonCAT,
                                  Expected_Non_CAT_Loss_other = mean(predict(model_nonCAT_other, type = "response")), 
                                  dispersion_nonCAT_other = as.numeric(summary(model_nonCAT_other)$dispersion),
                                  Exposure_pred_other_full = Exposure_forecasts_other, 
                                  Power_Non_CAT_loss = power_tw, 
                                  Expected_CAT_Loss = Hazard_loss_pred_dat[, -1],
                                  Var_CAT_loss = Hazard_var_agg_mat, 
                                  risk_aversion = risk_aversion_default, 
                                  CPI_pred_mat = CPI_expected_mat, CPI_start = CPI_start, 
                                  GDP_pred_mat = Real_GDP_forecasts, GDP_start = Real_GDP_start, 
                                  Exposure_pred_mat = Exposure_forecasts,
                                  Exposure_start = Exposure_start,
                                  simulation_DFA_list_all = DFA_components_all_scenarios, 
                                  equities_output = equity_TR_excess_adj_OP_list, 
                                  equities_output_brown = equity_TR_excess_adj_OP_brown_list,
                                  n.sims = n.sims, 
                                  horizon = 2,
                                  assumption_alternative = FALSE)
    
    ## Initial capital requirement for medium size: 
    UL_sims_Medium <- calc_surplus_func_full_cycle(Risk_free_weight = Risk_free_weight_default, 
                                  Brown_weight = Brown_weight, 
                                  market_cap = market_share_input[market_share_input$Company == "Medium", ]$Market_shares,
                                  reinsurance_excess = Reinsurance_structure_input[Reinsurance_structure_input$Size == "Medium", ]$Aggregate_Excess,
                                  reinsurance_limit_multiple = Reinsurance_structure_input[Reinsurance_structure_input$Size == "Medium", ]$Aggregate_Limit/Reinsurance_structure_input[Reinsurance_structure_input$Size == "Medium", ]$Aggregate_Excess, 
                                  S_init = 0, 
                                  S_init_norm = 0,
                                  scenario_index = s, 
                                  Expected_Non_CAT_Loss = mean(predict(model_nonCAT, type = "response")),
                                  dispersion_nonCAT = dispersion_nonCAT,
                                  Expected_Non_CAT_Loss_other = mean(predict(model_nonCAT_other, type = "response")), 
                                  dispersion_nonCAT_other = as.numeric(summary(model_nonCAT_other)$dispersion),
                                  Exposure_pred_other_full = Exposure_forecasts_other, 
                                  Power_Non_CAT_loss = power_tw, 
                                  Expected_CAT_Loss = Hazard_loss_pred_dat[, -1],
                                  Var_CAT_loss = Hazard_var_agg_mat, 
                                  risk_aversion = risk_aversion_default, 
                                  CPI_pred_mat = CPI_expected_mat, CPI_start = CPI_start, 
                                  GDP_pred_mat = Real_GDP_forecasts, GDP_start = Real_GDP_start, 
                                  Exposure_pred_mat = Exposure_forecasts,
                                  Exposure_start = Exposure_start,
                                  simulation_DFA_list_all = DFA_components_all_scenarios, 
                                  equities_output = equity_TR_excess_adj_OP_list, 
                                  equities_output_brown = equity_TR_excess_adj_OP_brown_list,
                                  n.sims = n.sims, 
                                  horizon = 2,
                                  assumption_alternative = FALSE)
    
    ## Initial capital requirement for small size: 
    UL_sims_Small <- calc_surplus_func_full_cycle(Risk_free_weight = Risk_free_weight_default, 
                                  Brown_weight = Brown_weight, 
                                  market_cap = market_share_input[market_share_input$Company == "Small", ]$Market_shares,
                                  reinsurance_excess = Reinsurance_structure_input[Reinsurance_structure_input$Size == "Small", ]$Aggregate_Excess,
                                  reinsurance_limit_multiple = Reinsurance_structure_input[Reinsurance_structure_input$Size == "Small", ]$Aggregate_Limit/Reinsurance_structure_input[Reinsurance_structure_input$Size == "Small", ]$Aggregate_Excess, 
                                  S_init = 0, 
                                  S_init_norm = 0,
                                  scenario_index = s, 
                                  Expected_Non_CAT_Loss = mean(predict(model_nonCAT, type = "response")),
                                  dispersion_nonCAT = dispersion_nonCAT,
                                  Expected_Non_CAT_Loss_other = mean(predict(model_nonCAT_other, type = "response")), 
                                  dispersion_nonCAT_other = as.numeric(summary(model_nonCAT_other)$dispersion),
                                  Exposure_pred_other_full = Exposure_forecasts_other, 
                                  Power_Non_CAT_loss = power_tw, 
                                  Expected_CAT_Loss = Hazard_loss_pred_dat[, -1],
                                  Var_CAT_loss = Hazard_var_agg_mat, 
                                  risk_aversion = risk_aversion_default, 
                                  CPI_pred_mat = CPI_expected_mat, CPI_start = CPI_start, 
                                  GDP_pred_mat = Real_GDP_forecasts, GDP_start = Real_GDP_start, 
                                  Exposure_pred_mat = Exposure_forecasts,
                                  Exposure_start = Exposure_start,
                                  simulation_DFA_list_all = DFA_components_all_scenarios, 
                                  equities_output = equity_TR_excess_adj_OP_list, 
                                  equities_output_brown = equity_TR_excess_adj_OP_brown_list,
                                  n.sims = n.sims, 
                                  horizon = 2,
                                  assumption_alternative = FALSE)
    
## Calculate the initial capital requirements at the 99.5th quantile times the PCA multiple: : 
init_capital_L[s] <- quantile(UL_sims_Large$Underwriting_Loss_sims, 0.995) * PCA_multiple_mean
init_capital_M[s] <- quantile(UL_sims_Medium$Underwriting_Loss_sims, 0.995) * PCA_multiple_mean
init_capital_S[s] <- quantile(UL_sims_Small$Underwriting_Loss_sims, 0.995) * PCA_multiple_mean
}
```






```{r Plot of intial capital levels with excess and limit considered, fig.height = 7}
par(mfrow = c(3, 1))
## Plot nominal capital levels:
barplot(init_capital_L/1e9, names.arg = scenario_list, main = "Initial capital (Large)", ylab = "Capital (Billions)")
barplot(init_capital_M/1e9, names.arg = scenario_list, main = "Initial capital (Medium)", ylab = "Capital (Billions)")
barplot(init_capital_S/1e9, names.arg = scenario_list, main = "Initial capital (Small)", ylab = "Capital (Billions)")
```




```{r Derive the capital simulations for the whole forecasting period (Derive the simulations results with reinsurance cycles), warning=FALSE}
## Define empty list to store the outputs:
capitalPremiums_sims_list_L_cycle <- list()
capitalPremiums_sims_list_M_cycle <- list()
capitalPremiums_sims_list_S_cycle <- list()
#capital_sims_list_Market_cycle <- list()

## Calculate the simulated capital: 
for(s in 1:4){
    ## Initial capital requirement for large size: 
    UL_sims_Large_cycle <- calc_surplus_func_full_cycle(Risk_free_weight = Risk_free_weight_default, 
                                  Brown_weight = Brown_weight, 
                                  market_cap = market_share_input[market_share_input$Company == "Large", ]$Market_shares,
                                  reinsurance_excess = Reinsurance_structure_input[Reinsurance_structure_input$Size == "Large", ]$Aggregate_Excess,
                                  reinsurance_limit_multiple = Reinsurance_structure_input[Reinsurance_structure_input$Size == "Large", ]$Aggregate_Limit/Reinsurance_structure_input[Reinsurance_structure_input$Size == "Large", ]$Aggregate_Excess, 
                                  S_init = init_capital_L[s], 
                                  S_init_norm = init_capital_L_norm[s],
                                  scenario_index = s, 
                                  Expected_Non_CAT_Loss = mean(predict(model_nonCAT, type = "response")),
                                  dispersion_nonCAT = dispersion_nonCAT,
                                  Expected_Non_CAT_Loss_other = mean(predict(model_nonCAT_other, type = "response")), 
                                  dispersion_nonCAT_other = as.numeric(summary(model_nonCAT_other)$dispersion),
                                  Exposure_pred_other_full = Exposure_forecasts_other, 
                                  Power_Non_CAT_loss = power_tw, 
                                  Expected_CAT_Loss = Hazard_loss_pred_dat[, -1],
                                  Var_CAT_loss = Hazard_var_agg_mat, 
                                  risk_aversion = risk_aversion_default, 
                                  CPI_pred_mat = CPI_expected_mat, CPI_start = CPI_start, 
                                  GDP_pred_mat = Real_GDP_forecasts, GDP_start = Real_GDP_start, 
                                  Exposure_pred_mat = Exposure_forecasts,
                                  Exposure_start = Exposure_start,
                                  simulation_DFA_list_all = DFA_components_all_scenarios, 
                                  equities_output = equity_TR_excess_adj_OP_list, 
                                  equities_output_brown = equity_TR_excess_adj_OP_brown_list,
                                  n.sims = n.sims, 
                                  horizon = forecast_horizon,
                                  k1_param = k1_default,
                                  S0_param = S_ref_default,
                                  k3_param = 0,
                                  assumption_alternative = FALSE)
    
    ## Initial capital requirement for medium size: 
    UL_sims_Medium_cycle <- calc_surplus_func_full_cycle(Risk_free_weight = Risk_free_weight_default, 
                                  Brown_weight = Brown_weight, 
                                  market_cap = market_share_input[market_share_input$Company == "Medium", ]$Market_shares,
                                  reinsurance_excess = Reinsurance_structure_input[Reinsurance_structure_input$Size == "Medium", ]$Aggregate_Excess,
                                  reinsurance_limit_multiple = Reinsurance_structure_input[Reinsurance_structure_input$Size == "Medium", ]$Aggregate_Limit/Reinsurance_structure_input[Reinsurance_structure_input$Size == "Medium", ]$Aggregate_Excess, 
                                  S_init = init_capital_M[s], 
                                  S_init_norm = init_capital_M_norm[s],
                                  scenario_index = s, 
                                  Expected_Non_CAT_Loss = mean(predict(model_nonCAT, type = "response")),
                                  dispersion_nonCAT = dispersion_nonCAT,
                                  Expected_Non_CAT_Loss_other = mean(predict(model_nonCAT_other, type = "response")), 
                                  dispersion_nonCAT_other = as.numeric(summary(model_nonCAT_other)$dispersion),
                                  Exposure_pred_other_full = Exposure_forecasts_other, 
                                  Power_Non_CAT_loss = power_tw, 
                                  Expected_CAT_Loss = Hazard_loss_pred_dat[, -1],
                                  Var_CAT_loss = Hazard_var_agg_mat, 
                                  risk_aversion = risk_aversion_default, 
                                  CPI_pred_mat = CPI_expected_mat, CPI_start = CPI_start, 
                                  GDP_pred_mat = Real_GDP_forecasts, GDP_start = Real_GDP_start, 
                                  Exposure_pred_mat = Exposure_forecasts,
                                  Exposure_start = Exposure_start,
                                  simulation_DFA_list_all = DFA_components_all_scenarios, 
                                  equities_output = equity_TR_excess_adj_OP_list, 
                                  equities_output_brown = equity_TR_excess_adj_OP_brown_list,
                                  n.sims = n.sims, 
                                  horizon = forecast_horizon,
                                  k1_param = k1_default,
                                  S0_param = S_ref_default,
                                  k3_param = 0,
                                  assumption_alternative = FALSE)
    
    ## Initial capital requirement for small size: 
    UL_sims_Small_cycle <- calc_surplus_func_full_cycle(Risk_free_weight = Risk_free_weight_default, 
                                  Brown_weight = Brown_weight, 
                                  market_cap = market_share_input[market_share_input$Company == "Small", ]$Market_shares,
                                  reinsurance_excess = Reinsurance_structure_input[Reinsurance_structure_input$Size == "Small", ]$Aggregate_Excess,
                                  reinsurance_limit_multiple = Reinsurance_structure_input[Reinsurance_structure_input$Size == "Small", ]$Aggregate_Limit/Reinsurance_structure_input[Reinsurance_structure_input$Size == "Small", ]$Aggregate_Excess, 
                                  S_init = init_capital_S[s], 
                                  S_init_norm = init_capital_S_norm[s],
                                  scenario_index = s, 
                                  Expected_Non_CAT_Loss = mean(predict(model_nonCAT, type = "response")),
                                  dispersion_nonCAT = dispersion_nonCAT,
                                  Expected_Non_CAT_Loss_other = mean(predict(model_nonCAT_other, type = "response")), 
                                  dispersion_nonCAT_other = as.numeric(summary(model_nonCAT_other)$dispersion),
                                  Exposure_pred_other_full = Exposure_forecasts_other, 
                                  Power_Non_CAT_loss = power_tw, 
                                  Expected_CAT_Loss = Hazard_loss_pred_dat[, -1],
                                  Var_CAT_loss = Hazard_var_agg_mat, 
                                  risk_aversion = risk_aversion_default, 
                                  CPI_pred_mat = CPI_expected_mat, CPI_start = CPI_start, 
                                  GDP_pred_mat = Real_GDP_forecasts, GDP_start = Real_GDP_start, 
                                  Exposure_pred_mat = Exposure_forecasts,
                                  Exposure_start = Exposure_start,
                                  simulation_DFA_list_all = DFA_components_all_scenarios, 
                                  equities_output = equity_TR_excess_adj_OP_list, 
                                  equities_output_brown = equity_TR_excess_adj_OP_brown_list,
                                  n.sims = n.sims, 
                                  horizon = forecast_horizon,
                                  k1_param = k1_default,
                                  S0_param = S_ref_default,
                                  k3_param = 0,
                                  assumption_alternative = FALSE)
    
## store the simulated outputs:
capitalPremiums_sims_list_L_cycle[[s]] <- UL_sims_Large_cycle
capitalPremiums_sims_list_M_cycle[[s]] <- UL_sims_Medium_cycle
capitalPremiums_sims_list_S_cycle[[s]] <- UL_sims_Small_cycle
}
```




Based on the capital simulations results for different sizes of insurers, derive the market surplus level:


```{r Calculate the capital results with reinsurance cycle considered}
## Create empty lists: 
capital_sims_list_Market_cycle <- list()
capital_sims_re_list_Market_cycle <- list()
premium_sims_re_list_Market_cycle <- list()
premium_re_all_scanarios_base_nominal_mat <- matrix(NA, nrow = forecast_horizon, ncol = 4)

for(s in 1:4){
    ## Calculate the market surplus:
    capital_sims_list_Market_cycle[[s]] <- capitalPremiums_sims_list_L_cycle[[s]]$Capital_sims*market_share_input[market_share_input$Company == "Large", ]$Numbers + capitalPremiums_sims_list_M_cycle[[s]]$Capital_sims*market_share_input[market_share_input$Company == "Medium", ]$Numbers + capitalPremiums_sims_list_S_cycle[[s]]$Capital_sims*market_share_input[market_share_input$Company == "Small", ]$Numbers
    ## Calculate the market reinsurance capital:
    capital_sims_re_list_Market_cycle[[s]] <- capitalPremiums_sims_list_L_cycle[[s]]$Capital_sims_re*market_share_input[market_share_input$Company == "Large", ]$Numbers + capitalPremiums_sims_list_M_cycle[[s]]$Capital_sims_re*market_share_input[market_share_input$Company == "Medium", ]$Numbers + capitalPremiums_sims_list_S_cycle[[s]]$Capital_sims_re*market_share_input[market_share_input$Company == "Small", ]$Numbers
    ## Calculate the reinsurance premiums:
    premium_sims_re_list_Market_cycle[[s]] <- capitalPremiums_sims_list_L_cycle[[s]]$Premiums_Re*market_share_input[market_share_input$Company == "Large", ]$Numbers + capitalPremiums_sims_list_M_cycle[[s]]$Premiums_Re*market_share_input[market_share_input$Company == "Medium", ]$Numbers + capitalPremiums_sims_list_S_cycle[[s]]$Premiums_Re*market_share_input[market_share_input$Company == "Small", ]$Numbers
    ## Calculate the base reinsurance premiums:
    premium_re_all_scanarios_base_nominal_mat[, s] <-  capitalPremiums_sims_list_L_cycle[[s]]$Premiums_Re_Unadj*market_share_input[market_share_input$Company == "Large", ]$Numbers + capitalPremiums_sims_list_M_cycle[[s]]$Premiums_Re_Unadj*market_share_input[market_share_input$Company == "Medium", ]$Numbers + capitalPremiums_sims_list_S_cycle[[s]]$Premiums_Re_Unadj*market_share_input[market_share_input$Company == "Small", ]$Numbers
} 
```


Adjust the surplus simulations to satisfy the capital constraint:

- The surplus of general insurers are adjusted such that the surplus is set to zero the year after insolvency


```{r Adjust the capitals by setting the equity level after default to be zero (with reinsurance cyle considered)}
capital_sims_list_Market_adj_all_scenarios_cycle <- list()
for(s in 1:4){
    capital_sims_list_Market_adj_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    for(i in 1:n.sims){
        capital_sim <- capital_sims_list_Market_cycle[[s]][, i]
        ## Find the index of the first time that the surplus turns to negative: 
        first_negative_idx <- which(capital_sim <= 0)[1] 
        ## Set all numbers after the first negative number to zero
        if (!is.na(first_negative_idx) && first_negative_idx < length(capital_sim)) {
          capital_sim[(first_negative_idx + 1):length(capital_sim)] <- 0
        }
    capital_sims_list_Market_adj_mat[, i] <- capital_sim
    }
capital_sims_list_Market_adj_all_scenarios_cycle[[s]] <- capital_sims_list_Market_adj_mat
}
```

Derive the normalised values of the market capital:

```{r Derive the normalised values of the market capital (with reinsurance cyle considered)}
## Capital:
capital_sims_list_Market_adj_all_scenarios_norm_cycle <- list()
capital_sims_list_Market_adj_all_scenarios_CPI_adj_cycle <- list()
## Reinsurance capital:
capital_re_sims_list_Market_adj_all_scenarios_norm_cycle <- list()
capital_re_sims_list_Market_adj_all_scenarios_CPI_adj_cycle <- list()
## Reinsurance premiums
premium_re_sims_list_Market_adj_all_scenarios_norm_cycle <- list()
premium_re_sims_list_Market_adj_all_scenarios_CPI_adj_cycle <- list()

for(s in 1:4){
    Real_GDP_pred <- Real_GDP_forecasts[, s]
    ## Capital:
    capital_norm_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    capital_CPI_adj_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    ## Reinsurance capital:
    capital_re_norm_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    capital_re_CPI_adj_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    ## Reinsurance premiums:
    premiums_re_norm_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    premiums_re_CPI_adj_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    
    for(i in 1:n.sims){
        CPI_sim <- CPI_start * cumprod(1 + DFA_components_all_scenarios[[s]]$MEV_module$Inflation_rates[, i])
        ## Capital
        capital_norm <- capital_sims_list_Market_adj_all_scenarios_cycle[[s]][, i] * Real_GDP_start/Real_GDP_pred * CPI_start/CPI_sim
        capital_CPI_adj <- capital_sims_list_Market_adj_all_scenarios_cycle[[s]][, i] * CPI_start/CPI_sim
        capital_norm_mat[, i] <- capital_norm
        capital_CPI_adj_mat[, i] <- capital_CPI_adj
        ## Reinsurance Capital
        capital_re_norm <- capital_sims_re_list_Market_cycle[[s]][, i] * Real_GDP_start/Real_GDP_pred * CPI_start/CPI_sim
        capital_re_CPI_adj <- capital_sims_re_list_Market_cycle[[s]][, i] * CPI_start/CPI_sim
        capital_re_norm_mat[, i] <- capital_re_norm
        capital_re_CPI_adj_mat[, i] <- capital_re_CPI_adj
        ## Reinsurance Premiums
        premiums_re_norm <- premium_sims_re_list_Market_cycle[[s]][, i] * Real_GDP_start/Real_GDP_pred * CPI_start/CPI_sim
        premiums_re_CPI_adj <- premium_sims_re_list_Market_cycle[[s]][, i] * CPI_start/CPI_sim
        premiums_re_norm_mat[, i] <- premiums_re_norm
        premiums_re_CPI_adj_mat[, i] <- premiums_re_CPI_adj
        
    }
capital_sims_list_Market_adj_all_scenarios_norm_cycle[[s]] <- capital_norm_mat
capital_sims_list_Market_adj_all_scenarios_CPI_adj_cycle[[s]] <- capital_CPI_adj_mat
capital_re_sims_list_Market_adj_all_scenarios_norm_cycle[[s]] <- capital_re_norm_mat
capital_re_sims_list_Market_adj_all_scenarios_CPI_adj_cycle[[s]] <- capital_re_CPI_adj_mat
premium_re_sims_list_Market_adj_all_scenarios_norm_cycle[[s]] <- premiums_re_norm_mat
premium_re_sims_list_Market_adj_all_scenarios_CPI_adj_cycle[[s]] <- premiums_re_CPI_adj_mat
}
```


```{r Normalise the reinsurance base premiums}
premium_re_all_scanarios_base_norm_mat <- matrix(NA, nrow = forecast_horizon, ncol = 4)
for(s in 1:4){
    premium_re_all_scanarios_base_norm_mat[, s] <- premium_re_all_scanarios_base_nominal_mat[, s] * Real_GDP_start/Real_GDP_forecasts[, s]*CPI_start/CPI_expected_mat[, s]
}
```


Calculate the summary statistics for the adjusted primary insurers' market capitals: 

```{r Calculate the market statistics for the adjusted capital levels (with reinsurance cycle considered)}
market_capital_adj_quantiles_list_cycle <- list()
market_capital_adj_quantiles_list_norm_cycle <- list()
market_capital_adj_quantiles_list_CPI_adj_cycle <- list()
market_capital_adj_median_cycle <- matrix(NA, nrow = forecast_horizon, ncol = 4)
market_capital_adj_median_CPI_adj_cycle <- matrix(NA, nrow = forecast_horizon, ncol = 4)
market_capital_adj_default_cycle <- matrix(NA, nrow = forecast_horizon, ncol = 4)
for(s in 1:4){
    ## Derive quantiles for market capital: 
    market_capital_adj_quantiles_list_cycle[[s]] <-  apply(capital_sims_list_Market_adj_all_scenarios_cycle[[s]], MARGIN = 1, FUN = function(x) multi_func(x, quantile_low = 0.005, quantile_high = 0.995))
    ## Also drive the quantiles for normalised surplus level: 
    market_capital_adj_quantiles_list_norm_cycle[[s]] <- apply(capital_sims_list_Market_adj_all_scenarios_norm_cycle[[s]], MARGIN = 1, FUN = function(x) multi_func(x, quantile_low = 0.005, quantile_high = 0.995))
    ## Also drive the quantiles for CPI-adjusted surplus level: 
    market_capital_adj_quantiles_list_CPI_adj_cycle[[s]] <- apply(capital_sims_list_Market_adj_all_scenarios_CPI_adj_cycle[[s]], MARGIN = 1, FUN = function(x) multi_func(x, quantile_low = 0.005, quantile_high = 0.995))
    ## Derive median for market capital: 
    market_capital_adj_median_cycle[, s] <- apply(capital_sims_list_Market_adj_all_scenarios_cycle[[s]], MARGIN = 1, FUN = median)
    ## Derive median for CPI adjusted market capital:
    market_capital_adj_median_CPI_adj_cycle[, s] <- apply(capital_sims_list_Market_adj_all_scenarios_CPI_adj_cycle[[s]], MARGIN = 1, FUN = median)
    ## Derive PD for market capital: 
    market_capital_adj_default_cycle[, s] <- apply(capital_sims_list_Market_adj_all_scenarios_cycle[[s]], MARGIN = 1, FUN = function(x) mean(x <= 0))
}
```

Calculate the summary statistics for the adjusted re-insurers' market capitals: 

```{r Derive the reinsurance capital and premium statistics}
market_capital_re_quantiles_list_cycle <- list()
premium_re_quantiles_list_cycle <- list()
market_capital_re_quantiles_list_cycle_norm <- list()
premium_re_quantiles_list_cycle_norm <- list()
market_capital_re_quantiles_list_cycle_CPI_adj <- list()
premium_re_quantiles_list_cycle_CPI_adj <- list()
solvency_ratios_Re_list_cycle <- list()
## Define the lower abnd upper bounds of stats calculated: 
re_stats_lb <- 0.005
re_stats_ub <- 0.995
## Calculate the statistics:
for(s in 1:4){
    ## Nominal:
    market_capital_re_quantiles_list_cycle[[s]] <- apply(capital_sims_re_list_Market_cycle[[s]], MARGIN = 1, FUN = function(x) multi_func(x, quantile_low = re_stats_lb, quantile_high = re_stats_ub))
    premium_re_quantiles_list_cycle[[s]] <- apply(premium_sims_re_list_Market_cycle[[s]], MARGIN = 1, FUN = function(x) multi_func(x, quantile_low = re_stats_lb, quantile_high = re_stats_ub))
    ## Normalised:
    market_capital_re_quantiles_list_cycle_norm[[s]] <- apply(capital_re_sims_list_Market_adj_all_scenarios_norm_cycle[[s]], MARGIN = 1, FUN = function(x) multi_func(x, quantile_low = re_stats_lb, quantile_high = re_stats_ub))
    premium_re_quantiles_list_cycle_norm[[s]] <- apply(premium_re_sims_list_Market_adj_all_scenarios_norm_cycle[[s]], MARGIN = 1, FUN = function(x) multi_func(x, quantile_low = re_stats_lb, quantile_high = re_stats_ub))
    ## CPI-adjusted: 
    market_capital_re_quantiles_list_cycle_CPI_adj[[s]] <- apply(capital_re_sims_list_Market_adj_all_scenarios_CPI_adj_cycle[[s]], MARGIN = 1, FUN = function(x) multi_func(x, quantile_low = re_stats_lb, quantile_high = re_stats_ub))
    premium_re_quantiles_list_cycle_CPI_adj[[s]] <- apply(premium_re_sims_list_Market_adj_all_scenarios_CPI_adj_cycle[[s]], MARGIN = 1, FUN = function(x) multi_func(x, quantile_low = re_stats_lb, quantile_high = re_stats_ub))
    ## Calculate solvency ratios:
    solvency_ratios_Re_list_cycle[[s]] <- apply(capital_re_sims_list_Market_adj_all_scenarios_norm_cycle[[s]]/premium_re_sims_list_Market_adj_all_scenarios_norm_cycle[[s]], MARGIN = 1, FUN = function(x) multi_func(x, quantile_low = re_stats_lb, quantile_high = re_stats_ub))
    
}
```




## Presentations of simulation results

This section generates the plots shown in Section 3.2 in the paper. 


### Section 3.2.1. Climate and hazards

Find the quantiles of the all simulated climate variables: 

```{r Find the quantiles of the all simulated climate variables}
NS_quantiles_list <- list()
Air_quantiles_list <- list()
SST_quantiles_list <- list()
SST_grad_quantiles_list <- list()
MSLP_quantiles_list <- list()
mfwixx_quantiles_list <- list()
rx5day_quantiles_list <- list()
## Define the quantiles to be plotted: 
#quantiles_clim <- c(0.05, 0.5, 0.95)
for(s in 1:4){
   NS_quantiles_list[[s]] <- apply(DFA_components_all_scenarios[[s]]$Climate_module$NS_Temp, MARGIN = 1, FUN = function(x) multi_func(x)) 
   Air_quantiles_list[[s]] <- apply(DFA_components_all_scenarios[[s]]$Climate_module$Air_Temp, MARGIN = 1, FUN = function(x) multi_func(x))
   SST_quantiles_list[[s]] <- apply(DFA_components_all_scenarios[[s]]$Climate_module$SST, MARGIN = 1, FUN = function(x) multi_func(x))  
   SST_grad_quantiles_list[[s]] <- apply(DFA_components_all_scenarios[[s]]$Climate_module$SST_gradients, MARGIN = 1, FUN = function(x) multi_func(x)) 
   MSLP_quantiles_list[[s]] <- apply(DFA_components_all_scenarios[[s]]$Climate_module$MSLP, MARGIN = 1, FUN = function(x) multi_func(x)) 
   mfwixx_quantiles_list[[s]] <- apply(DFA_components_all_scenarios[[s]]$Climate_module$mfwixx, MARGIN = 1, FUN = function(x) multi_func(x))  
   rx5day_quantiles_list[[s]] <- apply(DFA_components_all_scenarios[[s]]$Climate_module$rx5day, MARGIN = 1, FUN = function(x) multi_func(x))  
}
```

Plot of simulation results for climate module:

- This generates Figure 3.1 shown in the paper. 

```{r Plot of simulation results for climate module, fig.height = 7}
#par(mfrow = c(2,2))
##Define horizon to show:
horizon <- 38
start_year <- 2023
#pdf("Plotting/Climate/ClimateSimulations.pdf")
par(mfrow = c(3, 2))
##Near-surface temperature:
plot_quantiles_scenario(NS_quantiles_list , forecast_periods = start_year:(start_year+horizon - 1), var_name = "Near-surface temperature", main = "Near-surface temperature", y_range = c(20, 28), colour_list = c("green", "orange", "red", "brown"), horizon = horizon, transparency = 0.1, label_percents = F)
legend("topleft", legend = c("SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5"), col = c("green", "orange", "red", "brown"), cex = 0.38, lty = c(1,1,1,1))
# ##Air temperature:
# plot_quantiles_scenario(Air_quantiles_list , forecast_periods = start_year:(start_year+horizon - 1), var_name = "Air temperature", main = "Atmospheric temperature", y_range = c(-36, -30), colour_list = c("green", "orange", "red", "brown"), horizon = horizon, transparency = 0.1, label_percents = F)
# legend("topleft", legend = c("SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5"), col = c("green", "orange", "red", "brown"), cex = 0.38, lty = c(1,1,1,1))
##SST:
plot_quantiles_scenario(SST_quantiles_list , forecast_periods = start_year:(start_year+horizon - 1), var_name = "Sea-surface temperature", main = "SST", y_range = c(24, 28), colour_list = c("green", "orange", "red", "brown"), horizon = horizon, transparency = 0.1, label_percents = F)
legend("topleft", legend = c("SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5"), col = c("green", "orange", "red", "brown"), cex = 0.38, lty = c(1,1,1,1))
##SST gradients
plot_quantiles_scenario(SST_grad_quantiles_list , forecast_periods = start_year:(start_year+horizon - 1), var_name = "Sea-surface temperature gradients", main = "SST gradients", y_range = c(-2.5, 0.5), colour_list = c("green", "orange", "red", "brown"), horizon = horizon, transparency = 0.1, label_percents = F)
legend("topleft", legend = c("SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5"), col = c("green", "orange", "red", "brown"), cex = 0.38, lty = c(1,1,1,1))
#MSLP
plot_quantiles_scenario(MSLP_quantiles_list, forecast_periods = start_year:(start_year+horizon - 1), var_name = "MSLP", main = "MSLP", y_range = c(101100, 101900), colour_list = c("green", "orange", "red", "brown"), horizon = horizon, transparency = 0.1, label_percents = F)
legend("topleft", legend = c("SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5"), col = c("green", "orange", "red", "brown"), cex = 0.38, lty = c(1,1,1,1))
##FWI
plot_quantiles_scenario(mfwixx_quantiles_list , forecast_periods = start_year:(start_year+horizon - 1), var_name = "mfwixx", main = "Extreme FWI", y_range = c(80, 170), colour_list = c("green", "orange", "red", "brown"), horizon = horizon, transparency = 0.1, label_percents = F)
legend("topleft", legend = c("SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5"), col = c("green", "orange", "red", "brown"), cex = 0.38, lty = c(1,1,1,1))
##rx5day
plot_quantiles_scenario(rx5day_quantiles_list , forecast_periods = start_year:(start_year+horizon - 1), var_name = "rx5day", main = "Extreme precipitation", y_range = c(30, 210), colour_list = c("green", "orange", "red", "brown"), horizon = horizon, transparency = 0.1, label_percents = F)
legend("topleft", legend = c("SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5"), col = c("green", "orange", "red", "brown"), cex = 0.38, lty = c(1,1,1,1))
#dev.off()
```



Derive the summary statistics of hazard losses: 


```{r Find the quantiles of simulated hazards}
Flood_count_quantiles_list <- list()
Flood_loss_quantiles_list <- list()

Bushfire_count_quantiles_list <- list()
Bushfire_loss_quantiles_list <- list()

TC_count_quantiles_list <- list()
TC_loss_quantiles_list <- list()

Storm_count_quantiles_list <- list()
Storm_loss_quantiles_list <- list()

ECL_count_quantiles_list <- list()
ECL_loss_quantiles_list <- list()

Hailstorm_count_quantiles_list <- list()
Hailstorm_loss_quantiles_list <- list()

## Define the quantiles to be plotted: 
#quantiles_hazard<- c(0.05, 0.5, 0.95)
for(s in 1:4){
    ## Calculate the quantiles for event count: 
    ## Flood: 
   Flood_count_quantiles_list[[s]] <- apply(DFA_components_all_scenarios[[s]]$Hazard_module_count$Flood, MARGIN = 1, FUN = function(x) multi_func(x)) 
   Flood_loss_quantiles_list[[s]] <- apply(DFA_components_all_scenarios[[s]]$Hazard_module_sev$Flood, MARGIN = 1, FUN = function(x) multi_func(x)) 
   
   ## Bushfire: 
   Bushfire_count_quantiles_list[[s]] <- apply(DFA_components_all_scenarios[[s]]$Hazard_module_count$Bushfire, MARGIN = 1, FUN = function(x) multi_func(x)) 
   Bushfire_loss_quantiles_list[[s]] <- apply(DFA_components_all_scenarios[[s]]$Hazard_module_sev$Bushfire, MARGIN = 1, FUN = function(x) multi_func(x)) 
   
   ## TC: 
   TC_count_quantiles_list[[s]] <- apply(DFA_components_all_scenarios[[s]]$Hazard_module_count$TC, MARGIN = 1, FUN = function(x) multi_func(x)) 
   TC_loss_quantiles_list[[s]] <- apply(DFA_components_all_scenarios[[s]]$Hazard_module_sev$TC, MARGIN = 1, FUN = function(x) multi_func(x)) 
   
   ## Storm: 
   Storm_count_quantiles_list[[s]] <- apply(DFA_components_all_scenarios[[s]]$Hazard_module_count$Storm, MARGIN = 1, FUN = function(x) multi_func(x)) 
   Storm_loss_quantiles_list[[s]] <- apply(DFA_components_all_scenarios[[s]]$Hazard_module_sev$Storm, MARGIN = 1, FUN = function(x) multi_func(x)) 
   
   ## ECL: 
   ECL_count_quantiles_list[[s]] <- apply(DFA_components_all_scenarios[[s]]$Hazard_module_count$ECL, MARGIN = 1, FUN = function(x) multi_func(x)) 
   ECL_loss_quantiles_list[[s]] <- apply(DFA_components_all_scenarios[[s]]$Hazard_module_sev$ECL, MARGIN = 1, FUN = function(x) multi_func(x)) 
   
   ## Hailstorm: 
   Hailstorm_count_quantiles_list[[s]] <- apply(DFA_components_all_scenarios[[s]]$Hazard_module_count$Hailstorm, MARGIN = 1, FUN = function(x) multi_func(x)) 
   Hailstorm_loss_quantiles_list[[s]] <- apply(DFA_components_all_scenarios[[s]]$Hazard_module_sev$Hailstorm, MARGIN = 1, FUN = function(x) multi_func(x)) 
}

```


Plot of simulation results for hazard losses:

- This generates Figure 3.2 shown in the paper.

```{r Plot of simulation results for hazard losses, fig.height = 7}
#pdf("Plotting/Hazards/Simulations_individual_hazard_losses.pdf")
par(mfrow = c(3, 2))
horizon <- 38
#y_range_hazard_sev <- c(0,1.4e10)
## Flood
plot_quantiles_scenario(Flood_loss_quantiles_list , forecast_periods = start_year:(start_year+horizon - 1), var_name = "Flood losses", main = "Flood losses (normalised)", y_range = c(0, 3e10), colour_list = c("green", "orange", "red", "brown"), horizon = horizon, transparency = 0.1, label_percents = F)
legend("topleft", legend = c("SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5"), col = c("green", "orange", "red", "brown"), cex = 0.6, lty = c(1,1,1,1))

## Bushfire
plot_quantiles_scenario(Bushfire_loss_quantiles_list , forecast_periods = start_year:(start_year+horizon - 1), var_name = "Bushfire losses", main = "Bushfires losses (normalised)", y_range = c(0, 4e9), colour_list = c("green", "orange", "red", "brown"), horizon = horizon, transparency = 0.1, label_percents = F)
legend("topleft", legend = c("SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5"), col = c("green", "orange", "red", "brown"), cex = 0.6, lty = c(1,1,1,1))
## TC
plot_quantiles_scenario(TC_loss_quantiles_list , forecast_periods = start_year:(start_year+horizon - 1), var_name = "TC losses", main = "Tropical cyclones losses (normalised)", y_range = c(0, 2e10), colour_list = c("green", "orange", "red", "brown"), horizon = horizon, transparency = 0.1, label_percents = F)
legend("topleft", legend = c("SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5"), col = c("green", "orange", "red", "brown"), cex = 0.6, lty = c(1,1,1,1))
## Storm
plot_quantiles_scenario(Storm_loss_quantiles_list , forecast_periods = start_year:(start_year+horizon - 1), var_name = "Storms losses", main = "Storm losses (normalised)", y_range = c(0, 2e9), colour_list = c("green", "orange", "red", "brown"), horizon = horizon, transparency = 0.1, label_percents = F)
legend("topleft", legend = c("SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5"), col = c("green", "orange", "red", "brown"), cex = 0.6, lty = c(1,1,1,1))
## ECL
plot_quantiles_scenario(ECL_loss_quantiles_list , forecast_periods = start_year:(start_year+horizon - 1), var_name = "ECL losses", main = "East Coast Lows losses (normalised)", y_range = c(0, 1e9), colour_list = c("green", "orange", "red", "brown"), horizon = horizon, transparency = 0.1, label_percents = F)
legend("topleft", legend = c("SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5"), col = c("green", "orange", "red", "brown"), cex = 0.6, lty = c(1,1,1,1))
## Hailstorms
plot_quantiles_scenario(Hailstorm_loss_quantiles_list , forecast_periods = start_year:(start_year+horizon - 1), var_name = "Hailstorms losses", main = "Hailstorm losses (normalised)", y_range = c(0, 3e9), colour_list = c("green", "orange", "red", "brown"), horizon = horizon, transparency = 0.1, label_percents = F)
legend("topleft", legend = c("SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5"), col = c("green", "orange", "red", "brown"), cex = 0.6, lty = c(1,1,1,1))
#dev.off()
```





### Section 3.2.2. Investment returns


We first derive the cumulative investment returns:

```{r Calculation of cumulative investment returns}
investment_returns_real_all_scenarios_list <- list()
investment_returns_nominal_all_scenarios_list <- list()
investment_returns_rates_nominal_all_scenarios_list <- list()
inflation_rates_cum_all_scenarios_list <- list()
log_investment_returns_nominal_all_scenarios_list <- list()
log_inflation_rates_cum_all_scenarios_list <- list()
log_consumption_growth_cum_all_scenarios_list <- list()
for(s in 1:4){
    investment_returns_real_cum_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    investment_returns_nominal_cum_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    inflation_rates_cum_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    log_investment_returns_nominal_cum_mat  <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    log_inflation_rates_cum_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    log_consumption_growth_cum_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    investment_returns_nominal_mat <- matrix(NA, nrow = forecast_horizon, ncol = n.sims)
    for(i in 1:n.sims){
         ## Get the inflation rates:
         Short_rates_real <-  DFA_components_all_scenarios[[s]]$MEV_module$Real_rates[, i]
         Short_rates_nominal <- DFA_components_all_scenarios[[s]]$MEV_module$Nominal_rates[, i]
         inflation_rates <- DFA_components_all_scenarios[[s]]$MEV_module$Inflation_rates[, i]
         consumption_growth <- pmax(Adj_consumption_growth_list[[s]][, i], -1)
         ## Get the brown and other excess return:
         brown_return <- equity_TR_excess_adj_OP_list[[s]][, i]
         other_return <- equity_TR_excess_adj_OP_brown_list[[s]][, i]
         
         
         ## Get the excess equity return:
         equity_return_excess <- Brown_weight*brown_return + (1-Brown_weight)*other_return
         #### Get the nominal equity return: 
         equity_return <- pmax(equity_return_excess + Short_rates_real, -1)
         equity_return_nominal <- pmax(equity_return_excess + Short_rates_nominal, -1)
         ## Get the total return
         investment_return_real <- Risk_free_weight_default*Short_rates_real + (1-Risk_free_weight_default)*equity_return
         investment_return_nominal <- Risk_free_weight_default*Short_rates_nominal + (1-Risk_free_weight_default)*equity_return_nominal
        
         ## Get the cumulative real and nominal investment returns:
         investment_returns_real_cum_mat[, i] <- cumprod(1 + investment_return_real) - 1
         investment_returns_nominal_cum_mat[, i] <- cumprod(1 + investment_return_nominal) - 1
         investment_returns_nominal_mat[, i] <- investment_return_nominal
         log_investment_returns_nominal_cum_mat[, i] <- log(cumprod(1 + investment_return_nominal))
         ## Get the cumulative inflation rates:
         inflation_rates_cum_mat[, i] <- cumprod(1 + inflation_rates) - 1
         log_inflation_rates_cum_mat[, i] <- log(cumprod(1 + inflation_rates))
         ## Get the cumulative consumption growth:
         log_consumption_growth_cum_mat[, i] <- log(cumprod(1 + consumption_growth))
    }
investment_returns_real_all_scenarios_list[[s]] <- investment_returns_real_cum_mat
investment_returns_nominal_all_scenarios_list[[s]] <- investment_returns_nominal_cum_mat
investment_returns_rates_nominal_all_scenarios_list[[s]] <- investment_returns_nominal_mat
log_investment_returns_nominal_all_scenarios_list[[s]] <- log_investment_returns_nominal_cum_mat
inflation_rates_cum_all_scenarios_list[[s]] <- inflation_rates_cum_mat
log_inflation_rates_cum_all_scenarios_list[[s]] <- log_inflation_rates_cum_mat
log_consumption_growth_cum_all_scenarios_list[[s]] <- log_consumption_growth_cum_mat
}
```

```{r Derive the cumulative economic growth and adjusted growth}
log_gdp_potential_cum_growth_mat <- matrix(NA, nrow = forecast_horizon, ncol = 4)
log_consumption_cum_growth_quantiles_all_scenarios_list <- list()
log_consumption_cum_median_all_scenarios_mat <- matrix(NA, nrow = forecast_horizon, ncol = 4)
for(s in 1:4){
    ## Derive the base economic growth:
    gdp_growth_base <- na.omit(Real_GDP_forecasts_all[, s]/lag(Real_GDP_forecasts_all[, s])-1)[2:(forecast_horizon + 1)]
    log_gdp_potential_cum_growth_mat[, s] <- log(cumprod(1 + gdp_growth_base))
    ## Derive the adjusted economic growth:
    log_consumption_cum_growth_quantiles_all_scenarios_list[[s]] <-  apply(log_consumption_growth_cum_all_scenarios_list[[s]], MARGIN = 1, FUN = function(x) multi_func(x[is.finite(x) == T], quantile_low = 0.005, quantile_high = 0.995))
    log_consumption_cum_median_all_scenarios_mat[, s] <- apply(log_consumption_growth_cum_all_scenarios_list[[s]], MARGIN = 1, FUN = median)
}
```


Then calculate the summary statistics of log-scale cumulative investment returns:

```{r Calcualte the quantiles statistics of cummulative investment returns}
investment_returns_real_quantiles_all_scenarios_list <- list()
investment_returns_real_median_all_scenarios_mat <- matrix(NA, nrow = forecast_horizon, ncol = 4)

investment_returns_nominal_quantiles_all_scenarios_list <- list()
investment_returns_nominal_median_all_scenarios_mat <- matrix(NA, nrow = forecast_horizon, ncol = 4)

inflation_cum_quantiles_all_scenarios_list <- list()
inflation_cum_median_all_scenarios_mat <- matrix(NA, nrow = forecast_horizon, ncol = 4)

log_investment_returns_nominal_quantiles_all_scenarios_list <- list()
log_investment_returns_nominal_median_all_scenarios_mat <- matrix(NA, nrow = forecast_horizon, ncol = 4)

log_inflation_cum_quantiles_all_scenarios_list <- list()
log_inflation_cum_median_all_scenarios_mat <- matrix(NA, nrow = forecast_horizon, ncol = 4)


for(s in 1:4){
    ## Calculate real cumulative investment returns:
    investment_returns_real_quantiles_all_scenarios_list[[s]] <-  apply(investment_returns_real_all_scenarios_list[[s]], MARGIN = 1, FUN = function(x) multi_func(x, quantile_low = 0.05, quantile_high = 0.95))
    investment_returns_real_median_all_scenarios_mat[, s] <- apply(investment_returns_real_all_scenarios_list[[s]], MARGIN = 1, FUN = median)
    ## Calculate nominal cumulative investment returns:
    investment_returns_nominal_quantiles_all_scenarios_list[[s]] <-  apply(investment_returns_nominal_all_scenarios_list[[s]], MARGIN = 1, FUN = function(x) multi_func(x, quantile_low = 0.05, quantile_high = 0.95))
    investment_returns_nominal_median_all_scenarios_mat[, s] <- apply(investment_returns_nominal_all_scenarios_list[[s]], MARGIN = 1, FUN = median)
    ## Calculate log compounded investment returns:
    log_investment_returns_nominal_quantiles_all_scenarios_list[[s]] <- apply(log_investment_returns_nominal_all_scenarios_list[[s]], MARGIN = 1, FUN = function(x) multi_func(x, quantile_low = 0.05, quantile_high = 0.95))
    log_investment_returns_nominal_median_all_scenarios_mat[, s] <- apply(log_investment_returns_nominal_all_scenarios_list[[s]], MARGIN = 1, FUN = median)
    ## Calculate cumulative inflation rates:
    inflation_cum_quantiles_all_scenarios_list[[s]] <-  apply(inflation_rates_cum_all_scenarios_list[[s]], MARGIN = 1, FUN = function(x) multi_func(x, quantile_low = 0.05, quantile_high = 0.95))
    inflation_cum_median_all_scenarios_mat[, s] <- apply(inflation_rates_cum_all_scenarios_list[[s]], MARGIN = 1, FUN = median)
    ## Calculate log compounded inflation rates:
    log_inflation_cum_quantiles_all_scenarios_list[[s]] <-  apply(log_inflation_rates_cum_all_scenarios_list[[s]], MARGIN = 1, FUN = function(x) multi_func(x, quantile_low = 0.05, quantile_high = 0.95))
    log_inflation_cum_median_all_scenarios_mat[, s] <- apply(log_inflation_rates_cum_all_scenarios_list[[s]], MARGIN = 1, FUN = median)
}
```


Plot the mean and the uncertainty range of log-sale of cumulative investment returns:

- This generates Figure 3.3 shown in the paper. 


```{r Plot the quantile range of log-sale of cumulative investment returns (horizon up to 2060), fig.height = 3}
horizon <- 38
#pdf("Plotting/Asset/Simulations_cum_investment_returns_2060.pdf")
plot_quantiles_scenario(log_investment_returns_nominal_quantiles_all_scenarios_list, forecast_periods = start_year:(start_year+ horizon - 1), var_name = "Cumulative returns (log)", main = "Cumulative investment returns (log-scale)", y_range = c(-0.2, 5), colour_list = c("green", "orange", "red", "brown"), horizon = horizon , transparency = 0.1, label_percents = F)
legend("topleft", legend = c("SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5"), col = c("green", "orange", "red", "brown"), cex = 0.6, lty = c(1,1,1,1))
#dev.off()
```


Plot the log-scale cumulative consumption growth: 

- This generates Figure 3.4 shown in the paper. 

```{r Plot the log-scale cumulative consumption growth (horizon up to 2060), fig.height = 3}
## Plot of compounded economic growth:
#pdf("Plotting/Asset/Log_compound_economic_growth.pdf")
plot(x = start_year:(start_year + horizon - 1), y =  log_gdp_potential_cum_growth_mat[1:horizon, 1], col = col_list[1], type = "l", ylim = c(0, 1.5), xlab = "Years", ylab = "Compounded growth (log)", main = "Compounded economic growth (log)")
for(s in 2:4){
    points(x = start_year:(start_year + horizon - 1), y =  log_gdp_potential_cum_growth_mat[1:horizon, s], col = col_list[s], type = "l")
}
legend("topleft", legend = c("SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5"), col = c("green", "orange", "red", "brown"), cex = 0.6, lty = c(1,1,1,1))
#dev.off()
```



Calculate the mean and uncertainty range of the simulated damage ratios: 

```{r Find the quantiles of simulated economic damage ratios}
#quantiles_liabilities <- c(0.05, 0.5, 0.95)
Eco_damage_ratio_quantiles_list <- list()
Total_damage_ratio_quantiles_list <- list()
for(s in 1:4){
   Eco_damage_ratio_quantiles_list[[s]] <- apply(DFA_components_all_scenarios[[s]]$Liabilities_module$CAT_damage_ratio * Total_damage_scalar_mean, MARGIN = 1, FUN = function(x) multi_func(x)) 
   Total_damage_ratio_quantiles_list[[s]] <- apply(DFA_components_all_scenarios[[s]]$Liabilities_module$CAT_damage_ratio * Eco_damage_scalar_mean, MARGIN = 1, FUN = function(x) multi_func(x)) 
}
```

Plot the uninsured damage ratios and total damage ratios:

- This generates Figure 3.5 and 3.6 in Section 3.2.2. 

```{r Plot of uninsured damage ratios and total damage ratios}
horizon <- 38
##  Uninsured damage ratio: 
#pdf("Plotting/Asset/Uninsured_damage_ratios_2060.pdf")
plot_quantiles_scenario_2(Eco_damage_ratio_quantiles_list, forecast_periods = start_year:(start_year+horizon - 1), var_name = "Damage ratios", main = "Ratios of uninsured damage to GDP", y_range = c(0, 0.02), colour_list = c("green", "orange", "red", "brown"), horizon = horizon, transparency = 0.1, label_percents = T, step_plot_perc = 0.005)
legend("topleft", legend = c("SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5"), col = c("green", "orange", "red", "brown"), cex = 0.6, lty = c(1,1,1,1))
#dev.off()
## Total damage ratios:
#pdf("Plotting/Asset/Total_damage_ratios_2060.pdf")
plot_quantiles_scenario_2(Total_damage_ratio_quantiles_list, forecast_periods = start_year:(start_year+horizon - 1), var_name = "Damage ratios", main = "Ratios of total damage to GDP", y_range = c(0, 0.03), colour_list = c("green", "orange", "red", "brown"), horizon = horizon, transparency = 0.1, label_percents = T, step_plot_perc = 0.005)
legend("topleft", legend = c("SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5"), col = c("green", "orange", "red", "brown"), cex = 0.6, lty = c(1,1,1,1))
#dev.off()
```

Benchmarking analysis: 

- This generates Figure A in Appendix D.

```{r Import and interpolate the benchmarking data for damage ratios}
## Import the data: 
DICE_2023_dat <- read_excel("Data/Benchmarking/DICE_model_outputs_2023.xlsx", sheet = "Damage_data")
DICE_2016_dat <- read_excel("Data/Benchmarking/DICE_model_outputs_2016.xlsx", sheet = "Damage_data")
## Interpolate the data:
interpolated_years <- min(DICE_2023_dat$Year): max(DICE_2023_dat$Year)
## Interpolate the DICE damage outputs data: 
### DICE 2023: 
Damage_ratios_DICE2023 <- data.frame(Year = interpolated_years,
                                              Damage_BASE = spline(DICE_2023_dat$Year, DICE_2023_dat$Damage_BASE, n = length(interpolated_years))$y,
                                              Damage_Opt = spline(DICE_2023_dat$Year, DICE_2023_dat$Damage_Opt, n = length(interpolated_years))$y,
                                              Damage_Lim2 = spline(DICE_2023_dat$Year, DICE_2023_dat$Damage_Lim2, n = length(interpolated_years))$y) %>% filter(Year >= 2023)
### DICE 2016:
Damage_ratios_DICE2016 <- data.frame(Year = interpolated_years,
                                              Damage_BASE = spline(DICE_2016_dat$Year, DICE_2016_dat$Damage_BASE, n = length(interpolated_years))$y,
                                              Damage_Opt = spline(DICE_2016_dat$Year, DICE_2016_dat$Damage_Opt, n = length(interpolated_years))$y) %>% filter(Year >= 2023)
```

```{r Plot the simulated damage ratios in comparison with the damage ratios projected from DICE model}
horizon <- 38
#pdf("Plotting/Benchmarking/Damage_ratios_comparison.pdf")
plot_quantiles_scenario_2(Total_damage_ratio_quantiles_list, forecast_periods = start_year:(start_year+horizon - 1), var_name = "Damage ratios", main = "Ratios of total damage to GDP", y_range = c(0, 0.04), colour_list = c("green", "orange", "red", "brown"), horizon = horizon, transparency = 0.1, label_percents = T, step_plot_perc = 0.005, quantile_lines = F)
legend("topleft", legend = c("SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5", "Baseline (DICE-2023)", "Cost-benefit optimal (DICE-2023)", "2°C Target", "Baseline (DICE-2016)", "Cost-benefit optimal (DICE-2016)"), col = c("green", "orange", "red", "brown", "#FF6347", "#FFB347", "#228B22","#FF6347", "#FFB347"), cex = 0.6, lty = c(1,1,1,1, 1, 1, 1, 2, 2), pch = c(rep(NA, 4), rep(16, 3), rep(17, 2)))
## Add the comparison to DICE model
points(Damage_ratios_DICE2023$Year, Damage_ratios_DICE2023$Damage_BASE, col = "#FF6347", type = "o", pch = 16)
points(Damage_ratios_DICE2023$Year, Damage_ratios_DICE2023$Damage_Opt, col = "#FFB347", type = "o", pch = 16)
points(Damage_ratios_DICE2023$Year, Damage_ratios_DICE2023$Damage_Lim2,  col = "#228B22", type = "o", pch = 16)
points(Damage_ratios_DICE2016$Year, Damage_ratios_DICE2016$Damage_BASE, col = "#FF6347", type = "o", pch = 17, lty = 2)
points(Damage_ratios_DICE2016$Year, Damage_ratios_DICE2016$Damage_Opt, col = "#FFB347", type = "o", pch = 17, lty = 2)
#dev.off()
```

### Section 3.2.3. Premiums and underwriting losses


Plot the normalised gross CAT premiums below:

- This generates Figure 3.7 in Section 3.2.3.

```{r Calculate the normalised gross premiums, fig.height = 7}
col_list <- c("green", "orange", "red", "brown")
#par(mfrow = c(4, 1))
horizon <- 38

#pdf("Plotting/Premiums/Gross_CAT_premiums_normalised_2060.pdf")
## Plot the gross CAT premiums:
plot(x = start_year_proj:(start_year_proj + horizon - 1), y = gross_premium_CAT_norm_mat[1:horizon, 1], type = "l", col = col_list[1], xlab = "Years", ylab = "Premiums", main = "Gross CAT premiums (normalised)", ylim = c(4e9, 1e10))
for(s in 2:4){
    points(x = start_year_proj:(start_year_proj + horizon - 1), y = gross_premium_CAT_norm_mat[1:horizon, s], type = "l", col = col_list[s])
}
legend("topleft", legend = c("SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5"), col = c("green", "orange", "red", "brown"), cex = 0.6, lty = c(1,1,1,1))
#dev.off()
```


Plot the projected proportions of CAT premiums together with historical proportions of CAT losses:

- This generates Figure 3.8 in Section 3.2.3.

```{r Plot the historical proportions of CAT losses and projected proportions, fig.height = 7}
horizon <- 38
#pdf("Plotting/Premiums/Gross_CAT_premiums_proportions_2060_withHistorical.pdf")
plot(x = c(GI_Ultimate_Losses_PM$Year, start_year_proj:(start_year_proj + horizon - 1)), y = c(GI_Ultimate_Losses_PM$CAT_loss_annual/GI_Ultimate_Losses_PM$Ultimate_Cost, rep(NA, each = horizon)), type = "p", ylim = c(0, 0.4), pch = 16, xlab = "Years", ylab = "Proportions", main = "Projected proportion of CAT premiums")
for(s in 1:4){
    points(x = c(GI_Ultimate_Losses_PM$Year, start_year_proj:(start_year_proj + horizon - 1)), y = c(rep(NA, nrow(GI_Ultimate_Losses_PM)),prop_CAT_premium_total_mat[1:horizon, s]), type = "l", col = col_list[s])
}
abline(h = mean(GI_Ultimate_Losses_PM$CAT_loss_annual/GI_Ultimate_Losses_PM$Ultimate_Cost), lty = 2)
legend("topleft", legend = c("Historical observed proportion of CAT losses","Historical average proportion of CAT losses", "SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5"), col = c("black", "black","green", "orange", "red", "brown"), cex = 0.6, lty = c(NA,2,1,1,1,1), pch = c(16, rep(NA, 5)))
#dev.off()
```



Plot the normalised reinsurance premiums and the relative change:

- This generates Figure 3.9 to Figure 3.11 in Section 3.2.3.

```{r Plot of reinsurance premiums (with cyclical effects considered)}
#pdf("Plotting/Premiums/Reinsurance_premiums_soladj_proj_2060_mean.pdf")
horizon <- 38
plot(x = start_year:(start_year+horizon - 1), y = premium_re_quantiles_list_cycle_norm[[1]][2, 1:horizon], type = "l", col = col_list[1], ylim = c(5e8, 2.5e9), xlab = "Years", ylab = "Reinsurance premium", main = "Reinsurance premiums (normalised; mean)", cex.lab = 0.9)
for(s in 2:4){
    points(x = start_year:(start_year+horizon - 1), y = premium_re_quantiles_list_cycle_norm[[s]][2, 1:horizon], type = "l", col = col_list[s], lty = 1)
}
legend("topleft", legend = c("SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5"), col = c("green", "orange", "red", "brown"), cex = 0.8, lty = c(1,1,1,1))
#dev.off()
```




```{r Plot the relative change in reinsurance premiums in comparison with the base premiums}
## Relative change:
#par(mfrow = c(3, 1))
#pdf("Plotting/Premiums/Reinsurance_premiums_solAdj_vs_base_mean.pdf")
## Mean:
plot(x = start_year:(start_year+horizon - 1), y = premium_re_quantiles_list_cycle_norm[[1]][2, 1:horizon]/premium_re_all_scanarios_base_norm_mat[1:horizon, 1] - 1, type = "l", col = col_list[1], ylim = c(0, 1), main = "Relative difference to base reinsurance premiums (mean)", xlab  = "Years", ylab = "Ratios", yaxt = "n")
axis(2, at = seq(0, 1, by = 0.2), labels = paste0(seq(0, 1, by = 0.2) * 100, '%'), las = 1)
for(s in 2:4){
    points(x = start_year:(start_year+horizon - 1), y = premium_re_quantiles_list_cycle_norm[[s]][2, 1:horizon]/premium_re_all_scanarios_base_norm_mat[1:horizon, s] - 1, type = "l", col = col_list[s])
}
legend("bottomright", legend = c("SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5"), col = c("green", "orange", "red", "brown"), cex = 0.8, lty = c(1,1,1,1))
#dev.off()
#pdf("Plotting/Premiums/Reinsurance_premiums_solAdj_vs_base_995th.pdf")
## 99.5th quantile:
plot(x = start_year:(start_year+horizon - 1), y = premium_re_quantiles_list_cycle_norm[[1]][3, 1:horizon]/premium_re_all_scanarios_base_norm_mat[1:horizon, 1] - 1, type = "l", col = col_list[1], ylim = c(0, 10), main = "Relative difference to base reinsurance premiums (99.5th)", xlab  = "Years", ylab = "Ratios", yaxt = "n")
axis(2, at = seq(0, 8, by = 2), labels = paste0(seq(0, 8, by = 2) * 100, '%'), las = 1)
for(s in 2:4){
    points(x = start_year:(start_year+horizon - 1), y = premium_re_quantiles_list_cycle_norm[[s]][3, 1:horizon]/premium_re_all_scanarios_base_norm_mat[1:horizon, s]- 1, type = "l", col = col_list[s])
}
legend("bottomright", legend = c("SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5"), col = c("green", "orange", "red", "brown"), cex = 0.8, lty = c(1,1,1,1))
#dev.off()
```



Calculate and plot the underwriting losses:

- This generates Figure 3.12 shown in Section Section 3.2.3 in the paper. 


```{r Calculation of the underwriting losses}
#### Calculation of reinsurance recoveries and net CAT losses: 
CAT_net_losses_market_list <- list()
for(s in 1:4){
    ## Calculate reinsurance recoveries for each company:
    re_calc_L <- calc_reinsurance_recoveries_norm(horizon = 78, market_cap = market_share_input[market_share_input$Company == "Large", ]$Market_shares, scenario_index = s, reinsurance_excess = excess_default_Large, reinsurance_limit_multiple = limit_default_large/excess_default_Large)
    re_calc_M <- calc_reinsurance_recoveries_norm(horizon = 78, market_cap = market_share_input[market_share_input$Company == "Medium", ]$Market_shares, scenario_index = s, reinsurance_excess = excess_default_Medium, reinsurance_limit_multiple = limit_default_Medium/excess_default_Medium)
    re_calc_S <- calc_reinsurance_recoveries_norm(horizon = 78, market_cap = market_share_input[market_share_input$Company == "Small", ]$Market_shares, scenario_index = s, reinsurance_excess = excess_default_Small, reinsurance_limit_multiple = limit_default_Small/excess_default_Small)
    ## Calculate reinsurance recoveries for the whole market: 
    CAT_net_losses_market_list[[s]] <- re_calc_L$CAT_loss_net * market_share_input[market_share_input$Company == "Large", ]$Numbers + re_calc_M$CAT_loss_net * market_share_input[market_share_input$Company == "Medium", ]$Numbers + re_calc_S$CAT_loss_net * market_share_input[market_share_input$Company == "Small", ]$Numbers
}

####### Also calculate the net loss based on nominal values: 
CAT_net_losses_market_list_nominal <- list()
for(s in 1:4){
    ## Calculate reinsurance recoveries for each company:
    re_calc_L <- calc_reinsurance_recoveries(horizon = 78, market_cap = market_share_input[market_share_input$Company == "Large", ]$Market_shares, scenario_index = s, reinsurance_excess = excess_default_Large, reinsurance_limit_multiple = limit_default_large/excess_default_Large, CPI_pred_mat = CPI_expected_mat, CPI_start = CPI_start, 
                                  GDP_pred_mat = Real_GDP_forecasts, GDP_start = Real_GDP_start)
    re_calc_M <- calc_reinsurance_recoveries(horizon = 78, market_cap = market_share_input[market_share_input$Company == "Medium", ]$Market_shares, scenario_index = s, reinsurance_excess = excess_default_Medium, reinsurance_limit_multiple = limit_default_Medium/excess_default_Medium, CPI_pred_mat = CPI_expected_mat, CPI_start = CPI_start, 
                                  GDP_pred_mat = Real_GDP_forecasts, GDP_start = Real_GDP_start)
    re_calc_S <- calc_reinsurance_recoveries(horizon = 78, market_cap = market_share_input[market_share_input$Company == "Small", ]$Market_shares, scenario_index = s, reinsurance_excess = excess_default_Small, reinsurance_limit_multiple = limit_default_Small/excess_default_Small, CPI_pred_mat = CPI_expected_mat, CPI_start = CPI_start, 
                                  GDP_pred_mat = Real_GDP_forecasts, GDP_start = Real_GDP_start)
    ## Calculate reinsurance recoveries for the whole market: 
    CAT_net_losses_market_list_nominal[[s]] <- re_calc_L$CAT_loss_net * market_share_input[market_share_input$Company == "Large", ]$Numbers + re_calc_M$CAT_loss_net * market_share_input[market_share_input$Company == "Medium", ]$Numbers + re_calc_S$CAT_loss_net * market_share_input[market_share_input$Company == "Small", ]$Numbers
}

##### Calculation of the underwriting losses: 
UL_losses_market_list <- list()
for(s in 1:4){
    Net_CAT_premium <- gross_premium_CAT_norm_mat[, s] - reinsurance_premium_market_all_scenarios[, s]
    UL_losses_market_list[[s]] <- CAT_net_losses_market_list[[s]] - Net_CAT_premium
}
UL_losses_market_median_mat <- matrix(NA, nrow = forecast_horizon, ncol = 4)
UL_losses_market_quantiles_all_scenarios_list <- list()
for(s in 1:4){
   UL_losses_market_quantiles_all_scenarios_list[[s]] <- apply(UL_losses_market_list[[s]], MARGIN = 1, FUN = function(x) multi_func(x, quantile_low = 0.05, quantile_high = 0.95))
   UL_losses_market_median_mat[, s] <- apply(UL_losses_market_list[[s]], MARGIN = 1, FUN = median)
}

```





```{r Plot the quantile range of the simulated underwritting losses, fig.height = 3}
#pdf("Plotting/Liabilities/Simulations_underwritinglosses.pdf")
horizon <- 38
plot_quantiles_scenario(UL_losses_market_quantiles_all_scenarios_list, forecast_periods = start_year:(start_year+ horizon - 1), var_name = "Underwriting losses", main = "Simulated underwriting losses (normalised)", y_range = c(-1e10, 3e10), colour_list = c("green", "orange", "red", "brown"), horizon = horizon, transparency = 0.1, label_percents = F)
legend("topleft", legend = c("SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5"), col = c("green", "orange", "red", "brown"), cex = 0.6, lty = c(1,1,1,1))
abline(h = 0, lty = 6)
#dev.off()
```



# Section 3.3 Risk and return measures

This section presents the results shown in Section 3.3 in the paper. 

Plot the expected and median market surplus (log-scale):

- This generates Figure 3.13 and 3.14 in Section 3.3 in the paper. 


```{r Plot of expected and median surplus with adjusted capital (with reinsuranc cycle considered), fig.height = 7}
#par(mfrow = c(2, 1))
## Plot of median surplus (with reinsurance): 
#pdf("Plotting/Surplus/Median_market_surplus_re_cycle.pdf")
horizon <- 38
plot(x = start_year:(start_year + horizon - 1), y = log(market_capital_adj_median_cycle[1:horizon, 1]), col = col_list[1], type = "l", xlab = "Years", ylab = "Median surplus", main = "Median market surplus (log-scale)", ylim = c(23, 32))
for(s in 2:4){
    points(x = start_year:(start_year + horizon - 1), y = log(market_capital_adj_median_cycle[1:horizon, s]), col = col_list[s], type = "l")
}
legend("topleft", legend = c("SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5"), col = c("green", "orange", "red", "brown"), cex = 0.6, lty = c(1,1,1,1))
#dev.off()

## Plot of mean surplus (with reinsurance): 
#pdf("Plotting/Surplus/Mean_market_surplus_re_cycle.pdf")
plot(x = start_year:(start_year + horizon - 1), y = log(market_capital_adj_quantiles_list_cycle[[1]][2, 1:horizon]), col = col_list[1], type = "l", xlab = "Years", ylab = "Mean surplus", main = "Mean market surplus (log-scale)", ylim = c(23, 32))
for(s in 2:4){
    points(x = start_year:(start_year + horizon - 1), y = log(market_capital_adj_quantiles_list_cycle[[s]][2, 1:horizon]), col = col_list[s], type = "l")
}
legend("topleft", legend = c("SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5"), col = c("green", "orange", "red", "brown"), cex = 0.6, lty = c(1,1,1,1))
#dev.off()
```

Plot the market surplus (log-scale) over the log-scale cumulative investment returns:

- This generates Figure 3.15 and Figure 3.16 in Section 3.3 in the paper. 


```{r Plot the monetary surplus values (log-scale) over the log investment returns}
## Visualise the relationship between median surplus (log) and log investment returns: 
#pdf("Plotting/Surplus/Median_surplus_trends_drivers_cycle.pdf")
horizon <- 38
par(mfrow = c(2, 2))
for(s in 1:4){
## Fit a linear regression model:
Inv_lm <- lm(log(market_capital_adj_median_cycle[1:horizon, s]) ~ log_investment_returns_nominal_median_all_scenarios_mat[1:horizon, s])
#Plot:
plot(x = log_investment_returns_nominal_median_all_scenarios_mat[1:horizon, s], y = log(market_capital_adj_median_cycle[1:horizon, s]), col = col_list[s], pch = 16, xlab = "Median investment returns (log)", ylab = "Median surplus (log)", main =scenario_list[s])
## add the fitted line: 
points(x = log_investment_returns_nominal_median_all_scenarios_mat[1:horizon, s], y = fitted(Inv_lm), type = "l", lwd = 1.5, lty = 2)
legend("bottomright", legend = paste("R-squared:", round(summary(Inv_lm)$r.squared, 3)), lty = 2, cex = 0.8)
}
#dev.off()


## Visualise the relationship between Mean surplus (log) and log investment returns: 

#pdf("Plotting/Surplus/Mean_surplus_trends_drivers_cycle.pdf")
par(mfrow = c(2, 2))
for(s in 1:4){
## Fit a linear regression model:
Inv_lm <- lm(log(market_capital_adj_quantiles_list_cycle[[s]][2, 1:horizon]) ~  log_investment_returns_nominal_quantiles_all_scenarios_list[[s]][2, 1:horizon])
#Plot:
plot(x = log_investment_returns_nominal_quantiles_all_scenarios_list[[s]][2, 1:horizon], y = log(market_capital_adj_quantiles_list_cycle[[s]][2, 1:horizon]), col = col_list[s], pch = 16, xlab = "Mean investment returns (log)", ylab = "Expected surplus (log)", main = scenario_list[s])
## add the fitted line: 
points(x = log_investment_returns_nominal_quantiles_all_scenarios_list[[s]][2, 1:horizon], y = fitted(Inv_lm), type = "l", lwd = 1.5, lty = 2)
legend("bottomright", legend = paste("R-squared:", round(summary(Inv_lm)$r.squared, 3)), lty = 2, cex = 0.8)
}
#dev.off()
```

Plot the market insolvency probabilities: 

- This generates Figure 3.17 shown in Section 3.3. 


```{r Plot of market insolvency probabilities (with reinsurnace cycle considered)}
#pdf("Plotting/Surplus/Market_Insolvency_probs_cycle.pdf")
horizon <- 38
plot(x = start_year:(start_year + horizon - 1), y = market_capital_adj_default_cycle[1:horizon, 1], col = col_list[1], type = "l", ylim = c(0, 0.3), xlab = "Years", ylab = "Insolvency probabilities", main = "Market insolvency probability")
for(s in 2:4){
    points(x = start_year:(start_year + horizon - 1), y = market_capital_adj_default_cycle[1:horizon, s], col = col_list[s], type = "l")
}
legend("topleft", legend = c("SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5"), col = c("green", "orange", "red", "brown"), cex = 0.6, lty = c(1,1,1,1))
#dev.off()
```



Calculate and plot the market deficit given insolvency ratios:

- This generates Figure 3.18 shown in Section 3.3. 


```{r Calculte the LGD ratios for adjusted capital levels (with reinsurance cycle considered)}
LGD_quantiles_all_scenarios_list_ratios_adj_cycle <- list()
LGD_all_sims_list_cycle <- list()
for(s in 1:4){
    ## Create empty matries to store LGD ratios:
    LGD_ratios_lower_quant <- c()
    LGD_ratios_mean <- c()
    LGD_ratios_upper_quant <- c()
    
    LGD_all_list <- list()
    for(i in 1:forecast_horizon){
        ## Calculate the dollar values of loss-at-insolvency: 
        Loss <- pmax(-capital_sims_list_Market_adj_all_scenarios_cycle[[s]][i, ], 0)
        Loss_at_default <- Loss[which(Loss > 0)]
        ## Derive the total insurance claims liabilities payable to policyholders:
        Exposure_total_Liab <- DFA_components_all_scenarios[[s]]$Liabilities_module$Non_CAT_loss_nominal[i, ] + DFA_components_all_scenarios[[s]]$Liabilities_module$Non_CAT_other_loss_nominal[i, ] + CAT_net_losses_market_list_nominal[[s]][i, ]
        ## Derive the Exposure At Default: 
        EAD <- Exposure_total_Liab[which(Loss > 0)]
        ## Calculate the average LGD ratios:
        LGD_ratios <- pmin(Loss_at_default/EAD, 1)
        ## Store the outputs for LGD ratios
        LGD_ratios_mean[i] <- mean(LGD_ratios)
        LGD_ratios_lower_quant[i] <- quantile(LGD_ratios, 0.05)
        LGD_ratios_upper_quant[i] <- quantile(LGD_ratios, 0.95)
        LGD_all_list[[i]] <- LGD_ratios
        
    }
LGD_quantiles_all_scenarios_list_ratios_adj_cycle[[s]] <- rbind(LGD_ratios_lower_quant, LGD_ratios_mean, LGD_ratios_upper_quant)
LGD_all_sims_list_cycle[[s]] <- LGD_all_list
}
```



```{r Plot of loss given insolvency ratios (with reinsurance cycle considered)}
#pdf("Plotting/Surplus/Market_deficit_given_insolvency_cycle.pdf")
horizon <- 50
plot_quantiles_scenario(LGD_quantiles_all_scenarios_list_ratios_adj_cycle, forecast_periods = start_year:(start_year + horizon - 1), var_name = "Deficit given insolvency", main = "Market deficit given insolvency (ratios)", y_range = c(0, 1), colour_list = c("green", "orange", "red", "brown"), horizon = horizon, transparency = 0.1, label_percents = F)
legend("topleft", legend = c("SSP 2.6", "SSP 4.5", "SSP 7.5", "SSP 8.5"), col = c("green", "orange", "red", "brown"), cex = 0.6, lty = c(1,1,1,1))
#dev.off()
```


Calculation and the plot of mean CAGR:

- This generates Figure 3.19 shown in the paper. 

```{r Calculation of mean CAGR of surplus under different scenarios}
h_list <- c(3, 5, 10, 15)
mean_CAGR_allScenarios <- matrix(NA, nrow = length(h_list), ncol = 4)
## Compute a compound annual growth rate (CAGR) of surplus:
for(s in 1:4){
    mean_CAGR <- c()
    for(i in 1:length(h_list)){
    h <- h_list[i]
    mean_CAGR[i] <- mean((capital_sims_list_Market_adj_all_scenarios_cycle[[s]][h, ]/capital_sims_list_Market_adj_all_scenarios_cycle[[s]][1, ])^(1/h)-1, na.rm = T)
    }
mean_CAGR_allScenarios[, s] <- mean_CAGR
}
colnames(mean_CAGR_allScenarios) = scenario_list
rownames(mean_CAGR_allScenarios) = h_list
```

```{r Plot of mean CAGR under different scenarios}
#pdf("Plotting/Surplus/MeanGACR.pdf")
# Create pretty tick marks for the y-axis
y_breaks <- pretty(c(0, max(as.vector(mean_CAGR_allScenarios))))

# Plotting ---------------------------------------------------
barplot(
  height    = t(mean_CAGR_allScenarios),       # transpose the matrix
  beside    = TRUE,                            # side-by-side bars
  col       = col_list,  # or use any color palette
  names.arg = rownames(mean_CAGR_allScenarios), # x-axis labels for horizons
  xlab      = "Horizon",
  ylab      = "Mean CAGR (%)",
  main      = "Comparison of mean CAGR across scenarios",
  yaxt = "n",
  ylim = c(0, 0.2), 
  legend.text = TRUE,                          # show a legend
  args.legend = list(x = "topright", bty = "n", cex = 0.8) # legend position and style
)
axis(2,
     at = y_breaks,
     labels = paste0(y_breaks*100, "%"),  # append % sign
     las = 1)  
#dev.off()
```

